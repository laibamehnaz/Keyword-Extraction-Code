{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KEYWORD EXTRACTION CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to be added as the component to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KeyphraseExtraction(object):\n",
    "    # component name, will show up in the pipeline\n",
    "    name = 'Keyphrases' \n",
    "\n",
    "    \n",
    "    def __init__(self, nlp):\n",
    "           Doc.set_extension('score_keyphrases_by_tfidf', getter = self.score_keyphrases_by_tfidf ,  force = True)\n",
    "        \n",
    "\n",
    "    def __call__(self, doc):\n",
    "           return doc  \n",
    "\n",
    "    \n",
    "    # helper function for score_keyphrases_by_tfidf\n",
    "    def extract_candidate_chunks(self , text, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'):\n",
    "        import itertools, nltk, string\n",
    "    \n",
    "        # exclude candidates that are stop words or entirely punctuation\n",
    "        punct = set(string.punctuation)\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "        # tokenize, POS-tag, and chunk using regular expressions\n",
    "        chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "        tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "        all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in tagged_sents))\n",
    "    \n",
    "        # join constituent chunk words into a single chunked phrase\n",
    "        candidates = [' '.join(word for word, pos, chunk in group).lower() for key, group in itertools.groupby(all_chunks, lambda word__pos__chunk : word__pos__chunk[2] != 'O') if key]\n",
    "\n",
    "        return [cand for cand in candidates if cand not in stop_words and not all(char in punct for char in cand)]    \n",
    "        \n",
    "   \n",
    "\n",
    "    def score_keyphrases_by_tfidf(self , doc , candidates='chunks'):\n",
    "        import  nltk\n",
    "        import gensim\n",
    "        # sentence tokenising\n",
    "        #texts = nltk.sent_tokenize(texts)\n",
    "        texts_as_string = str(doc)\n",
    "        list_of_string = []\n",
    "        for text in nltk.sent_tokenize(texts_as_string):\n",
    "            list_of_string.append(str(text))\n",
    "    \n",
    "        texts = list_of_string\n",
    "    \n",
    "        # extract candidates from each text in texts, either chunks or words\n",
    "        if candidates == 'chunks':\n",
    "            boc_texts = [self.extract_candidate_chunks(text) for text in texts]\n",
    "        \n",
    "        elif candidates == 'words':\n",
    "            boc_texts = [self.extract_candidate_words(text) for text in texts]\n",
    "   \n",
    "        # make gensim dictionary and corpus\n",
    "        dictionary = gensim.corpora.Dictionary(boc_texts)\n",
    "    \n",
    "        corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]\n",
    "    \n",
    "         # transform corpus with tf*idf model\n",
    "        tfidf = gensim.models.TfidfModel(corpus)\n",
    "        corpus_tfidf = tfidf[corpus]\n",
    "       \n",
    "    \n",
    "    \n",
    "        # instead of returning\n",
    "        list_tfidf = []\n",
    "        for docs in corpus_tfidf:\n",
    "            for d in docs:\n",
    "                list_tfidf.append(d)\n",
    "    \n",
    "        list1 = []\n",
    "        list2 = []\n",
    "        for i in dictionary:\n",
    "            list1.append(dictionary[i])\n",
    "            list2.append(list_tfidf[i][1])\n",
    "            \n",
    "    \n",
    "        answer_dictionary = dict(zip(list1 , list2))\n",
    "        return answer_dictionary\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text used in the algorithm\n",
    "Everyone has an opinion about finding your passion. It's either the best piece of career advice you've ever heard or the worst. Bill Gates is all for it. He discovered a passion for writing software as a kid and kept at it. Seemed to work out pretty well for him.Mark Cuban is vehemently against hanging your success on finding your passion. Just because you're passionate about something doesn't mean you're good at it. He advises you find where you're putting in the most effort, then double down on that to achieve success.Stanford researchers recently decided to get to the bottom of the matter. They performed a series of experiments and published their findings in Psychological Science.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    component = KeyphraseExtraction(nlp)  # initialise component\n",
    "    nlp.add_pipe(component, last = True)  # add last to the pipeline\n",
    "    \n",
    "    text = \"Everyone has an opinion about finding your passion. It's either the best piece of career advice you've ever heard or the worst. Bill Gates is all for it. He discovered a passion for writing software as a kid and kept at it. Seemed to work out pretty well for him. Mark Cuban is vehemently against hanging your success on finding your passion. Just because you're passionate about something doesn't mean you're good at it. He advises you find where you're putting in the most effort, then double down on that to achieve success. Stanford researchers recently decided to get to the bottom of the matter. They performed a series of experiments and published their findings in Psychological Science.\"  \n",
    "    doc = nlp(text)\n",
    "    print('Pipeline', nlp.pipe_names)\n",
    "    print() \n",
    "    print()\n",
    "    dictionary_of_keyphrases = doc._.score_keyphrases_by_tfidf\n",
    "    for key , value in dictionary_of_keyphrases.items():\n",
    "           print(\"%-40s %4.3f\" % ( key , value ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline ['tagger', 'parser', 'ner', 'Keyphrases']\n",
      "\n",
      "\n",
      "everyone                                 0.663\n",
      "opinion                                  0.663\n",
      "passion                                  0.347\n",
      "piece of career advice                   1.000\n",
      "bill gates                               1.000\n",
      "kept                                     0.289\n",
      "kid                                      0.553\n",
      "software                                 0.553\n",
      "mark cuban                               0.553\n",
      "success                                  0.394\n",
      "something                                0.753\n",
      "effort                                   0.527\n",
      "bottom                                   1.000\n",
      "matter                                   0.573\n",
      "stanford researchers                     0.820\n",
      "findings in psychological science        0.577\n",
      "series of experiments                    0.577\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
