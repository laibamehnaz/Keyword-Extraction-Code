{"Fast Generation of Result Snippets in Web Search\nAndrew Turpin &\nYohannes Tsegay\nRMIT University\nMelbourne, Australia\naht@cs.rmit.edu.au\nytsegay@cs.rmit.edu.au\nDavid Hawking\nCSIRO ICT Centre\nCanberra, Australia\ndavid.hawking@acm.org\nHugh E. Williams\nMicrosoft Corporation\nOne Microsoft Way\nRedmond, WA.\nhughw@microsoft.com\nABSTRACT\nThe presentation of query biased document snippets as part\nof results pages presented by search engines has become an\nexpectation of search engine users. In this paper we \nexplore the algorithms and data structures required as part of\na search engine to allow efficient generation of query biased\nsnippets. We begin by proposing and analysing a document\ncompression method that reduces snippet generation time\nby 58% over a baseline using the zlib compression library.\nThese experiments reveal that finding documents on \nsecondary storage dominates the total cost of generating \nsnippets, and so caching documents in RAM is essential for a\nfast snippet generation process. Using simulation, we \nexamine snippet generation performance for different size RAM\ncaches. Finally we propose and analyse document \nreordering and compaction, revealing a scheme that increases the\nnumber of document cache hits with only a marginal \naffect on snippet quality. This scheme effectively doubles the\nnumber of documents that can fit in a fixed size cache.\nCategories and Subject Descriptors\nH.3.3 [Information Storage and Retrieval]: Information\nSearch and Retrieval; H.3.4 [Information Storage and\nRetrieval]: Systems and Software-performance evaluation\n(efficiency and effectiveness);\nGeneral Terms\nAlgorithms, Experimentation, Measurement, Performance\n1. INTRODUCTION\nEach result in search results list delivered by current WWW\nsearch engines such as search.yahoo.com, google.com and\nsearch.msn.com typically contains the title and URL of the\nactual document, links to live and cached versions of the\ndocument and sometimes an indication of file size and type.\nIn addition, one or more snippets are usually presented, \ngiving the searcher a sneak preview of the document contents.\nSnippets are short fragments of text extracted from the\ndocument content (or its metadata). They may be static\n(for example, always show the first 50 words of the \ndocument, or the content of its description metadata, or a \ndescription taken from a directory site such as dmoz.org) or\nquery-biased [20]. A query-biased snippet is one selectively\nextracted on the basis of its relation to the searcher\"s query.\nThe addition of informative snippets to search results may\nsubstantially increase their value to searchers. Accurate\nsnippets allow the searcher to make good decisions about\nwhich results are worth accessing and which can be ignored.\nIn the best case, snippets may obviate the need to open any\ndocuments by directly providing the answer to the searcher\"s\nreal information need, such as the contact details of a person\nor an organization.\nGeneration of query-biased snippets by Web search \nengines indexing of the order of ten billion web pages and \nhandling hundreds of millions of search queries per day imposes\na very significant computational load (remembering that\neach search typically generates ten snippets). The \nsimpleminded approach of keeping a copy of each document in a\nfile and generating snippets by opening and scanning files,\nworks when query rates are low and collections are small,\nbut does not scale to the degree required. The overhead of\nopening and reading ten files per query on top of \naccessing the index structure to locate them, would be manifestly\nexcessive under heavy query load. Even storing ten billion\nfiles and the corresponding hundreds of terabytes of data is\nbeyond the reach of traditional filesystems. Special-purpose\nfilesystems have been built to address these problems [6].\nNote that the utility of snippets is by no means restricted\nto whole-of-Web search applications. Efficient generation of\nsnippets is also important at the scale of whole-of-government\nsearch services such as www.firstgov.gov (c. 25 million\npages) and govsearch.australia.gov.au (c. 5 million pages)\nand within large enterprises such as IBM [2] (c. 50 million\npages). Snippets may be even more useful in database or\nfilesystem search applications in which no useful URL or\ntitle information is present.\nWe present a new algorithm and compact single-file \nstructure designed for rapid generation of high quality snippets\nand compare its space/time performance against an obvious\nbaseline based on the zlib compressor on various data sets.\nWe report the proportion of time spent for disk seeks, disk\nreads and cpu processing; demonstrating that the time for\nlocating each document (seek time) dominates, as expected.\nAs the time to process a document in RAM is small in\ncomparison to locating and reading the document into \nmemory, it may seem that compression is not required. However,\nthis is only true if there is no caching of documents in RAM.\nControlling the RAM of physical systems for \nexperimentation is difficult, hence we use simulation to show that caching\ndocuments dramatically improves the performance of \nsnippet generation. In turn, the more documents can be \ncompressed, the more can fit in cache, and hence the more disk\nseeks can be avoided: the classic data compression tradeoff\nthat is exploited in inverted file structures and computing\nranked document lists [24].\nAs hitting the document cache is important, we examine\ndocument compaction, as opposed to compression, schemes\nby imposing an a priori ordering of sentences within a \ndocument, and then only allowing leading sentences into cache for\neach document. This leads to further time savings, with only\nmarginal impact on the quality of the snippets returned.\n2. RELATED WORK\nSnippet generation is a special type of extractive \ndocument summarization, in which sentences, or sentence \nfragments, are selected for inclusion in the summary on the basis\nof the degree to which they match the search query. This\nprocess was given the name of query-biased summarization\nby Tombros and Sanderson [20] The reader is referred to\nMani [13] and to Radev et al. [16] for overviews of the very\nmany different applications of summarization and for the\nequally diverse methods for producing summaries.\nEarly Web search engines presented query-independent\nsnippets consisting of the first k bytes of the result \ndocument. Generating these is clearly much simpler and much\nless computationally expensive than processing documents\nto extract query biased summaries, as there is no need to\nsearch the document for text fragments containing query\nterms. To our knowledge, Google was the first \nwhole-ofWeb search engine to provide query biased summaries, but\nsummarization is listed by Brin and Page [1] only under the\nheading of future work.\nMost of the experimental work using query-biased \nsummarization has focused on comparing their value to searchers\nrelative to other types of summary [20, 21], rather than \nefficient generation of summaries. Despite the importance of\nefficient summary generation in Web search, few algorithms\nappear in the literature. Silber and McKoy [19] describe a\nlinear-time lexical chaining algorithm for use in generic \nsummaries, but offer no empirical data for the performance of\ntheir algorithm. White et al [21] report some experimental\ntimings of their WebDocSum system, but the snippet \ngeneration algorithms themselves are not isolated, so it is difficult\nto infer snippet generation time comparable to the times we\nreport in this paper.\n3. SEARCH ENGINE ARCHITECTURES\nA search engine must perform a variety of activities, and is\ncomprised of many sub-systems, as depicted by the boxes in\nFigure 1. Note that there may be several other sub-systems\nsuch as the Advertising Engine or the Parsing Engine\nthat could easily be added to the diagram, but we have \nconcentrated on the sub-systems that are relevant to snippet\ngeneration. Depending on the number of documents that\nthe search engine indexes, the data and processes for each\nRanking\nEngine\nCrawling Engine\nIndexing Engine\nEngine\nLexicon Meta Data\nEngine\nEngine\nSnippet\nTerm&Doc#s\nSnippetperdoc\nWEB\nQuery Engine\nQuery Results Page\nTerm#s\nDoc#s\nInvertedlists\nDocs\nperdoc\nTitle,URL,etc\nDoc#s\nDocument meta data\nTerms\nQuerystring\nTerm#s\nFigure 1: An abstraction of some of the sub-systems\nin a search engine. Depending on the number of\ndocuments indexed, each sub-system could reside on\na single machine, be distributed across thousands of\nmachines, or a combination of both.\nsub-system could be distributed over many machines, or all\noccupy a single server and filesystem, competing with each\nother for resources. Similarly, it may be more efficient to\ncombine some sub-systems in an implementation of the \ndiagram. For example, the meta-data such as document title\nand URL requires minimal computation apart from \nhighlighting query words, but we note that disk seeking is likely\nto be minimized if title, URL and fixed summary \ninformation is stored contiguously with the text from which query\nbiased summaries are extracted. Here we ignore the fixed\ntext and consider only the generation of query biased \nsummaries: we concentrate on the Snippet Engine.\nIn addition to data and programs operating on that data,\neach sub-system also has its own memory management scheme.\nThe memory management system may simply be the \nmemory hierarchy provided by the operating system used on \nmachines in the sub-system, or it may be explicitly coded to\noptimise the processes in the sub-system.\nThere are many papers on caching in search engines (see\n[3] and references therein for a current summary), but it\nseems reasonable that there is a query cache in the Query\nEngine that stores precomputed final result pages for very\npopular queries. When one of the popular queries is issued,\nthe result page is fetched straight from the query cache. If\nthe issued query is not in the query cache, then the Query\nEngine uses the four sub-systems in turn to assemble a \nresults page.\n1. The Lexicon Engine maps query terms to integers.\n2. The Ranking Engine retrieves inverted lists for each\nterm, using them to get a ranked list of documents.\n3. The Snippet Engine uses those document numbers and\nquery term numbers to generate snippets.\n4. The Meta Data Engine fetches other information about\neach document to construct the results page.\nIN A document broken into one sentence per line,\nand a sequence of query terms.\n1 For each line of the text, L = [w1, w2, . . . , wm]\n2 Let h be 1 if L is a heading, 0 otherwise.\n3 Let be 2 if L is the first line of a document,\n1 if it is the second line, 0 otherwise.\n4 Let c be the number of wi that are query\nterms, counting repetitions.\n5 Let d be the number of distinct query terms\nthat match some wi.\n6 Identify the longest contiguous run of query\nterms in L, say wj . . . wj+k.\n7 Use a weighted combination of c, d, k, h\nand to derive a score s.\n8 Insert L into a max-heap using s as the key.\nOUT Remove the number of sentences required from\nthe heap to form the summary.\nFigure 2: Simple sentence ranker that operates on\nraw text with one sentence per line.\n4. THE SNIPPET ENGINE\nFor each document identifier passed to the Snippet \nEngine, the engine must generate text, preferably containing\nquery terms, that attempts to summarize that document.\nPrevious work on summarization identifies the sentence as\nthe minimal unit for extraction and presentation to the\nuser [12]. Accordingly, we also assume a web snippet \nextraction process will extract sentences from documents. In order\nto construct a snippet, all sentences in a document should be\nranked against the query, and the top two or three returned\nas the snippet. The scoring of sentences against queries has\nbeen explored in several papers [7, 12, 18, 20, 21], with \ndifferent features of sentences deemed important.\nBased on these observations, Figure 2, shows the general\nalgorithm for scoring sentences in relevant documents, with\nthe highest scoring sentences making the snippet for each\ndocument. The final score of a sentence, assigned in Step 7,\ncan be derived in many different ways. In order to avoid\nbias towards any particular scoring mechanism, we compare\nsentence quality later in the paper using the individual \ncomponents of the score, rather than an arbitrary combination\nof the components.\n4.1 Parsing Web Documents\nUnlike well edited text collections that are often the target\nfor summarization systems, Web data is often poorly \nstructured, poorly punctuated, and contains a lot of data that do\nnot form part of valid sentences that would be candidates\nfor parts of snippets.\nWe assume that the documents passed to the Snippet\nEngine by the Indexing Engine have all HTML tags and\nJavaScript removed, and that each document is reduced to a\nseries of word tokens separated by non-word tokens. We \ndefine a word token as a sequence of alphanumeric characters,\nwhile a non-word is a sequence of non-alphanumeric \ncharacters such as whitespace and the other punctuation symbols.\nBoth are limited to a maximum of 50 characters. Adjacent,\nrepeating characters are removed from the punctuation.\nIncluded in the punctuation set is a special end of sentence\nmarker which replaces the usual three sentence terminators\n?!.. Often these explicit punctuation characters are \nmissing, and so HTML tags such as <br> and <p> are assumed to\nterminate sentences. In addition, a sentence must contain at\nleast five words and no more than twenty words, with longer\nor shorter sentences being broken and joined as required to\nmeet these criteria [10].\nUnterminated HTML tags-that is, tags with an open\nbrace, but no close brace-cause all text from the open brace\nto the next open brace to be discarded.\nA major problem in summarizing web pages is the \npresence of large amounts of promotional and navigational \nmaterial (navbars) visually above and to the left of the actual\npage content. For example, The most wonderful company\non earth. Products. Service. About us. Contact us. Try\nbefore you buy. Similar, but often not identical, \nnavigational material is typically presented on every page within a\nsite. This material tends to lower the quality of summaries\nand slow down summary generation.\nIn our experiments we did not use any particular \nheuristics for removing navigational information as the test \ncollection in use (wt100g) pre-dates the widespread take up of\nthe current style of web publishing. In wt100g, the average\nweb page size is more than half the current Web average [11].\nAnecdotally, the increase is due to inclusion of sophisticated\nnavigational and interface elements and the JavaScript \nfunctions to support them.\nHaving defined the format of documents that are \npresented to the Snippet Engine, we now define our Compressed\nToken System (CTS) document storage scheme, and the\nbaseline system used for comparison.\n4.2 Baseline Snippet Engine\nAn obvious document representation scheme is to simply\ncompress each document with a well known adaptive \ncompressor, and then decompress the document as required [1],\nusing a string matching algorithm to effect the algorithm in\nFigure 2. Accordingly, we implemented such a system, using\nzlib [4] with default parameters to compress every document\nafter it has been parsed as in Section 4.1.\nEach document is stored in a single file. While \nmanageable for our small test collections or small enterprises with\nmillions of documents, a full Web search engine may require\nmultiple documents to inhabit single files, or a special \npurpose filesystem [6].\nFor snippet generation, the required documents are \ndecompressed one at a time, and a linear search for provided\nquery terms is employed. The search is optimized for our\nspecific task, which is restricted to matching whole words\nand the sentence terminating token, rather than general \npattern matching.\n4.3 The CTS Snippet Engine\nSeveral optimizations over the baseline are possible. The\nfirst is to employ a semi-static compression method over the\nentire document collection, which will allow faster \ndecompression with minimal compression loss [24]. Using a \nsemistatic approach involves mapping words and non-words \nproduced by the parser to single integer tokens, with frequent\nsymbols receiving small integers, and then choosing a coding\nscheme that assigns small numbers a small number of bits.\nWords and non-words strictly alternate in the compressed\nfile, which always begins with a word.\nIn this instance we simply assign each symbol its ordinal\nnumber in a list of symbols sorted by frequency. We use the\nvbyte coding scheme to code the word tokens [22]. The set\nof non-words is limited to the 64 most common punctuation\nsequences in the collection itself, and are encoded with a flat\n6-bit binary code. The remaining 2 bits of each punctuation\nsymbol are used to store capitalization information.\nThe process of computing the semi-static model is \ncomplicated by the fact that the number of words and non-words\nappearing in large web collections is high. If we stored all\nwords and non-words appearing in the collection, and their\nassociated frequency, many gigabytes of RAM or a B-tree or\nsimilar on-disk structure would be required [23]. Moffat et\nal. [14] have examined schemes for pruning models during\ncompression using large alphabets, and conclude that rarely\noccurring terms need not reside in the model. Rather, rare\nterms are spelt out in the final compressed file, using a \nspecial word token (escape symbol), to signal their occurrence.\nDuring the first pass of encoding, two move-to-front queues\nare kept; one for words and one for non-words. Whenever\nthe available memory is consumed and a new symbol is \ndiscovered in the collection, an existing symbol is discarded\nfrom the end of the queue. In our implementation, we \nenforce the stricter condition on eviction that, where possible,\nthe evicted symbol should have a frequency of one. If there is\nno symbol with frequency one in the last half of the queue,\nthen we evict symbols of frequency two, and so on until\nenough space is available in the model for the new symbol.\nThe second pass of encoding replaces each word with its\nvbyte encoded number, or the escape symbol and an ASCII\nrepresentation of the word if it is not in the model. \nSimilarly each non-word sequence is replaced with its codeword,\nor the codeword for a single space character if it is not in the\nmodel. We note that this lossless compression of non-words\nis acceptable when the documents are used for snippet \ngeneration, but may not be acceptable for a document database.\nWe assume that a separate sub-system would hold cached\ndocuments for other purposes where exact punctuation is\nimportant.\nWhile this semi-static scheme should allow faster \ndecompression than the baseline, it also readily allows direct \nmatching of query terms as compressed integers in the compressed\nfile. That is, sentences can be scored without having to \ndecompress a document, and only the sentences returned as\npart of a snippet need to be decoded.\nThe CTS system stores all documents contiguously in one\nfile, and an auxiliary table of 64 bit integers indicating the\nstart offset of each document in the file. Further, it must\nhave access to the reverse mapping of term numbers, \nallowing those words not spelt out in the document to be \nrecovered and returned to the Query Engine as strings. The first\nof these data structures can be readily partitioned and \ndistributed if the Snippet Engine occupies multiple machines;\nthe second, however, is not so easily partitioned, as any\ndocument on a remote machine might require access to the\nwhole integer-to-string mapping. This is the second reason\nfor employing the model pruning step during construction of\nthe semi-static code: it limits the size of the reverse mapping\ntable that should be present on every machine implementing\nthe Snippet Engine.\n4.4 Experimental assessment of CTS\nAll experiments reported in this paper were run on a Sun\nFire V210 Server running Solaris 10. The machine consists\nof dual 1.34 GHz UltraSPARC IIIi processors and 4GB of\nwt10g wt50g wt100g\nNo. Docs. (\u00c3\u2014106\n) 1.7 10.1 18.5\nRaw Text 10, 522 56, 684 102, 833\nBaseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%)\nCTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%)\nTable 1: Total storage space (Mb) for documents\nfor the three test collections both compressed, and\nuncompressed.\n0 20 40 60\n0.00.20.40.60.8 Queries grouped in 100\"s\nTime(seconds) 0 20 40 60\n0.00.20.40.60.8 Queries grouped in 100\"s\nTime(seconds) 0 20 40 60\n0.00.20.40.60.8 Queries grouped in 100\"s\nTime(seconds)\nBaseline\nCTS with caching\nCTS without caching\nFigure 3: Time to generate snippets for 10 \ndocuments per query, averaged over buckets of 100\nqueries, for the first 7000 Excite queries on wt10g.\nRAM. All source code was compiled using gcc4.1.1 with -O9\noptimisation. Timings were run on an otherwise \nunoccupied machine and were averaged over 10 runs, with memory\nflushed between runs to eliminate any caching of data files.\nIn the absence of evidence to the contrary, we assume that\nit is important to model realistic query arrival sequences and\nthe distribution of query repetitions for our experiments.\nConsequently, test collections which lack real query logs,\nsuch as TREC ad-hoc and .GOV2 were not considered \nsuitable. Obtaining extensive query logs and associated result\ndoc-ids for a corresponding large collection is not easy. We\nhave used two collections (wt10g and wt100g) from the\nTREC Web Track [8] coupled with queries from Excite logs\nfrom the same (c. 1997) period. Further, we also made use\nof a medium sized collection wt50g, obtained by randomly\nsampling half of the documents from wt100g. The first two\nrows of Table 1 give the number of documents and the size\nin Mb of these collections.\nThe final two rows of Table 1 show the size of the resulting\ndocument sets after compression with the baseline and CTS\nschemes. As expected, CTS admits a small compression\nloss over zlib, but both substantially reduce the size of the\ntext to about 20% of the original, uncompressed size. Note\nthat the figures for CTS do not include the reverse mapping\nfrom integer token to string that is required to produce the\nfinal snippets as that occupies RAM. It is 1024 Mb in these\nexperiments.\nThe Zettair search engine [25] was used to produce a list\nof documents to summarize for each query. For the majority\nof the experiments the Okapi BM25 scoring scheme was used\nto determine document rankings. For the static caching \nexperiments reported in Section 5, the score of each document\nwt10g wt50g wt100g\nBaseline 75 157 183\nCTS 38 70 77\nReduction in time 49% 56% 58%\nTable 2: Average time (msec) for the final 7000\nqueries in the Excite logs using the baseline and CTS\nsystems on the 3 test collections.\nis a 50:50 weighted average of the BM25 score (normalized\nby the top scoring document for each query) and a score for\neach document independent of any query. This is to \nsimulate effects of ranking algorithms like PageRank [1] on the\ndistribution of document requests to the Snippet Engine. In\nour case we used the normalized Access Count [5] computed\nfrom the top 20 documents returned to the first 1 million\nqueries from the Excite log to determine the query \nindependent score component.\nPoints on Figure 3 indicate the mean running time to\ngenerate 10 snippets for each query, averaged in groups of\n100 queries, for the first 7000 queries in the Excite query\nlog. Only the data for wt10g is shown, but the other \ncollections showed similar patterns. The x-axis indicates the\ngroup of 100 queries; for example, 20 indicates the queries\n2001 to 2100. Clearly there is a caching effect, with times\ndropping substantially after the first 1000 or so queries are\nprocessed. All of this is due to the operating system caching\ndisk blocks and perhaps pre-fetching data ahead of specific\nread requests. This is evident because the baseline system\nhas no large internal data structures to take advantage of\nnon-disk based caching, it simply opens and processes files,\nand the speed up is evident for the baseline system.\nPart of this gain is due to the spatial locality of disk \nreferences generated by the query stream: repeated queries\nwill already have their document files cached in memory;\nand similarly different queries that return the same \ndocuments will benefit from document caching. But when the\nlog is processed after removing all but the first request for\neach document, the pronounced speed-up is still evident as\nmore queries are processed (not shown in figure). This \nsuggests that the operating system (or the disk itself) is reading\nand buffering a larger amount of data than the amount \nrequested and that this brings benefit often enough to make\nan appreciable difference in snippet generation times. This\nis confirmed by the curve labeled CTS without caching,\nwhich was generated after mounting the filesystem with a\nno-caching option (directio in Solaris). With disk caching\nturned off, the average time to generate snippets varies little.\nThe time to generate ten snippets for a query, averaged\nover the final 7000 queries in the Excite log as caching effects\nhave dissipated, are shown in Table 2. Once the system has\nstabilized, CTS is over 50% faster than the Baseline \nsystem. This is primarily due to CTS matching single integers\nfor most query words, rather than comparing strings in the\nbaseline system.\nTable 3 shows a break down of the average time to \ngenerate ten snippets over the final 7000 queries of the \nExcite log on the wt50g collection when entire documents are\nprocessed, and when only the first half of each document\nis processed. As can be seen, the majority of time spent\ngenerating a snippet is in locating the document on disk\n(Seek): 64% for whole documents, and 75% for half \ndocuments. Even if the amount of processing a document must\n% of doc processed Seek Read Score & Decode\n100% 45 4 21\n50% 45 4 11\nTable 3: Time to generate 10 snippets for a single\nquery (msec) for the wt50g collection averaged over\nthe final 7000 Excite queries when either all of each\ndocument is processed (100%) or just the first half\nof each document (50%).\nundergo is halved, as in the second row of the Table, there is\nonly a 14% reduction in the total time required to generate\na snippet. As locating documents in secondary storage \noccupies such a large proportion of snippet generation time, it\nseems logical to try and reduce its impact through caching.\n5. DOCUMENT CACHING\nIn Section 3 we observed that the Snippet Engine would\nhave its own RAM in proportion to the size of the \ndocument collection. For example, on a whole-of-Web search\nengine, the Snippet Engine would be distributed over many\nworkstations, each with at least 4 Gb of RAM. In a small\nenterprise, the Snippet Engine may be sharing RAM with\nall other sub-systems on a single workstation, hence only\nhave 100 Mb available. In this section we use simulation to\nmeasure the number of cache hits in the Snippet Engine as\nmemory size varies.\nWe compare two caching policies: a static cache, where\nthe cache is loaded with as many documents as it can hold\nbefore the system begins answering queries, and then never\nchanges; and a least-recently-used cache, which starts out as\nfor the static cache, but whenever a document is accessed it\nmoves to the front of a queue, and if a document is fetched\nfrom disk, the last item in the queue is evicted. Note that\ndocuments are first loaded into the caches in order of \ndecreasing query independent score, which is computed as \ndescribed in Section 4.4.\nThe simulations also assume a query cache exists for the\ntop Q most frequent queries, and that these queries are never\nprocessed by the Snippet Engine.\nAll queries passed into the simulations are from the second\nhalf of the Excite query log (the first half being used to \ncompute query independent scores), and are stemmed, stopped,\nand have their terms sorted alphabetically. This final \nalteration simply allows queries such as red dog and dog\nred to return the same documents, as would be the case\nin a search engine where explicit phrase operators would be\nrequired in the query to enforce term order and proximity.\nFigure 4 shows the percentage of document access that\nhit cache using the two caching schemes, with Q either 0\nor 10,000, on 535,276 Excite queries on wt100g. The \nxaxis shows the percentage of documents that are held in the\ncache, so 1.0% corresponds to about 185,000 documents.\nFrom this figure it is clear that caching even a small \npercentage of the documents has a large impact on reducing\nseek time for snippet generation. With 1% of documents\ncached, about 222 Mb for the wt100g collection, around\n80% of disk seeks are avoided. The static cache performs\nsurprisingly well (squares in Figure 4), but is outperformed\nby the LRU cache (circles). In an actual implementation of\nLRU, however, there may be fragmentation of the cache as\ndocuments are swapped in and out.\nThe reason for the large impact of the document cache is\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\n020406080100\nCache size (% of collection)\n%ofaccessesascachehits\nLRU Q=0\nLRU Q=10,000\nStatic Q=0\nStatic Q=10,000\nFigure 4: Percentage of the time that the Snippet\nEngine does not have to go to disk in order to \ngenerate a snippet plotted against the size of the \ndocument cache as a percentage of all documents in the\ncollection. Results are from a simulation on wt100g\nwith 535,276 Excite queries.\nthat, for a particular collection, some documents are much\nmore likely to appear in results lists than others. This effect\noccurs partly because of the approximately Zipfian query\nfrequency distribution, and partly because most Web search\nengines employ ranking methods which combine query based\nscores with static (a priori) scores determined from factors\nsuch as link graph measures, URL features, spam scores and\nso on [17]. Documents with high static scores are much more\nlikely to be retrieved than others.\nIn addition to the document cache, the RAM of the \nSnippet Engine must also hold the CTS decoding table that\nmaps integers to strings, which is capped by a parameter at\ncompression time (1 Gb in our experiments here). This is\nmore than compensated for by the reduced size of each \ndocument, allowing more documents into the document cache.\nFor example, using CTS reduces the average document size\nfrom 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM\ncould hold 368,442 uncompressed documents (2% of the \ncollection), or 850,691 documents plus a 1 Gb decompression\ntable (5% of the collection).\nIn fact, further experimentation with the model size \nreveals that the model can in fact be very small and still CTS\ngives good compression and fast scoring times. This is \nevidenced in Figure 5, where the compressed size of wt50g is\nshown in the solid symbols. Note that when no compression\nis used (Model Size is 0Mb), the collection is only 31 Gb as\nHTML markup, JavaScript, and repeated punctuation has\nbeen discarded as described in Section 4.1. With a 5 Mb\nmodel, the collection size drops by more than half to 14 Gb,\nand increasing the model size to 750 Mb only elicits a 2 Gb\ndrop in the collection size. Figure 5 also shows the average\ntime to score and decode a a snippet (excluding seek time)\nwith the different model sizes (open symbols). Again, there\nis a large speed up when a 5 Mb model is used, but little\n0 200 400 600\n15202530\nModel Size (Mb)\nCollectionSize(Gb)orTime(msec)\nSize (Gb)\nTime (msec)\nFigure 5: Collection size of the wt50g collection\nwhen compressed with CTS using different memory\nlimits on the model, and the average time to \ngenerate single snippet excluding seek time on 20000\nExcite queries using those models.\nimprovement with larger models. Similar results hold for\nthe wt100g collection, where a model of about 10 Mb \noffers substantial space and time savings over no model at all,\nbut returns diminish as the model size increases.\nApart from compression, there is another approach to \nreducing the size of each document in the cache: do not store\nthe full document in cache. Rather store sentences that are\nlikely to be used in snippets in the cache, and if during \nsnippet generation on a cached document the sentence scores do\nnot reach a certain threshold, then retrieve the whole \ndocument from disk. This raises questions on how to choose\nsentences from documents to put in cache, and which to\nleave on disk, which we address in the next section.\n6. SENTENCE REORDERING\nSentences within each document can be re-ordered so that\nsentences that are very likely to appear in snippets are at the\nfront of the document, hence processed first at query time,\nwhile less likely sentences are relegated to the rear of the\ndocument. Then, during query time, if k sentences with a\nscore exceeding some threshold are found before the entire\ndocument is processed, the remainder of the document is\nignored. Further, to improve caching, only the head of each\ndocument can be stored in the cache, with the tail residing\non disk. Note that we assume that the search engine is to\nprovide cached copies of a document-that is, the exact\ntext of the document as it was indexed-then this would be\nserviced by another sub-system in Figure 1, and not from\nthe altered copy we store in the Snippet Engine.\nWe now introduce four sentence reordering approaches.\n1. Natural order The first few sentences of a well authored\ndocument usually best describe the document content [12].\nThus simply processing a document in order should yield a\nquality snippet. Unfortunately, however, web documents are\noften not well authored, with little editorial or professional\nwriting skills brought to bear on the creation of a work of\nliterary merit. More importantly, perhaps, is that we are\nproducing query-biased snippets, and there is no guarantee\nthat query terms will appear in sentences towards the front\nof a document.\n2. Significant terms (ST) Luhn introduced the concept\nof a significant sentence as containing a cluster of \nsignificant terms [12], a concept found to work well by Tombros\nand Sanderson [20]. Let fd,t be the frequency of term t in\ndocument d, then term t is determined to be significant if\nfd,t \u00e2\u2030\u00a5\n8\n<\n:\n7 \u00e2\u02c6\u2019 0.1 \u00c3\u2014 (25 \u00e2\u02c6\u2019 sd), if sd < 25\n7, if 25 \u00e2\u2030\u00a4 sd \u00e2\u2030\u00a4 40\n7 + 0.1 \u00c3\u2014 (sd \u00e2\u02c6\u2019 40), otherwise,\nwhere sd is the number of sentences in document d. A \nbracketed section is defined as a group of terms where the leftmost\nand rightmost terms are significant terms, and no significant\nterms in the bracketed section are divided by more than four\nnon-significant terms. The score of a bracketed section is\nthe square of the number of significant words falling in the\nsection, divided by the total number of words in the entire\nsentence. The a priori score for a sentence is computed as\nthe maximum of all scores for the bracketed sections of the\nsentence. We then sort the sentences by this score.\n3. Query log based (QLt) Many Web queries repeat,\nand a small number of queries make up a large volume of\ntotal searches [9]. In order to take advantage of this bias,\nsentences that contain many past query terms should be\npromoted to the front of a document, while sentences that\ncontain few query terms should be demoted. In this scheme,\nthe sentences are sorted by the number of sentence terms\nthat occur in the query log. To ensure that long sentences do\nnot dominate over shorter qualitative sentences the score \nassigned to each sentence is divided by the number of terms in\nthat sentence giving each sentence a score between 0 and 1.\n4. Query log based (QLu) This scheme is as for QLt,\nbut repeated terms in the sentence are only counted once.\nBy re-ordering sentences using schemes ST, QLt or QLu,\nwe aim to terminate snippet generation earlier than if \nNatural Order is used, but still produce sentences with the same\nnumber of unique query terms (d in Figure 2), total number\nof query terms (c), the same positional score (h+ ) and the\nsame maximum span (k). Accordingly, we conducted \nexperiments comparing the methods, the first 80% of the Excite\nquery log was used to reorder sentences when required, and\nthe final 20% for testing.\nFigure 6 shows the differences in snippet scoring \ncomponents using each of the three methods over the Natural\nOrder method. It is clear that sorting sentences using the\nSignificant Terms (ST) method leads to the smallest change\nin the sentence scoring components. The greatest change\nover all methods is in the sentence position (h + ) \ncomponent of the score, which is to be expected as their is no\nguarantee that leading and heading sentences are processed\nat all after sentences are re-ordered. The second most \naffected component is the number of distinct query terms in a\nreturned sentence, but if only the first 50% of the document\nis processed with the ST method, there is a drop of only 8%\nin the number of distinct query terms found in snippets.\nDepending how these various components are weighted to\ncompute an overall snippet score, one can argue that there\nis little overall affect on scores when processing only half the\ndocument using the ST method.\nSpan (k)\nTerm Count (c)\nSentence Position (h + l)\nDistinct Terms (d)\n40%\n50%\n60%\n70%\nST\nQLt\nQLu\nST\nQLt\nQLu\nST\nQLt\nQLu\nST\nQLt\nQLu\nST\nQLt\nQLu\nRelativedifferencetoNaturalOrder\nDocuments size used\n90% 80% 70% 60% 50%\n0%\n10%\n20%\n30%\nFigure 6: Relative difference in the snippet score\ncomponents compared to Natural Ordered \ndocuments when the amount of documents processed is\nreduced, and the sentences in the document are \nreordered using Query Logs (QLt, QLu) or Significant\nTerms (ST).\n7. DISCUSSION\nIn this paper we have described the algorithms and \ncompression scheme that would make a good Snippet Engine\nsub-system for generating text snippets of the type shown on\nthe results pages of well known Web search engines. Our \nexperiments not only show that our scheme is over 50% faster\nthan the obvious baseline, but also reveal some very \nimportant aspects of the snippet generation problem. Primarily,\ncaching documents avoids seek costs to secondary memory\nfor each document that is to be summarized, and is vital for\nfast snippet generation. Our caching simulations show that\nif as little as 1% of the documents can be cached in RAM as\npart of the Snippet Engine, possibly distributed over many\nmachines, then around 75% of seeks can be avoided. Our\nsecond major result is that keeping only half of each \ndocument in RAM, effectively doubling the cache size, has little\naffect on the quality of the final snippets generated from\nthose half-documents, provided that the sentences that are\nkept in memory are chosen using the Significant Term \nalgorithm of Luhn [12]. Both our document compression and\ncompaction schemes dramatically reduce the time taken to\ngenerate snippets.\nNote that these results are generated using a 100Gb \nsubset of the Web, and the Excite query log gathered from the\nsame period as that subset was created. We are assuming, as\nthere is no evidence to the contrary, that this collection and\nlog is representative of search engine input in other domains.\nIn particular, we can scale our results to examine what \nresources would be required, using our scheme, to provide a\nSnippet Engine for the entire World Wide Web.\nWe will assume that the Snippet Engine is distributed\nacross M machines, and that there are N web pages in the\ncollection to be indexed and served by the search engine. We\nalso assume a balanced load for each machine, so each \nmachine serves about N/M documents, which is easily achieved\nin practice. Each machine, therefore, requires RAM to hold\nthe following.\n\u00e2\u20ac\u00a2 The CTS model, which should be 1/1000 of the size\nof the uncompressed collection (using results in \nFigure 5 and Williams et al. [23]). Assuming an average\nuncompressed document size of 8 Kb [11], this would\nrequire N/M \u00c3\u2014 8.192 bytes of memory.\n\u00e2\u20ac\u00a2 A cache of 1% of all N/M documents. Each document\nrequires 2 Kb when compressed with CTS (Table 1),\nand only half of each document is required using ST\nsentence reordering, requiring a total of N/M \u00c3\u20140.01\u00c3\u2014\n1024 bytes.\n\u00e2\u20ac\u00a2 The offset array that gives the start position of each\ndocument in the single, compressed file: 8 bytes per\nN/M documents.\nThe total amount of RAM required by a single machine,\ntherefore, would be N/M(8.192 + 10.24 + 8) bytes. \nAssuming that each machine has 8 Gb of RAM, and that there are\n20 billion pages to index on the Web, a total of M = 62 \nmachines would be required for the Snippet Engine. Of course\nin practice, more machines may be required to manage the\ndistributed system, to provide backup services for failed\nmachines, and other networking services. These machines\nwould also need access to 37 Tb of disk to store the \ncompressed document representations that were not in cache.\nIn this work we have deliberately avoided committing to\none particular scoring method for sentences in documents.\nRather, we have reported accuracy results in terms of the\nfour components that have been previously shown to be\nimportant in determining useful snippets [20]. The CTS\nmethod can incorporate any new metrics that may arise in\nthe future that are calculated on whole words. The \ndocument compaction techniques using sentence re-ordering,\nhowever, remove the spatial relationship between sentences,\nand so if a scoring technique relies on the position of a \nsentence within a document, the aggressive compaction \ntechniques reported here cannot be used.\nA variation on the semi-static compression approach we\nhave adopted in this work has been used successfully in \nprevious search engine design [24], but there are alternate \ncompression schemes that allow direct matching in compressed\ntext (see Navarro and M\u00c2\u00a8akinen [15] for a recent survey.) As\nseek time dominates the snippet generation process, we have\nnot focused on this portion of the snippet generation in \ndetail in this paper. We will explore alternate compression\nschemes in future work.\nAcknowledgments\nThis work was supported in part by ARC Discovery Project\nDP0558916 (AT). Thanks to Nick Lester and Justin Zobel\nfor valuable discussions.\n8. REFERENCES\n[1] S. Brin and L. Page. The anatomy of a large-scale\nhypertextual Web search engine. In WWW7, pages\n107-117, 1998.\n[2] R. Fagin, Ravi K., K. S. McCurley, J. Novak,\nD. Sivakumar, J. A. Tomlin, and D. P. Williamson.\nSearching the workplace web. In WWW2003,\nBudapest, Hungary, May 2003.\n[3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\nBoosting the performance of web search engines:\nCaching and prefetching query results by exploiting\nhistorical usage data. ACM Trans. Inf. Syst.,\n24(1):51-78, 2006.\n[4] J-L Gailly and M. Adler. Zlib Compression Library.\nwww.zlib.net. Accessed January 2007.\n[5] S. Garcia, H.E. Williams, and A. Cannane.\nAccess-ordered indexes. In V. Estivill-Castro, editor,\nProc. Australasian Computer Science Conference,\npages 7-14, Dunedin, New Zealand, 2004.\n[6] S. Ghemawat, H. Gobioff, and S. Leung. The google\nfile system. In SOSP \"03: Proc. of the 19th ACM\nSymposium on Operating Systems Principles, pages\n29-43, New York, NY, USA, 2003. ACM Press.\n[7] J. Goldstein, M. Kantrowitz, V. Mittal, and\nJ. Carbonell. Summarizing text documents: sentence\nselection and evaluation metrics. In SIGIR99, pages\n121-128, 1999.\n[8] D. Hawking, Nick C., and Paul Thistlewaite.\nOverview of TREC-7 Very Large Collection Track. In\nProc. of TREC-7, pages 91-104, November 1998.\n[9] B. J. Jansen, A. Spink, and J. Pedersen. A temporal\ncomparison of altavista web searching. J. Am. Soc.\nInf. Sci. Tech. (JASIST), 56(6):559-570, April 2005.\n[10] J. Kupiec, J. Pedersen, and F. Chen. A trainable\ndocument summarizer. In SIGIR95, pages 68-73, 1995.\n[11] S. Lawrence and C.L. Giles. Accessibility of\ninformation on the web. Nature, 400:107-109, July\n1999.\n[12] H.P. Luhn. The automatic creation of literature\nabstracts. IBM Journal, pages 159-165, April 1958.\n[13] I. Mani. Automatic Summarization, volume 3 of\nNatural Language Processing. John Benjamins\nPublishing Company, Amsterdam/Philadelphia, 2001.\n[14] A. Moffat, J. Zobel, and N. Sharman. Text\ncompression for dynamic document databases.\nKnowledge and Data Engineering, 9(2):302-313, 1997.\n[15] G. Navarro and V. M\u00c2\u00a8akinen. Compressed full text\nindexes. ACM Computing Surveys, 2007. To appear.\n[16] D. R. Radev, E. Hovy, and K. McKeown. Introduction\nto the special issue on summarization. Comput.\nLinguist., 28(4):399-408, 2002.\n[17] M. Richardson, A. Prakash, and E. Brill. Beyond\npagerank: machine learning for static ranking. In\nWWW06, pages 707-715, 2006.\n[18] T. Sakai and K. Sparck-Jones. Generic summaries for\nindexing in information retrieval. In SIGIR01, pages\n190-198, 2001.\n[19] H. G. Silber and K. F. McCoy. Efficiently computed\nlexical chains as an intermediate representation for\nautomatic text summarization. Comput. Linguist.,\n28(4):487-496, 2002.\n[20] A. Tombros and M. Sanderson. Advantages of query\nbiased summaries in information retrieval. In\nSIGIR98, pages 2-10, Melbourne, Aust., August 1998.\n[21] R. W. White, I. Ruthven, and J. M. Jose. Finding\nrelevant documents using top ranking sentences: an\nevaluation of two alternative schemes. In SIGIR02,\npages 57-64, 2002.\n[22] H. E. Williams and J. Zobel. Compressing integers for\nfast file access. Comp. J., 42(3):193-201, 1999.\n[23] H.E. Williams and J. Zobel. Searchable words on the\nWeb. International Journal on Digital Libraries,\n5(2):99-105, April 2005.\n[24] I. H. Witten, A. Moffat, and T. C. Bell. Managing\nGigabytes: Compressing and Indexing Documents and\nImages. Morgan Kaufmann Publishing, San Francisco,\nsecond edition, May 1999.\n[25] The Zettair Search Engine.\nwww.seg.rmit.edu.au/zettair. Accessed January 2007.\n": ["search engine", "snippet generation", "document caching", "link graph measure", "performance", "web summary", "special-purpose filesystem", "ram", "document compaction", "text fragment", "precomputed final result page", "vbyte coding scheme", "semi-static compression", "document cache", ""]}