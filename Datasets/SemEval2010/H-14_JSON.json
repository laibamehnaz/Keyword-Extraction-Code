{"Studying the Use of Popular Destinations\nto Enhance Web Search Interaction\nRyen W. White\nMicrosoft Research\nOne Microsoft Way\nRedmond, WA 98052\nryenw@microsoft.com\nMikhail Bilenko\nMicrosoft Research\nOne Microsoft Way\nRedmond, WA 98052\nmbilenko@microsoft.com\nSilviu Cucerzan\nMicrosoft Research\nOne Microsoft Way\nRedmond, WA 98052\nsilviu@microsoft.com\nABSTRACT\nWe present a novel Web search interaction feature which, for a\ngiven query, provides links to websites frequently visited by other\nusers with similar information needs. These popular destinations\ncomplement traditional search results, allowing direct navigation to\nauthoritative resources for the query topic. Destinations are\nidentified using the history of search and browsing behavior of\nmany users over an extended time period, whose collective behavior\nprovides a basis for computing source authority. We describe a user\nstudy which compared the suggestion of destinations with the\npreviously proposed suggestion of related queries, as well as with\ntraditional, unaided Web search. Results show that search enhanced\nby destination suggestions outperforms other systems for\nexploratory tasks, with best performance obtained from mining past\nuser behavior at query-level granularity.\nCategories and Subject Descriptors\nH.3.3 [Information Storage and Retrieval]: Information Search\nand Retrieval - search process.\nGeneral Terms\nHuman Factors, Experimentation.\n1. INTRODUCTION\nThe problem of improving queries sent to Information Retrieval\n(IR) systems has been studied extensively in IR research [4][11].\nAlternative query formulations, known as query suggestions, can be\noffered to users following an initial query, allowing them to modify\nthe specification of their needs provided to the system, leading to\nimproved retrieval performance. Recent popularity of Web search\nengines has enabled query suggestions that draw upon the query\nreformulation behavior of many users to make query\nrecommendations based on previous user interactions [10].\nLeveraging the decision-making processes of many users for query\nreformulation has its roots in adaptive indexing [8]. In recent years,\napplying such techniques has become possible at a much larger\nscale and in a different context than what was proposed in early\nwork. However, interaction-based approaches to query suggestion\nmay be less potent when the information need is exploratory, since\na large proportion of user activity for such information needs may\noccur beyond search engine interactions. In cases where directed\nsearching is only a fraction of users\" information-seeking behavior,\nthe utility of other users\" clicks over the space of top-ranked results\nmay be limited, as it does not cover the subsequent browsing\nbehavior. At the same time, user navigation that follows search\nengine interactions provides implicit endorsement of Web resources\npreferred by users, which may be particularly valuable for\nexploratory search tasks. Thus, we propose exploiting a\ncombination of past searching and browsing user behavior to\nenhance users\" Web search interactions.\nBrowser plugins and proxy server logs provide access to the\nbrowsing patterns of users that transcend search engine interactions.\nIn previous work, such data have been used to improve search result\nranking by Agichtein et al. [1]. However, this approach only\nconsiders page visitation statistics independently of each other, not\ntaking into account the pages\" relative positions on post-query\nbrowsing paths. Radlinski and Joachims [13] have utilized such\ncollective user intelligence to improve retrieval accuracy by using\nsequences of consecutive query reformulations, yet their approach\ndoes not consider users\" interactions beyond the search result page.\nIn this paper, we present a user study of a technique that exploits the\nsearching and browsing behavior of many users to suggest popular\nWeb pages, referred to as destinations henceforth, in addition to the\nregular search results. The destinations may not be among the \ntopranked results, may not contain the queried terms, or may not even\nbe indexed by the search engine. Instead, they are pages at which\nother users end up frequently after submitting same or similar\nqueries and then browsing away from initially clicked search\nresults. We conjecture that destinations popular across a large\nnumber of users can capture the collective user experience for\ninformation needs, and our results support this hypothesis.\nIn prior work, O\"Day and Jeffries [12] identified teleportation as\nan information-seeking strategy employed by users jumping to their\npreviously-visited information targets, while Anderson et al. [2]\napplied similar principles to support the rapid navigation of Web\nsites on mobile devices. In [19], Wexelblat and Maes describe a\nsystem to support within-domain navigation based on the browse\ntrails of other users. However, we are not aware of such principles\nbeing applied to Web search. Research in the area of recommender\nsystems has also addressed similar issues, but in areas such as\nquestion-answering [9] and relatively small online communities\n[16]. Perhaps the nearest instantiation of teleportation is search\nengines\" offering of several within-domain shortcuts below the title\nof a search result. While these may be based on user behavior and\npossibly site structure, the user saves at most one click from this\nfeature. In contrast, our proposed approach can transport users to\nlocations many clicks beyond the search result, saving time and\ngiving them a broader perspective on the available related\ninformation.\nThe conducted user study investigates the effectiveness of including\nlinks to popular destinations as an additional interface feature on\nsearch engine result pages. We compare two variants of this\napproach against the suggestion of related queries and unaided Web\nsearch, and seek answers to questions on: (i) user preference and\nsearch effectiveness for known-item and exploratory search tasks,\nand (ii) the preferred distance between query and destination used to\nidentify popular destinations from past behavior logs. The results\nindicate that suggesting popular destinations to users attempting\nexploratory tasks provides best results in key aspects of the\ninformation-seeking experience, while providing query refinement\nsuggestions is most desirable for known-item tasks.\nThe remainder of the paper is structured as follows. In Section 2 we\ndescribe the extraction of search and browsing trails from user\nactivity logs, and their use in identifying top destinations for new\nqueries. Section 3 describes the design of the user study, while\nSections 4 and 5 present the study findings and their discussion,\nrespectively. We conclude in Section 6 with a summary.\n2. SEARCH TRAILS AND DESTINATIONS\nWe used Web activity logs containing searching and browsing\nactivity collected with permission from hundreds of thousands of\nusers over a five-month period between December 2005 and April\n2006. Each log entry included an anonymous user identifier, a\ntimestamp, a unique browser window identifier, and the URL of a\nvisited Web page. This information was sufficient to reconstruct\ntemporally ordered sequences of viewed pages that we refer to as\ntrails. In this section, we summarize the extraction of trails, their\nfeatures, and destinations (trail end-points). In-depth description\nand analysis of trail extraction are presented in [20].\n2.1 Trail Extraction\nFor each user, interaction logs were grouped based on browser\nidentifier information. Within each browser instance, participant\nnavigation was summarized as a path known as a browser trail,\nfrom the first to the last Web page visited in that browser. Located\nwithin some of these trails were search trails that originated with a\nquery submission to a commercial search engine such as Google,\nYahoo!, Windows Live Search, and Ask. It is these search trails\nthat we use to identify popular destinations.\nAfter originating with a query submission to a search engine, trails\nproceed until a point of termination where it is assumed that the\nuser has completed their information-seeking activity. Trails must\ncontain pages that are either: search result pages, search engine\nhomepages, or pages connected to a search result page via a\nsequence of clicked hyperlinks. Extracting search trails using this\nmethodology also goes some way toward handling multi-tasking,\nwhere users run multiple searches concurrently. Since users may\nopen a new browser window (or tab) for each task [18], each task\nhas its own browser trail, and a corresponding distinct search trail.\nTo reduce the amount of noise from pages unrelated to the active\nsearch task that may pollute our data, search trails are terminated\nwhen one of the following events occurs: (1) a user returns to their\nhomepage, checks e-mail, logs in to an online service (e.g.,\nMySpace or del.ico.us), types a URL or visits a bookmarked page;\n(2) a page is viewed for more than 30 minutes with no activity; (3)\nthe user closes the active browser window. If a page (at step i)\nmeets any of these criteria, the trail is assumed to terminate on the\nprevious page (i.e., step i - 1).\nThere are two types of search trails we consider: session trails and\nquery trails. Session trails transcend multiple queries and terminate\nonly when one of the three termination criteria above are satisfied.\nQuery trails use the same termination criteria as session trails, but\nalso terminate upon submission of a new query to a search engine.\nApproximately 14 million query trails and 4 million session trails\nwere extracted from the logs. We now describe some trail features.\n2.2 Trail and Destination Analysis\nTable 1 presents summary statistics for the query and session trails.\nDifferences in user interaction between the last domain on the trail\n(Domain n) and all domains visited earlier (Domains 1 to (n - 1))\nare particularly important, because they highlight the wealth of user\nbehavior data not captured by logs of search engine interactions.\nStatistics are averages for all trails with two or more steps (i.e.,\nthose trails where at least one search result was clicked).\nTable 1. Summary statistics (mean averages) for search trails.\nMeasure Query trails Session trails\nNumber of unique domains 2.0 4.3\nTotal page\nviews\nAll domains 4.8 16.2\nDomains 1 to (n - 1) 1.4 10.1\nDomain n (destination) 3.4 6.2\nTotal time\nspent (secs)\nAll domains 172.6 621.8\nDomains 1 to (n - 1) 70.4 397.6\nDomain n (destination) 102.3 224.1\nThe statistics suggest that users generally browse far from the\nsearch results page (i.e., around 5 steps), and visit a range of\ndomains during the course of their search. On average, users visit 2\nunique (non search-engine) domains per query trail, and just over 4\nunique domains per session trail. This suggests that users often do\nnot find all the information they seek on the first domain they visit.\nFor query trails, users also visit more pages, and spend significantly\nlonger, on the last domain in the trail compared to all previous\ndomains combined.1\nThese distinctions of the last domains in the\ntrails may indicate user interest, page utility, or page relevance.2\n2.3 Destination Prediction\nFor frequent queries, most popular destinations identified from Web\nactivity logs could be simply stored for future lookup at search time.\nHowever, we have found that over the six-month period covered by\nour dataset, 56.9% of queries are unique, and 97% queries occur 10\nor fewer times, accounting for 19.8% and 66.3% of all searches\nrespectively (these numbers are comparable to those reported in\nprevious studies of search engine query logs [15,17]). Therefore, a\nlookup-based approach would prevent us from reliably suggesting\ndestinations for a large fraction of searches. To overcome this\nproblem, we utilize a simple term-based prediction model.\nAs discussed above, we extract two types of destinations: query\ndestinations and session destinations. For both destination types,\nwe obtain a corpus of query-destination pairs and use it to construct\nterm-vector representation of destinations that is analogous to the\nclassic tf.idf document representation in traditional IR [14].\nThen, given a new query q consisting of k terms t1\u00e2\u20ac\u00a6tk, we identify\nhighest-scoring destinations using the following similarity function:\n1\nIndependent measures t-test: t(~60M) = 3.89, p < .001\n2\nThe topical relevance of the destinations was tested for a subset of around\nten thousand queries for which we had human judgments. The average\nrating of most of the destinations lay between good and excellent.\nVisual inspection of those that did not lie in this range revealed that many\nwere either relevant but had no judgments, or were related but had indirect\nquery association (e.g., petfooddirect.com for query [dogs]).\n,\n:\nWhere query and destination term weights, an\ncomputed using standard tf.idf weighting and que\nsession-normalized smoothed tf.idf weighting, respec\nexploring alternative algorithms for the destination p\nremains an interesting challenge for future work, resu\nstudy described in subsequent sections demonstrate th\napproach provides robust, effective results.\n3. STUDY\nTo examine the usefulness of destinations, we con\nstudy investigating the perceptions and performance\non four Web search systems, two with destination sug\n3.1 Systems\nFour systems were used in this study: a baseline Web\nwith no explicit support for query refinement (Base\nsystem with a query suggestion method that recomme\nqueries (QuerySuggestion), and two systems that aug\nWeb search with destination suggestions using either\nquery trails (QueryDestination), or end-points of\n(SessionDestination).\n3.1.1 System 1: Baseline\nTo establish baseline performance against which othe\nbe compared, we developed a masked interface to a p\nengine without additional support in formulating q\nsystem presented the user-constructed query to the\nand returned ten top-ranking documents retrieved by t\nremove potential bias that may have been caused by\nperceptions, we removed all identifying information\nengine logos and distinguishing interface features.\n3.1.2 System 2: QuerySuggestion\nIn addition to the basic search functionality offered\nQuerySuggestion provides suggestions about f\nrefinements that searchers can make following an\nsubmission. These suggestions are computed usin\nengine query log over the timeframe used for trail ge\neach target query, we retrieve two sets of candidate su\ncontain the target query as a substring. One set is com\nmost frequent such queries, while the second set cont\nfrequent queries that followed the target query in que\ncandidate query is then scored by multiplying its sm\nfrequency by its smoothed frequency of following th\nin past search sessions, using Laplacian smoothing. B\nscores, six top-ranked query suggestions are returned.\nsix suggestions are found, iterative backoff is per\nprogressively longer suffixes of the target query; a si\nis described in [10].\nSuggestions were offered in a box positioned on the t\nresult page, adjacent to the search results. Figure\nposition of the suggestions on the page. Figure 1b sh\nview of the portion of the results page containing th\noffered for the query [hubble telescope]. To the left o\nnd , are\nery- and \nuserctively. While\nprediction task\nults of the user\nhat this simple\nnducted a user\nof 36 subjects\nggestions.\nsearch system\nline), a search\nends additional\ngment baseline\nr end-points of\nsession trails\ner systems can\npopular search\nqueries. This\nsearch engine\nthe engine. To\nsubjects\" prior\nsuch as search\nd by Baseline,\nfurther query\nn initial query\nng the search\neneration. For\nuggestions that\nmposed of 100\ntains 100 most\nery logs. Each\nmoothed overall\nhe target query\nBased on these\n. If fewer than\nrformed using\nimilar strategy\ntop-right of the\n1a shows the\nhows a zoomed\nhe suggestions\nof each query\n(a) Position of suggestions (b) Zoo\nFigure 1. Query suggestion presentation in\nsuggestion is an icon similar to a progress b\nnormalized popularity. Clicking a suggestion r\nresults for that query.\n3.1.3 System 3: QueryDestination\nQueryDestination uses an interface similar t\nHowever, instead of showing query refinemen\nquery, QueryDestination suggests up to six des\nvisited by other users who submitted queries s\none, and computed as described in the previous\nshows the position of the destination suggestio\npage. Figure 2b shows a zoomed view of the p\npage destinations suggested for the query [hubb\n(a) Position of destinations (b) Zoo\nFigure 2. Destination presentation in Que\nTo keep the interface uncluttered, the page title\nis shown on hover over the page URL (shown\nto the destination name, there is a clickable icon\nto execute a search for the current query wi\ndomain displayed. We show destinations as a\nthan increasing their search result rank, since\ndeviate from the original query (e.g., those\ntopics or not containing the original query terms\n3.1.4 System 4: SessionDestination\nThe interface functionality in SessionDestinat\nQueryDestination. The only difference between\nthe definition of trail end-points for queries use\ndestinations. QueryDestination directs users to\nend up at for the active or similar que\nSessionDestination directs users to the domains\nthe end of the search session that follows th\nqueries. This downgrades the effect of multi\n(i.e., we only care where users end up after sub\nrather than directing searchers to potentially irre\nmay precede a query reformulation.\n3.2 Research Questions\nWe were interested in determining the value of p\nTo do this we attempt to answer the following re\n3\nTo improve reliability, in a similar way to QueryS\nare only shown if their popularity exceeds a frequen\nmed suggestions\nQuerySuggestion.\nbar that encodes its\nretrieves new search\nto QuerySuggestion.\nnts for the submitted\nstinations frequently\nimilar to the current\ns section.3\nFigure 2a\nons on search results\nportion of the results\nle telescope].\nmed destinations\neryDestination.\ne of each destination\nin Figure 2b). Next\nn that allows the user\nithin the destination\na separate list, rather\nthey may topically\nfocusing on related\ns).\ntion is analogous to\nn the two systems is\ned in computing top\nthe domains others\nries. In contrast,\ns other users visit at\nhe active or similar\niple query iterations\nbmitting all queries),\nelevant domains that\npopular destinations.\nesearch questions:\nSuggestion, destinations\nncy threshold.\nRQ1: Are popular destinations preferable and more effective than\nquery refinement suggestions and unaided Web search for:\na. Searches that are well-defined (known-item tasks)?\nb. Searches that are ill-defined (exploratory tasks)?\nRQ2: Should popular destinations be taken from the end of query\ntrails or the end of session trails?\n3.3 Subjects\n36 subjects (26 males and 10 females) participated in our study.\nThey were recruited through an email announcement within our\norganization where they hold a range of positions in different\ndivisions. The average age of subjects was 34.9 years (max=62,\nmin=27, SD=6.2). All are familiar with Web search, and conduct\n7.5 searches per day on average (SD=4.1). Thirty-one subjects\n(86.1%) reported general awareness of the query refinements\noffered by commercial Web search engines.\n3.4 Tasks\nSince the search task may influence information-seeking behavior\n[4], we made task type an independent variable in the study. We\nconstructed six known-item tasks and six open-ended, exploratory\ntasks that were rotated between systems and subjects as described in\nthe next section. Figure 3 shows examples of the two task types.\nKnown-item task\nIdentify three tropical storms (hurricanes and typhoons) that have\ncaused property damage and/or loss of life.\nExploratory task\nYou are considering purchasing a Voice Over Internet Protocol\n(VoIP) telephone. You want to learn more about VoIP technology and\nproviders that offer the service, and select the provider and telephone\nthat best suits you.\nFigure 3. Examples of known-item and exploratory tasks.\nExploratory tasks were phrased as simulated work task situations\n[5], i.e., short search scenarios that were designed to reflect real-life\ninformation needs. These tasks generally required subjects to\ngather background information on a topic or gather sufficient\ninformation to make an informed decision. The known-item search\ntasks required search for particular items of information (e.g.,\nactivities, discoveries, names) for which the target was \nwelldefined. A similar task classification has been used successfully in\nprevious work [21]. Tasks were taken and adapted from the Text\nRetrieval Conference (TREC) Interactive Track [7], and questions\nposed on question-answering communities (Yahoo! Answers,\nGoogle Answers, and Windows Live QnA). To motivate the\nsubjects during their searches, we allowed them to select two\nknown-item and two exploratory tasks at the beginning of the\nexperiment from the six possibilities for each category, before\nseeing any of the systems or having the study described to them.\nPrior to the experiment all tasks were pilot tested with a small\nnumber of different subjects to help ensure that they were\ncomparable in difficulty and selectability (i.e., the likelihood that\na task would be chosen given the alternatives). Post-hoc analysis of\nthe distribution of tasks selected by subjects during the full study\nshowed no preference for any task in either category.\n3.5 Design and Methodology\nThe study used a within-subjects experimental design. System had\nfour levels (corresponding to the four experimental systems) and\nsearch tasks had two levels (corresponding to the two task types).\nSystem and task-type order were counterbalanced according to a\nGraeco-Latin square design.\nSubjects were tested independently and each experimental session\nlasted for up to one hour. We adhered to the following procedure:\n1. Upon arrival, subjects were asked to select two known-item and\ntwo exploratory tasks from the six tasks of each type.\n2. Subjects were given an overview of the study in written form that\nwas read aloud to them by the experimenter.\n3. Subjects completed a demographic questionnaire focusing on\naspects of search experience.\n4. For each of the four interface conditions:\na. Subjects were given an explanation of interface functionality\nlasting around 2 minutes.\nb. Subjects were instructed to attempt the task on the assigned\nsystem searching the Web, and were allotted up to 10 minutes\nto do so.\nc. Upon completion of the task, subjects were asked to complete\na post-search questionnaire.\n5. After completing the tasks on the four systems, subjects answered\na final questionnaire comparing their experiences on the systems.\n6. Subjects were thanked and compensated.\nIn the next section we present the findings of this study.\n4. FINDINGS\nIn this section we use the data derived from the experiment to\naddress our hypotheses about query suggestions and destinations,\nproviding information on the effect of task type and topic\nfamiliarity where appropriate. Parametric statistical testing is used\nin this analysis and the level of significance is set to < 0.05,\nunless otherwise stated. All Likert scales and semantic differentials\nused a 5-point scale where a rating closer to one signifies more\nagreement with the attitude statement.\n4.1 Subject Perceptions\nIn this section we present findings on how subjects perceived the\nsystems that they used. Responses to post-search (per-system) and\nfinal questionnaires are used as the basis for our analysis.\n4.1.1 Search Process\nTo address the first research question wanted insight into subjects\"\nperceptions of the search experience on each of the four systems. In\nthe post-search questionnaires, we asked subjects to complete four\n5-point semantic differentials indicating their responses to the\nattitude statement: The search we asked you to perform was. The\npaired stimuli offered as responses were: relaxing/stressful,\ninteresting/ boring, restful/tiring, and easy/difficult.\nThe average obtained differential values are shown in Table 1 for\neach system and each task type. The value corresponding to the\ndifferential All represents the mean of all three differentials,\nproviding an overall measure of subjects\" feelings.\nTable 1. Perceptions of search process (lower = better).\nDifferential\nKnown-item Exploratory\nB QS QD SD B QS QD SD\nEasy 2.6 1.6 1.7 2.3 2.5 2.6 1.9 2.9\nRestful 2.8 2.3 2.4 2.6 2.8 2.8 2.4 2.8\nInteresting 2.4 2.2 1.7 2.2 2.2 1.8 1.8 2\nRelaxing 2.6 1.9 2 2.2 2.5 2.8 2.3 2.9\nAll 2.6 2 1.9 2.3 2.5 2.5 2.1 2.7\nEach cell in Table 1 summarizes subject responses for 18 \ntasksystem pairs (18 subjects who ran a known-item task on Baseline\n(B), 18 subjects who ran an exploratory task on QuerySuggestion\n(QS), etc.). The most positive response across all systems for each\ndifferential-task pair is shown in bold. We applied two-way\nanalysis of variance (ANOVA) to each differential across all four\nsystems and two task types. Subjects found the search easier on\nQuerySuggestion and QueryDestination than the other systems for\nknown-item tasks.4\nFor exploratory tasks, only searches conducted\non QueryDestination were easier than on the other systems.5\nSubjects indicated that exploratory tasks on the three non-baseline\nsystems were more stressful (i.e., less relaxing) than the \nknownitem tasks.6\nAs we will discuss in more detail in Section 4.1.3,\nsubjects regarded the familiarity of Baseline as a strength, and may\nhave struggled to attempt a more complex task while learning a new\ninterface feature such as query or destination suggestions.\n4.1.2 Interface Support\nWe solicited subjects\" opinions on the search support offered by\nQuerySuggestion, QueryDestination, and SessionDestination. The\nfollowing Likert scales and semantic differentials were used:\n\u00e2\u20ac\u00a2 Likert scale A: Using this system enhances my effectiveness in\nfinding relevant information. (Effectiveness)7\n\u00e2\u20ac\u00a2 Likert scale B: The queries/destinations suggested helped me\nget closer to my information goal. (CloseToGoal)\n\u00e2\u20ac\u00a2 Likert scale C: I would re-use the queries/destinations\nsuggested if I encountered a similar task in the future (Re-use)\n\u00e2\u20ac\u00a2 Semantic differential A: The queries/destinations suggested by\nthe system were: relevant/irrelevant, useful/useless,\nappropriate/inappropriate.\nWe did not include these in the post-search questionnaire when\nsubjects used the Baseline system as they refer to interface support\noptions that Baseline did not offer. Table 2 presents the average\nresponses for each of these scales and differentials, using the labels\nafter each of the first three Likert scales in the bulleted list above.\nThe values for the three semantic differentials are included at the\nbottom of the table, as is their overall average under All.\nTable 2. Perceptions of system support (lower = better).\nScale /\nDifferential\nKnown-item Exploratory\nQS QD SD QS QD SD\nEffectiveness 2.7 2.5 2.6 2.8 2.3 2.8\nCloseToGoal 2.9 2.7 2.8 2.7 2.2 3.1\nRe-use 2.9 3 2.4 2.5 2.5 3.2\n1 Relevant 2.6 2.5 2.8 2.4 2 3.1\n2 Useful 2.6 2.7 2.8 2.7 2.1 3.1\n3 Appropriate 2.6 2.4 2.5 2.4 2.4 2.6\nAll {1,2,3} 2.6 2.6 2.6 2.6 2.3 2.9\nThe results show that all three experimental systems improved\nsubjects\" perceptions of their search effectiveness over Baseline,\nalthough only QueryDestination did so significantly.8\nFurther\nexamination of the effect size (measured using Cohen\"s d) revealed\nthat QueryDestination affects search effectiveness most positively.9\nQueryDestination also appears to get subjects closer to their\ninformation goal (CloseToGoal) than QuerySuggestion or\n4\neasy: F(3,136) = 4.71, p = .0037; Tukey post-hoc tests: all p \u00e2\u2030\u00a4 .008\n5\neasy: F(3,136) = 3.93, p = .01; Tukey post-hoc tests: all p \u00e2\u2030\u00a4 .012\n6\nrelaxing: F(1,136) = 6.47, p = .011\n7\nThis question was conditioned on subjects\" use of Baseline and their\nprevious Web search experiences.\n8\nF(3,136) = 4.07, p = .008; Tukey post-hoc tests: all p \u00e2\u2030\u00a4 .002\n9\nQS: d(K,E) = (.26, .52); QD: d(K,E) = (.77, 1.50); SD: d(K,E) = (.48, .28)\nSessionDestination, although only for exploratory search tasks.10\nAdditional comments on QuerySuggestion conveyed that subjects\nsaw it as a convenience (to save them typing a reformulation) rather\nthan a way to dramatically influence the outcome of their search.\nFor exploratory searches, users benefited more from being pointed\nto alternative information sources than from suggestions for\niterative refinements of their queries. Our findings also show that\nour subjects felt that QueryDestination produced more relevant\nand useful suggestions for exploratory tasks than the other\nsystems.11\nAll other observed differences between the systems were\nnot statistically significant.12\nThe difference between performance\nof QueryDestination and SessionDestination is explained by the\napproach used to generate destinations (described in Section 2).\nSessionDestination\"s recommendations came from the end of users\"\nsession trails that often transcend multiple queries. This increases\nthe likelihood that topic shifts adversely affect their relevance.\n4.1.3 System Ranking\nIn the final questionnaire that followed completion of all tasks on\nall systems, subjects were asked to rank the four systems in\ndescending order based on their preferences. Table 3 presents the\nmean average rank assigned to each of the systems.\nTable 3. Relative ranking of systems (lower = better).\nSystems Baseline QSuggest QDest SDest\nRanking 2.47 2.14 1.92 2.31\nThese results indicate that subjects preferred QuerySuggestion and\nQueryDestination overall. However, none of the differences\nbetween systems\" ratings are significant.13\nOne possible explanation\nfor these systems being rated higher could be that although the\npopular destination systems performed well for exploratory\nsearches while QuerySuggestion performed well for known-item\nsearches, an overall ranking merges these two performances. This\nrelative ranking reflects subjects\" overall perceptions, but does not\nseparate them for each task category. Over all tasks there appeared\nto be a slight preference for QueryDestination, but as other results\nshow, the effect of task type on subjects\" perceptions is significant.\nThe final questionnaire also included open-ended questions that\nasked subjects to explain their system ranking, and describe what\nthey liked and disliked about each system:\nBaseline:\nSubjects who preferred Baseline commented on the familiarity of\nthe system (e.g., was familiar and I didn\"t end up using\nsuggestions (S36)). Those who did not prefer this system\ndisliked the lack of support for query formulation (Can be\ndifficult if you don\"t pick good search terms (S20)) and difficulty\nlocating relevant documents (e.g., Difficult to find what I was\nlooking for (S13); Clunky current technology (S30)).\nQuerySuggestion:\nSubjects who rated QuerySuggestion highest commented on rapid\nsupport for query formulation (e.g., was useful in (1) saving\ntyping (2) coming up with new ideas for query expansion (S12);\nhelps me better phrase the search term (S24); made my next\nquery easier (S21)). Those who did not prefer this system\ncriticized suggestion quality (e.g., Not relevant (S11); Popular\n10\nF(2,102) = 5.00, p = .009; Tukey post-hoc tests: all p \u00e2\u2030\u00a4 .012\n11\nF(2,102) = 4.01, p = .01; \u00ce\u00b1 = .0167\n12\nTukey post-hoc tests: all p \u00e2\u2030\u00a5 .143\n13\nOne-way repeated measures ANOVA: F(3,105) = 1.50, p = .22\nqueries weren\"t what I was looking for (S18)) and the quality of\nresults they led to (e.g., Results (after clicking on suggestions)\nwere of low quality (S35); Ultimately unhelpful (S1)).\nQueryDestination:\nSubjects who preferred this system commented mainly on support\nfor accessing new information sources (e.g., provided potentially\nhelpful and new areas / domains to look at (S27)) and bypassing\nthe need to browse to these pages (Useful to try to \u00e2\u20ac\u02dccut to the\nchase\" and go where others may have found answers to the topic\n(S3)). Those who did not prefer this system commented on the\nlack of specificity in the suggested domains (Should just link to\nsite-specific query, not site itself (S16); Sites were not very\nspecific (S24); Too general/vague (S28)14\n), and the quality of\nthe suggestions (Not relevant (S11); Irrelevant (S6)).\nSessionDestination:\nSubjects who preferred this system commented on the utility of\nthe suggested domains (suggestions make an awful lot of sense in\nproviding search assistance, and seemed to help very nicely\n(S5)). However, more subjects commented on the irrelevance of\nthe suggestions (e.g., did not seem reliable, not much help\n(S30); Irrelevant, not my style (S21), and the related need to\ninclude explanations about why the suggestions were offered (e.g.,\nLow-quality results, not enough information presented (S35)).\nThese comments demonstrate a diverse range of perspectives on\ndifferent aspects of the experimental systems. Work is obviously\nneeded in improving the quality of the suggestions in all systems,\nbut subjects seemed to distinguish the settings when each of these\nsystems may be useful. Even though all systems can at times offer\nirrelevant suggestions, subjects appeared to prefer having them\nrather than not (e.g., one subject remarked suggestions were\nhelpful in some cases and harmless in all (S15)).\n4.1.4 Summary\nThe findings obtained from our study on subjects\" perceptions of\nthe four systems indicate that subjects tend to prefer\nQueryDestination for the exploratory tasks and QuerySuggestion\nfor the known-item searches. Suggestions to incrementally refine\nthe current query may be preferred by searchers on known-item\ntasks when they may have just missed their information target.\nHowever, when the task is more demanding, searchers appreciate\nsuggestions that have the potential to dramatically influence the\ndirection of a search or greatly improve topic coverage.\n4.2 Search Tasks\nTo gain a better understanding of how subjects performed during\nthe study, we analyze data captured on their perceptions of task\ncompleteness and the time that it took them to complete each task.\n4.2.1 Subject Perceptions\nIn the post-search questionnaire, subjects were asked to indicate on\na 5-point Likert scale the extent to which they agreed with the\nfollowing attitude statement: I believe I have succeeded in my\nperformance of this task (Success). In addition, they were asked\nto complete three 5-point semantic differentials indicating their\nresponse to the attitude statement: The task we asked you to\nperform was: The paired stimuli offered as possible responses\nwere clear/unclear, simple/complex, and familiar/\nunfamiliar. Table 4 presents the mean average response to these\nstatements for each system and task type.\n14\nAlthough the destination systems provided support for search within a\ndomain, subjects mainly chose to ignore this.\nTable 4. Perceptions of task and task success (lower = better).\nScale\nKnown-item Exploratory\nB QS QD SD B QS QD SD\nSuccess 2.0 1.3 1.4 1.4 2.8 2.3 1.4 2.6\n1 Clear 1.2 1.1 1.1 1.1 1.6 1.5 1.5 1.6\n2 Simple 1.9 1.4 1.8 1.8 2.4 2.9 2.4 3\n3 Familiar 2.2 1.9 2.0 2.2 2.6 2.5 2.7 2.7\nAll {1,2,3} 1.8 1.4 1.6 1.8 2.2 2.2 2.2 2.3\nSubject responses demonstrate that users felt that their searches had\nbeen more successful using QueryDestination for exploratory tasks\nthan with the other three systems (i.e., there was a two-way\ninteraction between these two variables).15\nIn addition, subjects\nperceived a significantly greater sense of completion with \nknownitem tasks than with exploratory tasks.16\nSubjects also found\nknown-item tasks to be more simple, clear, and familiar. 17\nThese responses confirm differences in the nature of the tasks we\nhad envisaged when planning the study. As illustrated by the\nexamples in Figure 3, the known-item tasks required subjects to\nretrieve a finite set of answers (e.g., find three interesting things to\ndo during a weekend visit to Kyoto, Japan). In contrast, the\nexploratory tasks were multi-faceted, and required subjects to find\nout more about a topic or to find sufficient information to make a\ndecision. The end-point in such tasks was less well-defined and\nmay have affected subjects\" perceptions of when they had\ncompleted the task. Given that there was no difference in the tasks\nattempted on each system, theoretically the perception of the tasks\"\nsimplicity, clarity, and familiarity should have been the same for all\nsystems. However, we observe a clear interaction effect between\nthe system and subjects\" perception of the actual tasks.\n4.2.2 Task Completion Time\nIn addition to asking subjects to indicate the extent to which they\nfelt the task was completed, we also monitored the time that it took\nthem to indicate to the experimenter that they had finished. The\nelapsed time from when the subject began issuing their first query\nuntil when they indicated that they were done was monitored using\na stopwatch and recorded for later analysis. A stopwatch rather\nthan system logging was used for this since we wanted to record the\ntime regardless of system interactions. Figure 4 shows the average\ntask completion time for each system and each task type.\nFigure 4. Mean average task completion time (\u00c2\u00b1 SEM).\n15\nF(3,136) = 6.34, p = .001\n16\nF(1,136) = 18.95, p < .001\n17\nF(1,136) = 6.82, p = .028; Known-item tasks were also more simple on\nQS (F(3,136) = 3.93, p = .01; Tukey post-hoc test: p = .01); \u00ce\u00b1 = .167\nKnown-item Exploratory\n0\n100\n200\n300\n400\n500\n600\nTask categories\nBaseline\nQSuggest\nTime(seconds)\nSystems\n348.8\n513.7\n272.3\n467.8\n232.3\n474.2\n359.8\n472.2\nQDestination\nSDestination\nAs can be seen in the figure above, the task completion times for the\nknown-item tasks differ greatly between systems.18\nSubjects\nattempting these tasks on QueryDestination and QuerySuggestion\ncomplete them in less time than subjects on Baseline and\nSessionDestination.19\nAs discussed in the previous section, subjects\nwere more familiar with the known-item tasks, and felt they were\nsimpler and clearer. Baseline may have taken longer than the other\nsystems since users had no additional support and had to formulate\ntheir own queries. Subjects generally felt that the recommendations\noffered by SessionDestination were of low relevance and\nusefulness. Consequently, the completion time increased slightly\nbetween these two systems perhaps as the subjects assessed the\nvalue of the proposed suggestions, but reaped little benefit from\nthem. The task completion times for the exploratory tasks were\napproximately equal on all four systems20\n, although the time on\nBaseline was slightly higher. Since these tasks had no clearly\ndefined termination criteria (i.e., the subject decided when they had\ngathered sufficient information), subjects generally spent longer\nsearching, and consulted a broader range of information sources\nthan in the known-item tasks.\n4.2.3 Summary\nAnalysis of subjects\" perception of the search tasks and aspects of\ntask completion shows that the QuerySuggestion system made\nsubjects feel more successful (and the task more simple, clear,\nand familiar) for the known-item tasks. On the other hand,\nQueryDestination was shown to lead to heightened perceptions of\nsearch success and task ease, clarity, and familiarity for the\nexploratory tasks. Task completion times on both systems were\nsignificantly lower than on the other systems for known-item tasks.\n4.3 Subject Interaction\nWe now focus our analysis on the observed interactions between\nsearchers and systems. As well as eliciting feedback on each\nsystem from our subjects, we also recorded several aspects of their\ninteraction with each system in log files. In this section, we analyze\nthree interaction aspects: query iterations, search-result clicks, and\nsubject engagement with the additional interface features offered by\nthe three non-baseline systems.\n4.3.1 Queries and Result Clicks\nSearchers typically interact with search systems by submitting\nqueries and clicking on search results. Although our system offers\nadditional interface affordances, we begin this section by analyzing\nquerying and clickthrough behavior of our subjects to better\nunderstand how they conducted core search activities. Table 5\nshows the average number of query iterations and search results\nclicked for each system-task pair. The average value in each cell is\ncomputed for 18 subjects on each task type and system.\nTable 5. Average query iterations and result clicks (per task).\nScale\nKnown-item Exploratory\nB QS QD SD B QS QD SD\nQueries 1.9 4.2 1.5 2.4 3.1 5.7 2.7 3.5\nResult clicks 2.6 2 1.7 2.4 3.4 4.3 2.3 5.1\nSubjects submitted fewer queries and clicked on fewer search\nresults in QueryDestination than in any of the other systems.21\nAs\n18\nF(3,136) = 4.56, p = .004\n19\nTukey post-hoc tests: all p \u00e2\u2030\u00a4 .021\n20\nF(3,136) = 1.06, p = .37\n21\nQueries: F(3,443) = 3.99; p = .008; Tukey post-hoc tests: all p \u00e2\u2030\u00a4 .004;\nSystems: F(3,431) = 3.63, p = .013; Tukey post-hoc tests: all p \u00e2\u2030\u00a4 .011\ndiscussed in the previous section, subjects using this system felt\nmore successful in their searches yet they exhibited less of the\ntraditional query and result-click interactions required for search\nsuccess on traditional search systems. It may be the case that\nsubjects\" queries on this system were more effective, but it is more\nlikely that they interacted less with the system through these means\nand elected to use the popular destinations instead. Overall,\nsubjects submitted most queries in QuerySuggestion, which is not\nsurprising as this system actively encourages searchers to iteratively\nre-submit refined queries. Subjects interacted similarly with\nBaseline and SessionDestination systems, perhaps due to the low\nquality of the popular destinations in the latter. To investigate this\nand related issues, we will next analyze usage of the suggestions on\nthe three non-baseline systems.\n4.3.2 Suggestion Usage\nTo determine whether subjects found additional features useful, we\nmeasure the extent to which they were used when they were\nprovided. Suggestion usage is defined as the proportion of\nsubmitted queries for which suggestions were offered and at least\none suggestion was clicked. Table 6 shows the average usage for\neach system and task category.\nTable 6. Suggestion uptake (values are percentages).\nMeasure\nKnown-item Exploratory\nQS QD SD QS QD SD\nUsage 35.7 33.5 23.4 30.0 35.2 25.3\nResults indicate that QuerySuggestion was used more for \nknownitem tasks than SessionDestination22\n, and QueryDestination was\nused more than all other systems for the exploratory tasks.23\nFor\nwell-specified targets in known-item search, subjects appeared to\nuse query refinement most heavily. In contrast, when subjects were\nexploring, they seemed to benefit most from the recommendation of\nadditional information sources. Subjects selected almost twice as\nmany destinations per query when using QueryDestination\ncompared to SessionDestination.24\nAs discussed earlier, this may\nbe explained by the lower perceived relevance and usefulness of\ndestinations recommended by SessionDestination.\n4.3.3 Summary\nAnalysis of log interaction data gathered during the study indicates\nthat although subjects submitted fewer queries and clicked fewer\nsearch results on QueryDestination, their engagement with\nsuggestions was highest on this system, particularly for exploratory\nsearch tasks. The refined queries proposed by QuerySuggestion\nwere used the most for the known-item tasks. There appears to be a\nclear division between the systems: QuerySuggestion was preferred\nfor known-item tasks, while QueryDestination provided most-used\nsupport for exploratory tasks.\n5. DISCUSSION AND IMPLICATIONS\nThe promising findings of our study suggest that systems offering\npopular destinations lead to more successful and efficient searching\ncompared to query suggestion and unaided Web search.\nSubjects seemed to prefer QuerySuggestion for the known-item\ntasks where the information-seeking goal was well-defined. If the\ninitial query does not retrieve relevant information, then subjects\n22\nF(2,355) = 4.67, p = .01; Tukey post-hoc tests: p = .006\n23\nTukey\"s post-hoc tests: all p \u00e2\u2030\u00a4 .027\n24\nQD: MK = 1.8, ME = 2.1; SD: MK = 1.1, ME = 1.2; F(1,231) = 5.49, p =\n.02; Tukey post-hoc tests: all p \u00e2\u2030\u00a4 .003; (M represents mean average).\nappreciate support in deciding what refinements to make to the\nquery. From examination of the queries that subjects entered for the\nknown-item searches across all systems, they appeared to use the\ninitial query as a starting point, and add or subtract individual terms\ndepending on search results. The post-search questionnaire asked\nsubjects to select from a list of proposed explanations (or offer their\nown explanations) as to why they used recommended query\nrefinements. For both known-item tasks and the exploratory tasks,\naround 40% of subjects indicated that they selected a query\nsuggestion because they wanted to save time typing a query,\nwhile less than 10% of subjects did so because the suggestions\nrepresented new ideas. Thus, subjects seemed to view\nQuerySuggestion as a time-saving convenience, rather than a way to\ndramatically impact search effectiveness.\nThe two variants of recommending destinations that we considered,\nQueryDestination and SessionDestination, offered suggestions that\ndiffered in their temporal proximity to the current query. The\nquality of the destinations appeared to affect subjects\" perceptions\nof them and their task performance. As discussed earlier, domains\nresiding at the end of a complete search session (as in\nSessionDestination) are more likely to be unrelated to the current\nquery, and thus are less likely to constitute valuable suggestions.\nDestination systems, in particular QueryDestination, performed best\nfor the exploratory search tasks, where subjects may have benefited\nfrom exposure to additional information sources whose topical\nrelevance to the search query is indirect. As with QuerySuggestion,\nsubjects were asked to offer explanations for why they selected\ndestinations. Over both task types they suggested that destinations\nwere clicked because they grabbed their attention (40%),\nrepresented new ideas (25%), or users couldn\"t find what they\nwere looking for (20%). The least popular responses were\nwanted to save time typing the address (7%) and the destination\nwas popular (3%).\nThe positive response to destination suggestions from the study\nsubjects provides interesting directions for design refinements. We\nwere surprised to learn that subjects did not find the popularity bars\nuseful, or hardly used the within-site search functionality, inviting\nre-design of these components. Subjects also remarked that they\nwould like to see query-based summaries for each suggested\ndestination to support more informed selection, as well as\ncategorization of destinations with capability of drill-down for each\ncategory. Since QuerySuggestion and QueryDestination perform\nwell in distinct task scenarios, integrating both in a single system is\nan interesting future direction. We hope to deploy some of these\nideas on Web scale in future systems, which will allow log-based\nevaluation across large user pools.\n6. CONCLUSIONS\nWe presented a novel approach for enhancing users\" Web search\ninteraction by providing links to websites frequently visited by past\nsearchers with similar information needs. A user study was\nconducted in which we evaluated the effectiveness of the proposed\ntechnique compared with a query refinement system and unaided\nWeb search. Results of our study revealed that: (i) systems\nsuggesting query refinements were preferred for known-item tasks,\n(ii) systems offering popular destinations were preferred for\nexploratory search tasks, and (iii) destinations should be mined\nfrom the end of query trails, not session trails. Overall, popular\ndestination suggestions strategically influenced searches in a way\nnot achievable by query suggestion approaches by offering a new\nway to resolve information problems, and enhance the \ninformationseeking experience for many Web searchers.\n7. REFERENCES\n[1] Agichtein, E., Brill, E. & Dumais, S. (2006). Improving Web\nsearch ranking by incorporating user behavior information. In\nProc. SIGIR, 19-26.\n[2] Anderson, C. et al. (2001). Adaptive Web navigation for\nwireless devices. In Proc. IJCAI, 879-884.\n[3] Anick, P. (2003). Using terminological feedback for Web\nsearch refinement: A log-based study. In Proc. SIGIR, 88-95.\n[4] Beaulieu, M. (1997). Experiments with interfaces to support\nquery expansion. J. Doc. 53, 1, 8-19.\n[5] Borlund, P. (2000). Experimental components for the\nevaluation of interactive information retrieval systems. J. Doc.\n56, 1, 71-90.\n[6] Downey et al. (2007). Models of searching and browsing:\nlanguages, studies and applications. In Proc. IJCAI, 1465-72.\n[7] Dumais, S.T. & Belkin, N.J. (2005). The TREC interactive\ntracks: putting the user into search. In Voorhees, E.M. and\nHarman, D.K. (eds.) TREC: Experiment and Evaluation in\nInformation Retrieval. Cambridge, MA: MIT Press, 123-153.\n[8] Furnas, G. W. (1985). Experience with an adaptive indexing\nscheme. In Proc. CHI, 131-135.\n[9] Hickl, A. et al. (2006). FERRET: Interactive \nquestionanswering for real-world environments. In Proc. of\nCOLING/ACL, 25-28.\n[10] Jones, R., et al. (2006). Generating query substitutions. In\nProc. WWW, 387-396.\n[11] Koenemann, J. & Belkin, N. (1996). A case for interaction: a\nstudy of interactive information retrieval behavior and\neffectiveness. In Proc. CHI, 205-212.\n[12] O\"Day, V. & Jeffries, R. (1993). Orienteering in an\ninformation landscape: how information seekers get from here\nto there. In Proc. CHI, 438-445.\n[13] Radlinski, F. & Joachims, T. (2005). Query chains: Learning to\nrank from implicit feedback. In Proc. KDD, 239-248.\n[14] Salton, G. & Buckley, C. (1988) Term-weighting approaches\nin automatic text retrieval. Inf. Proc. Manage. 24, 513-523.\n[15] Silverstein, C. et al. (1999). Analysis of a very large Web\nsearch engine query log. SIGIR Forum 33, 1, 6-12.\n[16] Smyth, B. et al. (2004). Exploiting query repetition and\nregularity in an adaptive community-based Web search engine.\nUser Mod. User Adapt. Int. 14, 5, 382-423.\n[17] Spink, A. et al. (2002). U.S. versus European Web searching\ntrends. SIGIR Forum 36, 2, 32-38.\n[18] Spink, A., et al. (2006). Multitasking during Web search\nsessions. Inf. Proc. Manage., 42, 1, 264-275.\n[19] Wexelblat, A. & Maes, P. (1999). Footprints: history-rich tools\nfor information foraging. In Proc. CHI, 270-277.\n[20] White, R.W. & Drucker, S.M. (2007). Investigating behavioral\nvariability in Web search. In Proc. WWW, 21-30.\n[21] White, R.W. & Marchionini, G. (2007). Examining the\neffectiveness of real-time query expansion. Inf. Proc. Manage.\n43, 685-704.\n": ["popular destination", "web search interaction", "improving query", "retrieval performance", "related query", "information-seeking experience", "query trail", "session trail", "lookup-based approach", "log-based evaluation", "user study", "search destination", "enhance web search", ""]}