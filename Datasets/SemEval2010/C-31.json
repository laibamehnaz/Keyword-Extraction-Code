{"Apocrita: A Distributed Peer-to-Peer File Sharing System\nfor Intranets\nJoshua J. Reynolds, Robbie McLeod, Qusay H. Mahmoud\nDistributed Computing and Wireless & Telecommunications Technology\nUniversity of Guelph-Humber\nToronto, ON, M9W 5L7 Canada\n{jreyno04,rmcleo01,qmahmoud}@uoguelph.ca\nABSTRACT\nMany organizations are required to author documents for various\npurposes, and such documents may need to be accessible by all\nmember of the organization. This access may be needed for\nediting or simply viewing a document. In some cases these\ndocuments are shared between authors, via email, to be edited.\nThis can easily cause incorrect version to be sent or conflicts\ncreated between multiple users trying to make amendments to a\ndocument. There may even be multiple different documents in the\nprocess of being edited. The user may be required to search for a\nparticular document, which some search tools such as Google\nDesktop may be a solution for local documents but will not find a\ndocument on another user\"s machine. Another problem arises\nwhen a document is made available on a user\"s machine and that\nuser is offline, in which case the document is no longer\naccessible. In this paper we present Apocrita, a revolutionary\ndistributed P2P file sharing system for Intranets.\nCategories and Subject Descriptors\nC.2.4 [Computer-Communication Networks]: Distributed\nSystems - Distributed applications.\nGeneral Terms\nDesign, Experimentation, Performance.\n1. INTRODUCTION\nThe Peer-to-Peer (P2P) computing paradigm is becoming a\ncompletely new form of mutual resource sharing over the\nInternet. With the increasingly common place broadband Internet\naccess, P2P technology has finally become a viable way to share\ndocuments and media files.\nThere are already programs on the market that enable P2P file\nsharing. These programs enable millions of users to share files\namong themselves. While the utilization of P2P clients is already\na gigantic step forward compared to downloading files off\nwebsites, using such programs are not without their problems.\nThe downloaded files still require a lot of manual management by\nthe user. The user still needs to put the files in the proper\ndirectory, manage files with multiple versions, delete the files\nwhen they are no longer wanted. We strive to make the process of\nsharing documents within an Intranet easier.\nMany organizations are required to author documents for various\npurposes, and such documents may need to be accessible by all\nmembers of the organization. This access may be needed for\nediting or simply viewing a document. In some cases these\ndocuments are sent between authors, via email, to be edited. This\ncan easily cause incorrect version to be sent or conflicts created\nbetween multiple users trying to make amendments to a\ndocument. There may even be multiple different documents in the\nprocess of being edited. The user may be required to search for a\nparticular document, which some search tools such as Google\nDesktop may be a solution for local documents but will not find a\ndocument on another user\"s machine. Furthermore, some\norganizations do not have a file sharing server or the necessary\nnetwork infrastructure to enable one. In this paper we present\nApocrita, which is a cost-effective distributed P2P file sharing\nsystem for such organizations.\nThe rest of this paper is organized as follows. In section 2, we\npresent Apocrita. The distributed indexing mechanism and\nprotocol are presented in Section 3. Section 4 presents the \npeer-topeer distribution model. A proof of concept prototype is presented\nin Section 5, and performance evaluations are discussed in\nSection 6. Related work is presented is Section 7, and finally\nconclusions and future work are discussed in Section 8.\n2. APOCRITA\nApocrita is a distributed peer-to-peer file sharing system, and has\nbeen designed to make finding documents easier in an Intranet\nenvironment. Currently, it is possible for documents to be located\non a user's machine or on a remote machine. It is even possible\nthat different revisions could reside on each node on the Intranet.\nThis means there must be a manual process to maintain document\nversions. Apocrita solves this problem using two approaches.\nFirst, due to the inherent nature of Apocrita, the document will\nonly reside on a single logical location. Second, Apocrita provides\na method of reverting to previous document versions. Apocrita\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. To copy\notherwise, or republish, to post on servers or to redistribute to lists,\nrequires prior specific permission and/or a fee.\nACMSE\"07, MARCH 23-24, 2007, WINSTON-SALEM, NC, USA.\nCOPYRIGHT 2007 ACM 978-1-59593-629-5/07/0003 \u00e2\u20ac\u00a6$5.00.\n174\nwill also distribute documents across multiple machines to ensure\nhigh availability of important documents. For example, if a\nmachine contains an important document and the machine is\ncurrently inaccessible, the system is capable of maintaining\navailability of the document through this distribution mechanism.\nIt provides a simple interface for searching and accessing files\nthat may exist either locally or remotely. The distributed nature of\nthe documents is transparent to the user. Apocrita supports a\ndecentralized network model where the peers use a discovery\nprotocol to determine peers.\nApocrita is intended for network users on an Intranet. The main\nfocus is organizations that may not have a network large enough\nto require a file server and supporting infrastructure. It eliminates\nthe need for documents to be manually shared between users\nwhile being edited and reduces the possibility of conflicting\nversions being distributed. The system also provides some\nredundancy and in the event of a single machine failure, no\nimportant documents will be lost. It is operating system\nindependent, and easy to access through a web browser or through\na standalone application. To decrease the time required for\nindexing a large number of documents, the indexing process is\ndistributed across available idle nodes. Local and remote files\nshould be easily accessible through a virtual mountable file\nsystem, providing transparency for users.\n3. DISTRIBUTED INDEXING\nApocrita uses a distributed index for all the documents that are\navailable on the Intranet. Each node will contain part of the full\nindex, and be aware of what part of the index each other node has.\nA node will be able to contact each node that contains a unique\nportion of the index. In addition, each node has a separate local\nindex of its own documents. But as discussed later, in the current\nimplementation, each node has a copy of the entire index.\nIndexing of the documents is distributed. Therefore, if a node is in\nthe process of indexing many documents, it will break up the\nwork over the nodes. Once a node\"s local index is updated with\nthe new documents, the distributed index will then be updated.\nThe current distributed indexing system consists of three separate\nmodules: NodeController, FileSender, and NodeIndexer. The\nresponsibility of each module is discussed later in this section.\n3.1 Indexing Protocol\nThe protocol we have designed for the distributed indexing is\ndepicted in Figure 1.\nFigure 1. Apocrita distributed indexing protocol.\nIDLE QUERY: The IDLE QUERY is sent out from the initiating\nnode to determine which other nodes may be able to help with the\noverall indexing process. There are no parameters sent with the\ncommand. The receiving node will respond with either a BUSY\nor IDLE command. If the IDLE command is received, the\ninitiating node will add the responding node to a list of available\ndistributed indexing helpers. In the case of a BUSY command\nbeing received, the responding node is ignored.\nBUSY: Once a node received an IDL QUERY, it will determine\nwhether it can be considered a candidate for distributed indexing.\nThis determination is based on the overall CPU usage of the node.\nIf the node is using most of its CPU for other processes, the node\nwill respond to the IDLE QUERY with a BUSY command.\nIDLE: As with the case of the BUSY response, the node\nreceiving the IDLE QUERY will determine its eligibility for\ndistributed indexing. To be considered a candidate for distributed\nindexing, the overall CPU usage must be at a minimum to all for\ndedicated indexing of the distributed documents. If this is the\ncase, the node will respond with an IDLE command.\nINCOMING FILE: Once the initiating node assembles a set of\nidle nodes to assist with the distributed indexing, it will divide the\ndocuments to be sent to the nodes. To do this, it sends an\nINCOMING FILE message, which contains the name of the file\nas well as the size in bytes. After the INCOMING FILE command\nhas been sent, the initiating node will begin to stream the file to\nthe other node. The initiating node will loop through the files that\nare to be sent to the other node; each file stream being preceded\nby the INCOMING FILE command with the appropriate\nparameters.\nINDEX FILE: Once the indexing node has completed the\nindexing process of the set of files, it must send the resultant\nindex back to the initiating node. The index is comprised of\nmultiple files, which exist on the file system of the indexing node.\nAs with the INCOMING FILE command, the indexing node\nstreams each index file after sending an INDEX FILE command.\nThe INDEX FILE command has two parameters: the first being\nthe name of the index, and the second is the size of the file in\nbytes.\nSEND COMPLETE: When sending the sets of files for both the\nindex and the files to be indexed, the node must notify the\ncorresponding node when the process is complete. Once the\ninitiating node is finished sending the set of documents to be\nindexed, it will then send a SEND COMPLETE command\nindicating to the indexing node that there are no more files and\nthe node can proceed with indexing the files. In the case of the\ninitiating node sending the index files, the indexing node will\ncomplete the transfer with the SEND COMPLETE command\nindicating to the initiating node that there are no more index files\nto be sent and the initiating node can then assemble those index\nfiles into the main index.\nThe NodeController is responsible for setting up connections with\nnodes in the idle state to distribute the indexing process. Using\nJXTA [5], the node controller will obtain a set of nodes. This set\nof nodes is iterated and each one is sent the IDLE QUERY\ncommand. The nodes that respond with idle are then collected.\nThe set of idle nodes includes the node initiating the distributed\nindexing process, referred to as the local node. Once the\ncollection of idle nodes is obtained, the node updates the set of\ncontrollers and evenly divides the set of documents that are to be\nindexed. For example, if there are 100 documents and 10 nodes\n(including the local node) then each node will have 10 documents\nto index. For each indexing node an instance of the FileSender\nobject is created. The FileSender is aware of the set of documents\nthat node is responsible for. Once a FileSender object has been\ncreated for each node, the NodeController waits for each\nFileSender to complete. When the FileSender objects have\ncompleted the NodeController will take the resultant indexes from\n175\neach node and pass them to an instance of the IndexCompiler,\nwhich maintains the index and the list of FileSenders. Once the\nIndexCompiler has completed it will return to the idle state and\nactivate the directory scanner to monitor the locally owned set of\ndocuments for changes that may require reindexing.\nThe NodeIndexer is responsible for receiving documents sent to it\nby the initiating node and then indexing them using the Lucene\nengine [7]. Once the indexing is complete the resulting index is\nstreamed back to the initiating node as well as compiled in the\nindexer nodes own local index. Before initiating the indexing\nprocess it must be sent an IDLE QUERY message. This is the\nfirst command that sets off the indexing process. The indexer\nnode will determine whether it is considered idle based on the\ncurrent CPU usage. As outlined in the protocol section if the node\nis not being used and has a low overall CPU usage percentage it\nwill return IDLE to the IDLE QUERY command. If the indexer\nnodes CPU usage is above 50% for a specified amount of time it\nis then considered to be busy and will respond to the IDLE\nQUERY command with BUSY. If a node is determined busy it\nreturns to its listening state waiting for another IDLE QUERY\nfrom another initiating node. If the node is determined to be idle it\nwill enter the state where it will receive files from the initiating\nnode that it is responsible for indexing. Once all of the files are\nreceived by the initiating node, indicated by a SEND\nCOMPLETE message, it starts an instance of the Lucene indexing\nengine. The files are stored in a temporary directory separate from\nthe nodes local documents that it is responsible for maintaining an\nindex of. The Lucene index writer then indexes all of the\ntransferred files. The index is stored on the drive within a\ntemporary directory separate from the current index. After the\nindexing of the files completes the indexer node enters the state\nwhere the index files are sent back to the initiating node. The\nindexer node loops through all of the files created by Lucene\"s\nIndexWriter and streams them to the initiating node. Once these\nfiles are sent back that index is then merged into the indexer\nnodes own full index of the existing files. It then enters the idle\nstate where it will then listen for any other nodes that required\ndistributing the indexing process.\nThe FileSender object is the initiating node equivalent of the\nindexer node. It initiates the communication between the initiating\nnode and the node that will assist in the distributed indexing. The\ninitiating node runs many instances of the FileSender node one\nfor each other node it has determined to be idle. Upon\ninstantiation of the FileSender it is passed the node that it is\nresponsible for contacting and the set of files that must be sent.\nThe FileSender\"s first job is to send the files that are to be indexed\nby the other idle node. The files are streamed one at a time to the\nother node. It sends each file using the INCOMING FILE\ncommand. With that command it sends the name of the file being\nsent and the size in bytes. Once all files have been sent the\nFileSender sends the SEND COMPLETE command. The\nFileSender creates an instance of Lucene\"s IndexWriter and\nprepares to create the index in a temporary directory on the file\nsystem. The FileSender will begin to receive the files that are to\nbe saved within the index. It receives an INDEX FILE command\nwith the name of the files and the size in bytes. This file is then\nstreamed into the temporary index directory on the FileSender\nnode. After the transfer of the index files has been completed the\nFileSender notifies the instance of the index compiler that it is\nready to combine the index. Each instance of the FileSender has\nits own unique section of temporary space to store the index that\nhas been transferred back from the indexing node. When\nnotifying the IndexCompiler it will also pass the location of the\nparticular FileSenders directory location of that index.\n4. PEER-TO-PEER DISTRIBUTION\nApocrita uses a peer-to-peer distribution model in order to\ndistribute files. Files are distributed solely from a serving node to\na client node without regard for the availability of file pieces from\nother clients in the network. This means that the file transfers will\nbe fast and efficient and should not severely affect the usability of\nserving nodes from the point of view of a local user. The JXTA\nframework [5] is used in order to implement peer-to-peer\nfunctionality. This has been decided due to the extremely \nshorttimeline of the project which allows us to take advantage of over\nfive years of testing and development and support from many\nlarge organizations employing JXTA in their own products. We\nare not concerned with any potential quality problems because\nJXTA is considered to be the most mature and stable peer-to-peer\nframework available.\nUsing JXTA terminology, there are three types of peers used in\nnode classification.\nEdge peers are typically low-bandwidth, non-dedicated nodes.\nDue to these characteristics, edge peers are not used with\nApocrita.\nRelay peers are typically higher-bandwidth, dedicated nodes.\nThis is the classification of all nodes in the Apocrita network, and,\nas such, are the default classification used.\nRendezvous peers are used to coordinate message passing\nbetween nodes in the Apocrita network. This means that a\nminimum of one rendezvous peer per subnet is required.\n4.1 Peer Discovery\nThe Apocrita server subsystem uses the JXTA Peer Discovery\nProtocol (PDP) in order to find participating peers within the\nnetwork as shown in Figure 2.\nFigure 2. Apocrita peer discovery process.\n176\nThe PDP listens for peer advertisements from other nodes in the\nApocrita swarm. If a peer advertisement is detected, the server\nwill attempt to join the peer group and start actively contributing\nto the network. If no peers are found by the discovery service, the\nserver will create a new peer group and start advertising this peer\ngroup. This new peer group will be periodically advertised on the\nnetwork; any new peers joining the network will attach to this\npeer group. A distinct advantage of using the JXTA PDP is that\nApocrita does not have to be sensitive to particular networking\nnuances such as Maximum Transmission Unit (MTU). In\naddition, Apocrita does not have to support one-to-many packet\ndelivery methods such as multicast and instead can rely on JXTA\nfor this support.\n4.2 Index Query Operation\nAll nodes in the Apocrita swarm have a complete and up-to-date\ncopy of the network index stored locally. This makes querying the\nindex for search results trivial. Unlike the Gnutella protocol, a\nquery does not have to propagate throughout the network. This\nalso means that the time to return query results is very fast - much\nfaster than protocols that rely on nodes in the network to pass the\nquery throughout the network and then wait for results. This is\ndemonstrated in Figure 3.\nFigure 3. Apocrita query operation.\nEach document in the swarm has a unique document\nidentification number (ID). A node will query the index and a\nresult will be returned with both the document ID number as well\nas a list of peers with a copy of the matched document ID. It is\nthen the responsibility of the searching peer to contact the peers in\nthe list to negotiate file transfer between the client and server.\n5. PROTOTYPE IMPLEMENTATION\nApocrita uses the Lucene framework [7], which is a project under\ndevelopment by the Apache Software Foundation. Apache\nLucene is a high-performance, full-featured text search engine\nlibrary written entirely in Java. In the current implementation,\nApocrita is only capable of indexing plain text documents.\nApocrita uses the JXTA framework [5] as a peer-to-peer transport\nlibrary between nodes. JXTA is used to pass both messages and\nfiles between nodes in the search network. By using JXTA,\nApocrita takes advantage of a reliable, and proven peer-to-peer\ntransport mechanism. It uses the pipe facility in order to pass\nmessages and files between nodes. The pipe facility provides\nmany different types of pipe advertisements. This includes an\nunsecured unicast pipe, a secured unicast pipe, and a propagated\nunsecured pipe.\nMessage passing is used to pass status messages between nodes in\norder to aid in indexing, searching, and retrieval. For example, a\nnode attempting to find an idle node to participate in indexing will\nquery nodes via the message facility. Idle nodes will reply with a\nstatus message to indicate they are available to start indexing.\nFile passing is used within Apocrita for file transfer. After a file\nhas been searched for and located within the peer group, a JXTA\nsocket will be opened and file transfer will take place. A JXTA\nsocket is similar to a standard Java socket, however a JXTA\nsocket uses JXTA pipes in underlying network transport. File\npassing uses an unsecured unicast pipe in order to transfer data.\nFile passing is also used within Apocrita for index transfer. Index\ntransfer works exactly like a file transfer. In fact, the index\ntransfer actually passes the index as a file. However, there is one\nkey difference between file transfer and index transfer. In the case\nof file transfer, a socket is created between only two nodes. In the\ncase of index transfer, a socket must be created between all nodes\nin the network in order to pass the index, which allows for all\nnodes to have a full and complete index of the entire network. In\norder to facilitate this transfer efficiently, index transfer will use\nan unsecured propagated pipe to communicate with all nodes in\nthe Apocrita network.\n6. PERFORMANCE EVALUATION\nIt is difficult to objectively benchmark the results obtained\nthrough Apocrita because there is no other system currently\navailable with the same goals as Apocrita. We have, however,\nevaluated the performance of the critical sections of the system.\nThe critical sections were determined to be the processes that are\nthe most time intensive. The evaluation was completed on\nstandard lab computers on a 100Mb/s Ethernet LAN; the\nmachines run Windows XP with a Pentium 4 CPU running at\n2.4GHz with 512 MB of RAM.\nThe indexing time has been run against both: the Time Magazine\ncollection [8], which contains 432 documents and 83 queries and\ntheir most relevant results, and the NPL collection [8] that has a\ntotal of 11,429 documents and 93 queries with expected results.\nEach document ranges in size between 4KB and 8KB. As Figure\n4 demonstrates, the number of nodes involved in the indexing\nprocess affects the time taken to complete the indexing \nprocesssometimes even drastically.\nFigure 4. Node vs. index time.\nThe difference in going from one indexing node to two indexing\nnodes is the most drastic and equates to an indexing time 37%\nfaster than a single indexing node. The different between two\n177\nindexing nodes and three indexing nodes is still significant and\nrepresents a 16% faster time than two indexing nodes. As the\nnumber of indexing nodes increases the results are less dramatic.\nThis can be attributed to the time overhead associated with having\nmany nodes perform indexing. The time needed to communicate\nwith a node is constant, so as the number of nodes increases, this\nconstant becomes more prevalent. Also, the complexity of joining\nthe indexing results is a complex operation and is complicated\nfurther as the number of indexing nodes increases.\nSocket performance is also a very important part of Apocrita.\nBenchmarks were performed using a 65MB file on a system with\nboth the client and server running locally. This was done to\nisolate possible network issues. Although less drastic, similar\nresults were shown when the client and server run on independent\nhardware. In order to mitigate possible unexpected errors, each\ntest was run 10 times.\nFigure 5. Java sockets vs. JXTA sockets.\nAs Figure 5 demonstrates, the performance of JXTA sockets is\nabysmal as compared to the performance of standard Java sockets.\nThe minimum transfer rate obtained using Java sockets is\n81,945KB/s while the minimum transfer rater obtained using\nJXTA sockets is much lower at 3, 805KB/s. The maximum\ntransfer rater obtain using Java sockets is 97,412KB/s while the\nmaximum transfer rate obtained using JXTA sockets is\n5,530KB/s. Finally, the average transfer rate using Java sockets is\n87,540KB/s while the average transfer rate using JXTA sockets is\n4,293KB/s.\nThe major problem found in these benchmarks is that the\nunderlying network transport mechanism does not perform as\nquickly or efficiently as expected. In order to garner a\nperformance increase, the JXTA framework needs to be\nsubstituted with a more traditional approach. The indexing time is\nalso a bottleneck and will need to be improved for the overall\nquality of Apocrita to be improved.\n7. RELATED WORK\nSeveral decentralized P2P systems [1, 2, 3] exist today that\nApocrita features some of their functionality. However, Apocrita\nalso has unique novel searching and indexing features that make\nthis system unique. For example, Majestic-12 [4] is a distributed\nsearch and indexing project designed for searching the Internet.\nEach user would install a client, which is responsible for indexing\na portion of the web. A central area for querying the index is\navailable on the Majestic-12 web page. The index itself is not\ndistributed, only the act of indexing is distributed. The distributed\nindexing aspect of this project most closely relates Apocrita goals.\nYaCy [6] is a peer-to-peer web search application. YaCy consists\nof a web crawler, an indexer, a built-in database engine, and a p2p\nindex exchange protocol. YaCy is designed to maintain a\ndistributed index of the Internet. It used a distributed hash table\n(DHT) to maintain the index. The local node is used to query but\nall results that are returned are accessible on the Internet. YaCy\nused many peers and DHT to maintain a distributed index.\nApocrita will also use a distributed index in future\nimplementations and may benefit from using an implementation\nof a DHT. YaCy however, is designed as a web search engine\nand, as such solves a much different problem than Apocrita.\n8. CONCLUSIONS AND FUTURE WORK\nWe presented Apocrita, a distributed P2P searching and indexing\nsystem intended for network users on an Intranet. It can help\norganizations with no network file server or necessary network\ninfrastructure to share documents. It eliminates the need for\ndocuments to be manually shared among users while being edited\nand reduce the possibility of conflicting versions being\ndistributed. A proof of concept prototype has been constructed,\nbut the results from measuring the network transport mechanism\nand the indexing time were not as impressive as initially\nenvisioned. Despite these shortcomings, the experience gained\nfrom the design and implementation of Apocrita has given us\nmore insight into building challenging distributed systems.\nFor future work, Apocrita will have a smart content distribution\nmodel in which a single instance of a file can intelligently and\ntransparently replicate throughout the network to ensure a copy of\nevery important file will always be available regardless of the\navailability of specific nodes in the network. In addition, we plan\nto integrate a revision control system into the content distribution\nportion of Apocrita so that users could have the ability to update\nan existing file that they found and have the old revision\nmaintained and the new revision propagated. Finally, the current\nimplementation has some overhead and redundancy due to the\nfact that the entire index is maintained on each individual node,\nwe plan to design a distributed index.\n9. REFERENCES\n[1] Rodrigues, R., Liskov, B., Shrira, L.: The Design of a Robust\nPeer-to-Peer System. Available online:\nhttp://www.pmg.lcs.mit.edu/~rodrigo/ew02-robust.pdf.\n[2] Chawathe, Y., Ratnasamy, S., Breslau, L., Lanham, N., and\nChenker, S.: Making Gnutella-like P2P Systems Scalable. In\nProceedings of SIGCOMM\"03, Karlsruhe, Germany.\n[3] Harvest: A Distributed Search System:\nhttp://harvest.sourceforge.net.\n[4] Majestic-12: Distributed Search Engine:\nhttp://www.majestic12.co.uk.\n[5] JXTA: http://www.jxta.org.\n[6] YaCy: Distributed P2P-based Web Indexing:\nhttp://www.yacy.net/yacy.\n[7] Lucene Search Engine Library: http://lucene.apache.org.\n[8] Test Collections (Time Magazine and NPL):\nwww.dcs.gla.ac.uk/idom/ir_resources/test_collections.\n178\n": ["peer-to-peer", "file sharing system", "intranet", "author", "document", "apocrita", "jxta", "distributed indexing", "peer-to-peer distribution model", "idle query", "index file", "incoming file", "p2p searching", "p2p", "file share", ""]}