{"Term Feedback for Information Retrieval\nwith Language Models\nBin Tan\u00e2\u20ac\u00a0\n, Atulya Velivelli\u00e2\u20ac\u00a1\n, Hui Fang\u00e2\u20ac\u00a0\n, ChengXiang Zhai\u00e2\u20ac\u00a0\nDept. of Computer Science\u00e2\u20ac\u00a0\n, Dept. of Electrical and Computer Engineering\u00e2\u20ac\u00a1\nUniversity of Illinois at Urbana-Champaign\nbintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu\nABSTRACT\nIn this paper we study term-based feedback for information \nretrieval in the language modeling approach. With term feedback\na user directly judges the relevance of individual terms without \ninteraction with feedback documents, taking full control of the query\nexpansion process. We propose a cluster-based method for \nselecting terms to present to the user for judgment, as well as effective\nalgorithms for constructing refined query language models from\nuser term feedback. Our algorithms are shown to bring significant\nimprovement in retrieval accuracy over a non-feedback baseline,\nand achieve comparable performance to relevance feedback. They\nare helpful even when there are no relevant documents in the top.\nCategories and Subject Descriptors\nH.3.3 [Information Search and Retrieval]: Retrieval models\nGeneral Terms\nAlgorithms\n1. INTRODUCTION\nIn the language modeling approach to information retrieval, \nfeedback is often modeled as estimating an improved query model or\nrelevance model based on a set of feedback documents [25, 13].\nThis is in line with the traditional way of doing relevance feedback\n- presenting a user with documents/passages for relevance \njudgment and then extracting terms from the judged documents or \npassages to expand the initial query. It is an indirect way of seeking\nuser\"s assistance for query model construction, in the sense that the\nrefined query model (based on terms) is learned through feedback\ndocuments/passages, which are high-level structures of terms. It\nhas the disadvantage that irrelevant terms, which occur along with\nrelevant ones in the judged content, may be erroneously used for\nquery expansion, causing undesired effects. For example, for the\nTREC query Hubble telescope achievements, when a relevant\ndocument talks more about the telescope\"s repair than its \ndiscoveries, irrelevant terms such as spacewalk can be added into the\nmodified query.\nWe can consider a more direct way to involve a user in query\nmodel improvement, without an intermediary step of document\nfeedback that can introduce noise. The idea is to present a \n(reasonable) number of individual terms to the user and ask him/her to\njudge the relevance of each term or directly specify their \nprobabilities in the query model. This strategy has been discussed in [15],\nbut to our knowledge, it has not been seriously studied in existing\nlanguage modeling literature. Compared to traditional relevance\nfeedback, this term-based approach to interactive query model \nrefinement has several advantages. First, the user has better \ncontrol of the final query model through direct manipulation of terms:\nhe/she can dictate which terms are relevant, irrelevant, and \npossibly, to what degree. This avoids the risk of bringing unwanted\nterms into the query model, although sometimes the user introduces\nlow-quality terms. Second, because a term takes less time to judge\nthan a document\"s full text or summary, and as few as around 20\npresented terms can bring significant improvement in retrieval \nperformance (as we will show later), term feedback makes it faster to\ngather user feedback. This is especially helpful for interactive \nadhoc search. Third, sometimes there are no relevant documents in\nthe top N of the initially retrieved results if the topic is hard. This\nis often true when N is constrained to be small, which arises from\nthe fact that the user is unwilling to judge too many documents. In\nthis case, relevance feedback is useless, as no relevant document\ncan be leveraged on, but term feedback is still often helpful, by\nallowing relevant terms to be picked from irrelevant documents.\nDuring our participation in the TREC 2005 HARD Track and\ncontinued study afterward, we explored how to exploit term \nfeedback from the user to construct improved query models for \ninformation retrieval in the language modeling approach. We identified\ntwo key subtasks of term-based feedback, i.e., pre-feedback \npresentation term selection and post-feedback query model \nconstruction, with effective algorithms developed for both. We imposed a\nsecondary cluster structure on terms and found that a cluster view\nsheds additional insight into the user\"s information need, and \nprovides a good way of utilizing term feedback. Through experiments\nwe found that term feedback improves significantly over the \nnonfeedback baseline, even though the user often makes mistakes in\nrelevance judgment. Among our algorithms, the one with best \nretrieval performance is TCFB, the combination of TFB, the direct\nterm feedback algorithm, and CFB, the cluster-based feedback \nalgorithm. We also varied the number of feedback terms and \nobserved reasonable improvement even at low numbers. Finally, by\ncomparing term feedback with document-level feedback, we found\nit to be a viable alternative to the latter with competitive retrieval\nperformance.\nThe rest of the paper is organized as follows. Section 2 discusses\nsome related work. Section 4 outlines our general approach to term\nfeedback. We present our method for presentation term selection in\nSection 3 and algorithms for query model construction in Section 5.\nThe experiment results are given in Section 6. Section 7 concludes\nthis paper.\n2. RELATED WORK\nRelevance feedback[17, 19] has long been recognized as an \neffective method for improving retrieval performance. Normally, the\ntop N documents retrieved using the original query are presented\nto the user for judgment, after which terms are extracted from the\njudged relevant documents, weighted by their potential of \nattracting more relevant documents, and added into the query model. The\nexpanded query usually represents the user\"s information need \nbetter than the original one, which is often just a short keyword query.\nA second iteration of retrieval using this modified query usually\nproduces significant increase in retrieval accuracy. In cases where\ntrue relevance judgment is unavailable and all top N documents are\nassumed to be relevant, it is called blind or pseudo feedback[5, 16]\nand usually still brings performance improvement.\nBecause document is a large text unit, when it is used for \nrelevance feedback many irrelevant terms can be introduced into the\nfeedback process. To overcome this, passage feedback is proposed\nand shown to improve feedback performance[1, 23]. A more direct\nsolution is to ask the user for their relevance judgment of feedback\nterms. For example, in some relevance feedback systems such as\n[12], there is an interaction step that allows the user to add or \nremove expansion terms after they are automatically extracted from\nrelevant documents. This is categorized as interactive query \nexpansion, where the original query is augmented with user-provided\nterms, which can come from direct user input (free-form text or\nkeywords)[22, 7, 10] or user selection of system-suggested terms\n(using thesauri[6, 22] or extracted from feedback documents[6, 22,\n12, 4, 7]).\nIn many cases term relevance feedback has been found to \neffectively improve retrieval performance[6, 22, 12, 4, 10]. For \nexample, the study in [12] shows that the user prefers to have explicit\nknowledge and direct control of which terms are used for query \nexpansion, and the penetrable interface that provides this freedom is\nshown to perform better than other interfaces. However, in some\nother cases there is no significant benefit[3, 14], even if the user\nlikes interacting with expansion terms. In a simulated study \ncarried out in [18], the author compares the retrieval performance of\ninteractive query expansion and automatic query expansion with a\nsimulated study, and suggests that the potential benefits of the \nformer can be hard to achieve. The user is found to be not good at\nidentifying useful terms for query expansion, when a simple term\npresentation interface is unable to provide sufficient semantic \ncontext of the feedback terms.\nOur work differs from the previous ones in two important \naspects. First, when we choose terms to present to the user for \nrelevance judgment, we not only consider single-term value (e.g., the\nrelative frequency of a term in the top documents, which can be\nmeasured by metrics such as Robertson Selection Value and \nSimplified Kullback-Leibler Distance as listed in [24]), but also \nexamine the cluster structure of the terms, so as to produce a balanced\ncoverage of the different topic aspects. Second, with the language\nmodelling framework, we allow an elaborate construction of the\nupdated query model, by setting different probabilities for different\nterms based on whether it is a query term, its significance in the\ntop documents, and its cluster membership. Although techniques\nfor adjusting query term weights exist for vector space models[17]\nand probablistic relevance models[9], most of the aforementioned\nworks do not use them, choosing to just append feedback terms to\nthe original query (thus using equal weights for them), which can\nlead to poorer retrieval performance. The combination of the two\naspects allows our method to perform much better than the \nbaseline.\nThe usual way for feedback term presentation is just to display\nthe terms in a list. There have been some works on alternative user\ninterfaces. [8] arranges terms in a hierarchy, and [11] compares\nthree different interfaces, including terms + checkboxes, terms +\ncontext (sentences) + checkboxes, sentences + input text box. In\nboth studies, however, there is no significant performance \ndifference. In our work we adopt the simplest approach of terms + \ncheckboxes. We focus on term presentation and query model \nconstruction from feedback terms, and believe using contexts to improve\nfeedback term quality should be orthogonal to our method.\n3. GENERAL APPROACH\nWe follow the language modeling approach, and base our method\non the KL-divergence retrieval model proposed in [25]. With this\nmodel, the retrieval task involves estimating a query language model\n\u00ce\u00b8q from a given query, a document language model \u00ce\u00b8d from each\ndocument, and calculating their KL-divergence D(\u00ce\u00b8q||\u00ce\u00b8d), which\nis then used to score the documents. [25] treats relevance feedback\nas a query model re-estimation problem, i.e., computing an updated\nquery model \u00ce\u00b8q given the original query text and the extra evidence\ncarried by the judged relevant documents. We adopt this view, and\ncast our task as updating the query model from user term feedback.\nThere are two key subtasks here: First, how to choose the best terms\nto present to the user for judgment, in order to gather maximal \nevidence about the user\"s information need. Second, how to compute\nan updated query model based on this term feedback evidence, so\nthat it captures the user\"s information need and translates into good\nretrieval performance.\n4. PRESENTATION TERM SELECTION\nProper selection of terms to be presented to the user for \njudgment is crucial to the success of term feedback. If the terms are\npoorly chosen and there are few relevant ones, the user will have a\nhard time looking for useful terms to help clarify his/her \ninformation need. If the relevant terms are plentiful, but all concentrate on\na single aspect of the query topic, then we will only be able to get\nfeedback on that aspect and missing others, resulting in a breadth\nloss in retrieved results. Therefore, it is important to carefully select\npresentation terms to maximize expected gain from user feedback,\ni.e., those that can potentially reveal most evidence of the user\"s\ninformation need. This is similar to active feedback[21], which\nsuggests that a retrieval system should actively probe the user\"s \ninformation need, and in the case of relevance feedback, the feedback\ndocuments should be chosen to maximize learning benefits (e.g. \ndiversely so as to increase coverage).\nIn our approach, the top N documents from an initial retrieval\nusing the original query form the source of feedback terms: all\nterms that appear in them are considered candidates to present to\nthe user. These documents serve as pseudo-feedback, since they\nprovide a much richer context than the original query (usually very\nshort), while the user is not asked to judge their relevance. Due to\nthe latter reason, it is possible to make N quite large (e.g., in our\nexperiments we set N = 60) to increase its coverage of different\naspects in the topic.\nThe simplest way of selecting feedback terms is to choose the\nmost frequent M terms from the N documents. This method, \nhowever, has two drawbacks. First, a lot of common noisy terms will be\nselected due to their high frequencies in the document collection,\nunless a stop-word list is used for filtering. Second, the \npresentation list will tend to be filled by terms from major aspects of the\ntopic; those from a minor aspect are likely to be missed due to their\nrelatively low frequencies.\nWe solve the above problems by two corresponding measures.\nFirst, we introduce a background model \u00ce\u00b8B that is estimated from\ncollection statistics and explains the common terms, so that they\nare much less likely to appear in the presentation list. Second, the\nterms are selected from multiple clusters in the pseudo-feedback\ndocuments, to ensure sufficient representation of different aspects\nof the topic.\nWe rely on the mixture multinomial model, which is used for\ntheme discovery in [26]. Specifically, we assume the N documents\ncontain K clusters {Ci| i = 1, 2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 K}, each characterized by\na multinomial word distribution (also known as unigram language\nmodel) \u00ce\u00b8i and corresponding to an aspect of the topic. The \ndocuments are regarded as sampled from a mixture of K + 1 \ncomponents, including the K clusters and the background model:\np(w|d) = \u00ce\u00bbBp(w|\u00ce\u00b8B) + (1 \u00e2\u02c6\u2019 \u00ce\u00bbB)\nK\ni=1\n\u00cf\u20acd,ip(w|\u00ce\u00b8i)\nwhere w is a word, \u00ce\u00bbB is the mixture weight for the background\nmodel \u00ce\u00b8B, and \u00cf\u20acd,i is the document-specific mixture weight for the\ni-th cluster model \u00ce\u00b8i. We then estimate the cluster models by \nmaximizing the probability of the pseudo-feedback documents being\ngenerated from the multinomial mixture model:\nlog p(D|\u00ce\u203a) =\nd\u00e2\u02c6\u02c6D w\u00e2\u02c6\u02c6V\nc(w; d) log p(w|d)\nwhere D = {di| i = 1, 2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 N} is the set of the N documents, V\nis the vocabulary, c(w; d) is w\"s frequency in d and \u00ce\u203a = {\u00ce\u00b8i| i =\n1, 2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 K} \u00e2\u02c6\u00aa {\u00cf\u20acdij | i = 1, 2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 N, j = 1, 2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 K} is the set\nof model parameters to estimate. The cluster models can be \nefficiently estimated using the Expectation-Maximization (EM) \nalgorithm. For its details, we refer the reader to [26]. Table 1 shows the\ncluster models for TREC query Transportation tunnel disasters\n(K = 3). Note that only the middle cluster is relevant.\nTable 1: Cluster models for topic 363 Transportation tunnel\ndisasters\nCluster 1 Cluster 2 Cluster 3\ntunnel 0.0768 tunnel 0.0935 tunnel 0.0454\ntransport 0.0364 fire 0.0295 transport 0.0406\ntraffic 0.0206 truck 0.0236 toll 0.0166\nrailwai 0.0186 french 0.0220 amtrak 0.0153\nharbor 0.0146 smoke 0.0157 train 0.0129\nrail 0.0140 car 0.0154 airport 0.0122\nbridg 0.0139 italian 0.0152 turnpik 0.0105\nkilomet 0.0136 firefight 0.0144 lui 0.0095\ntruck 0.0133 blaze 0.0127 jersei 0.0093\nconstruct 0.0131 blanc 0.0121 pass 0.0087\n\u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7\nFrom each of the K estimated clusters, we choose the L =\nM/K terms with highest probabilities to form a total of M \npresentation terms. If a term happens to be in top L in multiple clusters,\nwe assign it to the cluster where it has highest probability and let the\nother clusters take one more term as compensation. We also filter\nout terms in the original query text because they tend to always be\nrelevant when the query is short. The selected terms are then \npresented to the user for judgment. A sample (completed) feedback\nform is shown in Figure 1.\nIn this study we only deal with binary judgment: a presented\nterm is by default unchecked, and a user may check it to \nindicate relevance. We also do not explicitly exploit negative feedback\n(i.e., penalizing irrelevant terms), because with binary feedback an\nunchecked term is not necessarily irrelevant (maybe the user is \nunsure about its relevance). We could ask the user for finer \njudgment (e.g., choosing from highly relevant, somewhat relevant, do\nnot know, somewhat irrelevant and highly irrelevant), but binary\nfeedback is more compact, taking less space to display and less\nuser effort to make judgment.\n5. ESTIMATING QUERY MODELS FROM\nTERM FEEDBACK\nIn this section, we present several algorithms for exploiting term\nfeedback. The algorithms take as input the original query q, the\nclusters {\u00ce\u00b8i} as generated by the theme discovery algorithm, the set\nof feedback terms T and their relevance judgment R, and outputs\nan updated query language model \u00ce\u00b8q that makes best use of the\nfeedback evidence to capture the user\"s information need.\nFirst we describe our notations:\n\u00e2\u20ac\u00a2 \u00ce\u00b8q: The original query model, derived from query terms only:\np(w|\u00ce\u00b8q) =\nc(w; q)\n|q|\nwhere c(w; q) is the count of w in q, and |q| = w\u00e2\u02c6\u02c6q c(w; q)\nis the query length.\n\u00e2\u20ac\u00a2 \u00ce\u00b8q : The updated query model which we need to estimate\nfrom term feedback.\n\u00e2\u20ac\u00a2 \u00ce\u00b8i (i = 1, 2, . . . K): The unigram language model of cluster\nCi, as estimated using the theme discovery algorithm.\n\u00e2\u20ac\u00a2 T = {ti,j} (i = 1 . . . K, j = 1 . . . L): The set of terms \npresented to the user for judgment. ti,j is the j-th term chosen\nfrom cluster Ci.\n\u00e2\u20ac\u00a2 R = {\u00ce\u00b4w|w \u00e2\u02c6\u02c6 T}: \u00ce\u00b4w is an indicator variable that is 1 if w\nis judged relevant or 0 otherwise.\n5.1 TFB (Direct Term Feedback)\nThis is a straight-forward form of term feedback that does not\ninvolve any secondary structure. We give a weight of 1 to terms\njudged relevant by the user, a weight of \u00ce\u00bc to query terms, zero\nweight to other terms, and then apply normalization:\np(w|\u00ce\u00b8q ) =\n\u00ce\u00b4w + \u00ce\u00bc c(w; q)\nw \u00e2\u02c6\u02c6T \u00ce\u00b4w + \u00ce\u00bc|q|\nwhere w \u00e2\u02c6\u02c6T \u00ce\u00b4w is the total number of terms that are judged \nrelevant. We call this method TFB (direct Term FeedBack).\nIf we let \u00ce\u00bc = 1, this approach is equivalent to appending the\nrelevant terms after the original query, which is what standard query\nexpansion (without term reweighting) does. If we set \u00ce\u00bc > 1, we are\nputting more emphasis on the query terms than the checked ones.\nNote that the result model will be more biased toward \u00ce\u00b8q if the\noriginal query is long or the user feedback is weak, which makes\nsense, as we can trust more on the original query in either case.\nFigure 1: Filled clarification form for Topic 363\n363 transportation tunnel disasters\nPlease select all terms that are relevant to the topic.\ntraffic railway\nharbor rail\nbridge kilometer\nconstruct swiss\ncross link\nkong hong\nriver project\nmeter shanghai\nfire truck\nfrench smoke\ncar italian\nfirefights blaze\nblanc mont\nvictim franc\nrescue driver\nchamonix emerge\ntoll amtrak\ntrain airport\nturnpike lui\njersey pass\nrome z\ncenter electron\nroad boston\nspeed bu\nsubmit\n5.2 CFB (Cluster Feedback)\nHere we exploit the cluster structure that played an important\nrole when we selected the presentation terms. The clusters \nrepresent different aspects of the query topic, each of which may or\nmay not be relevant. If we are able to identify the relevant clusters,\nwe can combine them to generate a query model that is good at\ndiscovering documents belonging to these clusters (instead of the\nirrelevant ones). We could ask the user to directly judge the \nrelevance of a cluster after viewing representative terms in that cluster,\nbut this would sometimes be a difficult task for the user, who has to\nguess the semantics of a cluster via its set of terms, which may not\nbe well connected to one another due to a lack of context. \nTherefore, we propose to learn cluster feedback indirectly, inferring the\nrelevance of a cluster through the relevance of its feedback terms.\nBecause each cluster has an equal number of terms presented to\nthe user, the simplest measure of a cluster\"s relevance is the number\nof terms that are judged relevant in it. Intuitively, the more terms\nare marked relevant in a cluster, the closer the cluster is to the query\ntopic, and the more the cluster should participate in query \nmodification. If we combine the cluster models using weights determined\nthis way and then interpolate with the original query model, we\nget the following formula for query updating, which we call CFB\n(Cluster FeedBack):\np(w|\u00ce\u00b8q ) = \u00ce\u00bbp(w|\u00ce\u00b8q) + (1 \u00e2\u02c6\u2019 \u00ce\u00bb)\nK\ni=1\nL\nj=1 \u00ce\u00b4ti,j\nK\nk=1\nL\nj=1 \u00ce\u00b4tk,j\np(w|\u00ce\u00b8i)\nwhere L\nj=1 \u00ce\u00b4ti,j is the number of relevant terms in cluster Ci, and\nK\nk=1\nL\nj=1 \u00ce\u00b4tk,j is the total number of relevant terms.\nWe note that when there is only one cluster (K = 1), the above\nformula degenerates to\np(w|\u00ce\u00b8q ) = \u00ce\u00bbp(w|\u00ce\u00b8q) + (1 \u00e2\u02c6\u2019 \u00ce\u00bb)p(w|\u00ce\u00b81)\nwhich is merely pseudo-feedback of the form proposed in [25].\n5.3 TCFB (Term-cluster Feedback)\nTFB and CFB both have their drawbacks. TFB assigns non-zero\nprobabilities to the presented terms that are marked relevant, but\ncompletely ignores (a lot more) others, which may be left unchecked\ndue to the user\"s ignorance, or simply not included in the \npresentation list, but we should be able to infer their relevance from the\nchecked ones. For example, in Figure 1, since as many as 5 terms\nin the middle cluster (the third and fourth columns) are checked,\nwe should have high confidence in the relevance of other terms in\nthat cluster. CFB remedies TFB\"s problem by treating the terms\nin a cluster collectively, so that unchecked/unpresented terms \nreceive weights when presented terms in their clusters are judged as\nrelevant, but it does not distinguish which terms in a cluster are\npresented or judged. Intuitively, the judged relevant terms should\nreceive larger weights because they are explicitly indicated as \nrelevant by the user. Therefore, we try to combine the two methods,\nhoping to get the best out of both.\nWe do this by interpolating the TFB model with the CFB model,\nand call it TCFB:\np(w|\u00ce\u00b8q ) = \u00ce\u00b1p(w|\u00ce\u00b8qT F B\n) + (1 \u00e2\u02c6\u2019 \u00ce\u00b1)p(w|\u00ce\u00b8qCF B\n)\n6. EXPERIMENTS\nIn this section, we describe our experiment results. We first \ndescribe our experiment setup and present an overview of various\nmethods\" performance. Then we discuss the effects of varying\nthe parameter setting in the algorithms, as well as the number of\npresentation terms. Next we analyze user term feedback behavior\nand its relation to retrieval performance. Finally we compare term\nfeedback to relevance feedback and show that it has its particular\nadvantage.\n6.1 Experiment Setup and Basic Results\nWe took the opportunity of TREC 2005 HARD Track[2] for the\nevaluation of our algorithms. The tracks used the AQUAINT \ncollection, a 3GB corpus of English newswire text. The topics \nincluded 50 ones previously known to be hard, i.e. with low retrieval\nperformance. It is for these hard topics that user feedback is most\nhelpful, as it can provide information to disambiguate the queries;\nwith easy topics the user may be unwilling to spend efforts for\nfeedback if the automatic retrieval results are good enough. \nParticipants of the track were able to submit custom-designed clarification\nforms (CF) to solicit feedback from human assessors provided by\nTable 2: Retrieval performance for different methods and CF types. The last row is the percentage of MAP improvement over the\nbaseline. The parameter settings \u00ce\u00bc = 4, \u00ce\u00bb = 0.1, \u00ce\u00b1 = 0.3 are near optimal.\nBaseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C\nMAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304\nPr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473\nRR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906\n% 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8%\nTable 3: MAP variation with the number of presented terms.\n# terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C\n6 0.245 0.240 0.227 0.279 0.279 0.281 0.274\n12 0.261 0.261 0.242 0.299 0.286 0.297 0.281\n18 0.275 0.274 0.256 0.301 0.282 0.300 0.286\n24 0.276 0.281 0.265 0.303 0.292 0.305 0.292\n30 0.280 0.285 0.270 0.304 0.296 0.307 0.296\n36 0.282 0.288 0.272 0.307 0.297 0.309 0.297\n42 0.283 0.288 0.275 0.306 0.298 0.309 0.300\n48 0.288 0.288 0.278 0.305 0.301 0.309 0.303\nNIST. We designed three sets of clarification forms for term \nfeedback, differing in the choice of K, the number of clusters, and L,\nthe number of presented terms from each cluster. They are: 1\u00c3\u2014 48,\na big cluster with 48 terms, 3 \u00c3\u2014 16, 3 clusters with 16 terms each,\nand 6 \u00c3\u2014 8, 6 clusters with 8 terms each. The total number of \npresented terms (M) is fixed at 48, so by comparing the performance\nof different types of clarification forms we can know the effects of\ndifferent degree of clustering. For each topic, an assessor would\ncomplete the forms ordered by 6 \u00c3\u2014 8, 1 \u00c3\u2014 48 and 3 \u00c3\u2014 16, spending\nup to three minutes on each form. The sample clarification form\nshown in Figure 1 is of type 3 \u00c3\u2014 16. It is a simple and compact\ninterface in which the user can check relevant terms. The form is\nself-explanatory; there is no need for extra user training on how to\nuse it.\nOur initinal queries are constructed only using the topic title\ndescriptions, which are on average 2.7 words in length. As our\nbaseline we use the KL divergence retrieval method implemented\nin the Lemur Toolkit1\nwith 5 pseudo-feedback documents. We\nstem the terms, choose Dirichlet smoothing with a prior of 2000,\nand truncate query language models to 50 terms (these settings are\nused throughout the experiments). For all other parameters we use\nLemur\"s default settings. The baseline turns out to perform above\naverage among the track participants. After an initial run using this\nbaseline retrieval method, we take the top 60 documents for each\ntopic and apply the theme discovery algorithm to output the \nclusters (1, 3, or 6 of them), based on which we generate clarification\nforms. After user feedback is received, we run the term feedback\nalgorithms (TFB, CFB or TCFB) to estimate updated query \nmodels, which are then used for a second iteration of retrieval.\nWe evaluate the different retrieval methods\" performance on their\nrankings of the top 1000 documents. The evaluation metrics we\nadopt include mean average (non-interpolated) precision (MAP),\nprecision at top 30 (Pr@30) and total relevant retrieved (RR). Table\n2 shows the performance of various methods and configurations of\nK \u00c3\u2014 L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand\nfor the number of clusters (K). For example, TCFB3C means the\nTCFB method on the 3 \u00c3\u2014 16 clarification forms.\nFrom Table 2 we can make the following observations:\n1\nhttp://www.lemurproject.com\n1. All methods perform considerably better than the \npseudofeedback baseline, with TCFB3C achieving a highest 41.1%\nimprovement in MAP, indicating significant contribution of\nterm feedback for clarification of the user\"s information need.\nIn other words, term feedback is truly helpful for improving\nretrieval accuracy.\n2. For TFB, the performance is almost equal on the 1 \u00c3\u2014 48 and\n3 \u00c3\u2014 16 clarification forms in terms of MAP (although the\nlatter is slightly better in Pr@30 and RR), and a little worse\non the 6 \u00c3\u2014 8 ones.\n3. Both CFB3C and CFB6C perform better than their TFB \ncounterparts in all three metrics, suggesting that feedback on a\nsecondary cluster structure is indeed beneficial. CFB1C is\nactually worse because it cannot adjust the weight of its \n(single) cluster from term feedback and it is merely \npseudofeedback.\n4. Although TCFB is just a simple mixture of TFB and CFB\nby interpolation, it is able to outperform both. This supports\nour speculation that TCFB overcomes the drawbacks of TFB\n(paying attention only to checked terms) and CFB (not \ndistinguishing checked and unchecked terms in a cluster). \nExcept for TCFB6C v.s. CFB6C, the performance advantage\nof TCFB over TFB/CFB is significant at p < 0.05 using the\nWilcoxon signed rank test. This is not true in the case of TFB\nv.s. CFB, each of which is better than the other in nearly half\nof the topics.\n6.2 Reduction of Presentation Terms\nIn some situations we may have to reduce the number of \npresentation terms due to limits in display space or user feedback efforts.\nIt is interesting to know whether our algorithms\" performance \ndeteriorates when the user is presented with fewer terms. Because the\npresentation terms within each cluster are generated in decreasing\norder of their frequencies, the presentation list forms a subset of the\noriginal one if its size is reduced2\n. Therefore, we can easily \nsimulate what happens when the number of presentation terms decreases\n2\nThere are complexities arising from terms appearing in top L of\nmultiple clusters, but these are exceptions\nfrom M to M : we will keep all judgments of the top L = M /K\nterms in each cluster and discard those of others. Table 3 shows the\nperformance of various algorithms as the number of presentation\nterms ranges from 6 to 48.\nWe find that the performance of TFB is more susceptible to \npresentation term reduction than that of CFB or TCFB. For example,\nat 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while\nthe numbers for CFB3C and TCFB3C are 98.0% and 96.1% \nrespectively. We conjecture the reason to be that while TFB\"s \nperformance heavily depends on how many good terms are chosen\nfor query expansion, CFB only needs a rough estimate of cluster\nweights to work. Also, the 3 \u00c3\u2014 16 clarification forms seem to be\nmore robust than the 6 \u00c3\u2014 8 ones: at 12 terms the MAP of TFB6C is\n87.1% of that at 48 terms, lower than 90.6% for TFB3C. Similarly,\nfor CFB it is 95.0% against 98.0%. This is natual, as for a large\ncluster number of 6, it is easier to get into the situation where each\ncluster gets too few presentation terms to make topic diversification\nuseful.\nOverall, we are surprised to see that the algorithms are still able\nto perform reasonably well when the number of presentation terms\nis small. For example, at only 12 terms CFB3C (the clarification\nform is of size 3 \u00c3\u2014 4) can still improve 36.5% over the baseline,\ndropping slightly from 39.3% at 48 terms.\n6.3 User Feedback Analysis\nIn this part we study several aspects of user\"s term feedback \nbehavior, and whether they are connected to retrieval performance.\nFigure 2: Clarification form completion time distributions\n0\u00e2\u02c6\u201930 30\u00e2\u02c6\u201960 60\u00e2\u02c6\u201990 90\u00e2\u02c6\u2019120 120\u00e2\u02c6\u2019150 150\u00e2\u02c6\u2019180\n0\n5\n10\n15\n20\n25\n30\n35\ncompletion time (seconds)\n#topics\n1\u00c3\u201448\n3\u00c3\u201416\n6\u00c3\u20148\nFigure 2 shows the distribution of time needed to complete a\nclarification form3\n. We see that the user is usually able to finish\nterm feedback within a reasonably short amount of time: for more\nthan half of the topics the clarification form is completed in just\n1 minute, and only a small fraction of topics (less than 10% for\n1 \u00c3\u2014 48 and 3 \u00c3\u2014 16) take more than 2 minutes. This suggests that\nterm feedback is suitable for interactive ad-hoc retrieval, where a\nuser usually does not want to spend too much time on providing\nfeedback.\nWe find that a user often makes mistakes when judging term \nrelevance. Sometimes a relevant term may be left out because its \nconnection to the query topic is not obvious to the user. Other times a\ndubious term may be included but turns out to be irrelevant. Take\nthe topic in Figure 1 for example. There was a fire disaster in Mont\n3\nThe maximal time is 180 seconds, as the NIST assessor would be\nforced to submit the form at that moment.\nTable 4: Term selection statistics (topic average)\nCF Type 1 \u00c3\u2014 48 3 \u00c3\u2014 16 6 \u00c3\u2014 8\n# checked terms 14.8 13.3 11.2\n# rel. terms 15.0 12.6 11.2\n# rel. checked terms 7.9 6.9 5.9\nprecision 0.534 0.519 0.527\nrecall 0.526 0.548 0.527\nBlanc Tunnel between France and Italy in 1999, but the user failed\nto select such keywords as mont, blanc, french and italian\ndue to his/her ignorance of the event. Indeed, without proper \ncontext it would be hard to make perfect judgment.\nWhat is then, the extent to which the user is good at term \nfeedback? Does it have serious impact on retrieval performance? To \nanswer these questions, we need a measure of individual terms\" true\nrelevance. We adopt the Simplified KL Divergence metric used in\n[24] to decide query expansion terms as our term relevance \nmeasure:\n\u00cf\u0192KLD(w) = p(w|R) log\np(w|R)\np(w|\u00c2\u00acR)\nwhere p(w|R) is the probability that a relevant document contains\nterm w, and p(w|\u00c2\u00acR) is the probability that an irrelevant document\ncontains w, both of which can be easily computed via maximum\nlikelihood estimate given document-level relevance judgment. If\n\u00cf\u0192KLD(w) > 0, w is more likely to appear in relevant documents\nthan irrelevant ones.\nWe consider a term relevant if its Simplified KL Divergence\nvalue is greater than a certain threshold \u00cf\u01920. We can then define\nprecision and recall of user term judgment accordingly: precision\nis the fraction of terms checked by the user that are relevant; recall\nis the fraction of presented relevant terms that are checked by the\nuser. Table 4 shows the number of checked terms, relevant terms\nand relevant checked terms when \u00cf\u01920 is set to 1.0, as well as the\nprecision/recall of user term judgment.\nNote that when the clarification forms contain more clusters,\nfewer terms are checked: 14.8 for 1 \u00c3\u2014 48, 13.3 for 3 \u00c3\u2014 16 and\n11.2 for 6\u00c3\u20148. Similar pattern holds for relevant terms and relevant\nchecked terms. There seems to be a trade-off between increasing\ntopic diversity by clustering and losing extra relevant terms: when\nthere are more clusters, each of them gets fewer terms to present,\nwhich can hurt a major relevant cluster that contains many relevant\nterms. Therefore, it is not always helpful to have more clusters,\ne.g., TFB6C is actually worse than TFB1C.\nThe major finding we can make from Table 4 is that the user is\nnot particularly good at identifying relevant terms, which echoes\nthe discovery in [18]. In the case of 3 \u00c3\u2014 16 clarification forms, the\naverage number of terms checked as relevant by the user is 13.3\nper topic, and the average number of relevant terms whose \u00cf\u0192KLD\nvalue exceed 1.0 is 12.6. The user is able to recognize only 6.9\nof these terms on average. Indeed, the precision and recall of user\nfeedback terms (as defined previously) are far from perfect. On\nthe other hand, If the user had correctly checked all such relevant\nterms, the performance of our algorithms would have increased a\nlot, as shown in Table 5.\nWe see that TFB gets big improvement when there is an \noracle who checks all relevant terms, while CFB meets a bottleneck\naround MAP of 0.325, since all it does is adjust cluster weights,\nand when the learned weights are close to being accurate, it \ncannot benefit more from term feedback. Also note that TCFB fails to\noutperform TFB, probably because TFB is sufficiently accurate.\nTable 5: Change of MAP when using all (and only) relevant\nterms (\u00cf\u0192KLD > 1.0) for feedback.\noriginal term feedback relevant term feedback\nTF1 0.288 0.354\nTF3 0.288 0.354\nTF6 0.278 0.346\nCF3 0.305 0.325\nCF6 0.301 0.326\nTCF3 0.309 0.345\nTCF6 0.304 0.341\n6.4 Comparison with Relevance Feedback\nNow we compare term feedback with document-level relevance\nfeedback, in which the user is presented with the top N documents\nfrom an initial retrieval and asked to judge their relevance. The\nfeedback process is simulated using document relevance judgment\nfrom NIST. We use the mixture model based feedback method \nproposed in [25], with mixture noise set to 0.95 and feedback \ncoefficient set to 0.9.\nComparative evaluation of relevance feedback against other \nmethods is complicated by the fact that some documents have already\nbeen viewed during feedback, so it makes no sense to include them\nin the retrieval results of the second run. However, this does not\nhold for term feedback. Thus, to make it fair w.r.t. user\"s \ninformation gain, if the feedback documents are relevant, they should be\nkept in the top of the ranking; if they are irrelevant, they should be\nleft out. Therefore, we use relevance feedback to produce a ranking\nof top 1000 retrieved documents but with every feedback document\nexcluded, and then prepend the relevant feedback documents at the\nfront. Table 6 shows the performance of relevance feedback for\ndifferent values of N and compares it with TCFB3C.\nTable 6: Performance of relevance feedback for different \nnumber of feedback documents (N).\nN MAP Pr@30 RR\n5 0.302 0.586 4779\n10 0.345 0.670 4916\n20 0.389 0.772 5004\nTCFB3C 0.309 0.491 4947\nWe see that the performance of TCFB3C is comparable to that\nof relevance feedback using 5 documents. Although it is poorer\nthan when there are 10 feedback documents in terms of MAP and\nPr@30, it does retrieve more documents (4947) when going down\nthe ranked list.\nWe try to compare the quality of automatically inserted terms\nin relevance feedback with that of manually selected terms in term\nfeedback. This is done by truncating the relevance feedback \nmodified query model to a size equal to the number of checked terms\nfor the same topic. We can then compare the terms in the truncated\nmodel with the checked terms. Figure 3 shows the distribution of\nthe terms\" \u00cf\u0192KLD scores.\nWe find that term feedback tends to produce expansion terms\nof higher quality(those with \u00cf\u0192KLD > 1) compared to relevance\nfeedback (with 10 feedback documents). This does not contradict\nthe fact that the latter yields higher retrieval performance. Actually,\nwhen we use the truncated query model instead of the intact one\nrefined from relevance feedback, the MAP is only 0.304. The truth\nFigure 3: Comparison of expansion term quality between \nrelevance feedback (with 10 feedback documents) and term \nfeedback (with 3 \u00c3\u2014 16 CFs)\n\u00e2\u02c6\u20191\u00e2\u02c6\u20190 0\u00e2\u02c6\u20191 1\u00e2\u02c6\u20192 2\u00e2\u02c6\u20193 3\u00e2\u02c6\u20194 4\u00e2\u02c6\u20195 5\u00e2\u02c6\u20196\n0\n50\n100\n150\n200\n250\n300\n350\n\u00cf\u0192KLD\n#terms\nrelevance feedback\nterm feedback\nis, although there are many unwanted terms in the expanded query\nmodel from feedback documents, there are also more relevant terms\nthan what the user can possibly select from the list of presentation\nterms generated with pseudo-feedback documents, and the positive\neffects often outweights the negative ones.\nWe are interested to know under what circumstances term \nfeedback has advantage over relevance feedback. One such situation is\nwhen none of the top N feedback documents is relevant, rendering\nrelevance feedback useless. This is not infrequent, as one might\nhave thought: out of the 50 topics, there are 13 such cases when\nN = 5, 10 when N = 10, and still 3 when N = 20. When this\nhappens, one can only back off to the original retrieval method; the\npower of relevance feedback is lost.\nSurprisingly, in 11 out of 13 such cases where relevance \nfeedback seems impossible, the user is able to check at least 2 \nrelevant terms from the 3 \u00c3\u2014 16 clarification form (we consider term\nt to be relevant if \u00cf\u0192KLD(t) > 1.0). Furthermore, in 10 out of\nthem TCFB3C outperforms the pseudo-feedback baseline, \nincreasing MAP from 0.076 to 0.146 on average (these are particularly\nhard topics). We think that there are two possible explanations for\nthis phenomenon of term feedback being active even when \nrelevance feedback does not work: First, even if none of the top N\n(suppose it is a small number) documents are relevant, we may\nstill find relevant documents in top 60, which is more inclusive but\nusually unreachable when people are doing relevance feedback in\ninteractive ad-hoc search, from which we can draw feedback terms.\nThis is true for topic 367 piracy, where the top 10 feedback \ndocuments are all about software piracy, yet there are documents \nbetween 10-60 that are about piracy on the seas (which is about the\nreal information need), contributing terms such as pirate, ship\nfor selection in the clarification form. Second, for some topics,\na document needs to meet some special condition in order to be\nrelevant. The top N documents may be related to the topic, but\nnonetheless irrelevant. In this case, we may still extract useful\nterms from these documents, even if they do not qualify as \nrelevant ones. For example, in topic 639 consumer online shopping,\na document needs to mention what contributes to shopping growth\nto really match the specified information need, hence none of the\ntop 10 feedback documents are regarded as relevant. But \nnevertheless, the feedback terms such as retail, commerce are good for\nquery expansion.\n7. CONCLUSIONS\nIn this paper we studied the use of term feedback for \ninteractive information retrieval in the language modeling approach. We\nproposed a cluster-based method for selecting presentation terms\nas well as algorithms to estimate refined query models from user\nterm feedback. We saw significant improvement in retrieval \naccuracy brought by term feedback, in spite of the fact that a user often\nmakes mistakes in relevance judgment that hurts its performance.\nWe found the best-performing algorithm to be TCFB, which \nbenefits from the combination of directly observed term evidence with\nTFB and indirectly learned cluster relevance with CFB. When we\nreduced the number of presentation terms, term feedback is still\nable to keep much of its performance gain over the baseline. \nFinally, we compared term feedback to document-level relevance \nfeedback, and found that TCFB3C\"s performance is on a par with the\nlatter with 5 feedback documents. We regarded term feedback as a\nviable alternative to traditional relevance feedback, especially when\nthere are no relevant documents in the top.\nWe propose to extend our work in several ways. First, we want\nto study whether the use of various contexts can help the user to\nbetter identify term relevance, while not sacrificing the simplicity\nand compactness of term feedback. Second, currently all terms are\npresented to the user in a single batch. We could instead consider \niterative term feedback, by presenting a small number of terms first,\nand show more terms after receiving user feedback or stop when\nthe refined query is good enough. The presented terms should be\nselected dynamically to maximize learning benefits at any moment.\nThird, we have plans to incorporate term feedback into our UCAIR\ntoolbar[20], an Internet Explorer plugin, to make it work for web\nsearch. We are also interested in studying how to combine term\nfeedback with relevance feedback or implicit feedback. We could,\nfor example, allow the user to dynamically modify terms in a \nlanguage model learned from feedback documents.\n8. ACKNOWLEDGMENT\nThis work is supported in part by the National Science \nFoundation grants IIS-0347933 and IIS-0428472.\n9. REFERENCES\n[1] J. Allan. Relevance feedback with too much data. In Proceedings of\nthe 18th annual international ACM SIGIR conference on research\nand development in information retrieval, pages 337-343, 1995.\n[2] J. Allan. HARD track overview in TREC 2005 - High Accuracy\nRetrieval from Documents. In The Fourteenth Text REtrieval\nConference, 2005.\n[3] P. Anick. Using terminological feedback for web search refinement:\na log-based study. In Proceedings of the 26th annual international\nACM SIGIR conference on research and development in informaion\nretrieval, pages 88-95, 2003.\n[4] P. G. Anick and S. Tipirneni. The paraphrase search assistant:\nterminological feedback for iterative information seeking. In\nProceedings of the 22nd annual international ACM SIGIR\nconference on research and development in information retrieval,\npages 153-159, 1999.\n[5] C. Buckley, G. Salton, J. Allan, and A. Singhal. Automatic query\nexpansion using SMART. In Proceedings of the Third Text REtrieval\nConference, 1994.\n[6] D. Harman. Towards interactive query expansion. In Proceedings of\nthe 11th annual international ACM SIGIR conference on research\nand development in information retrieval, pages 321-331, 1988.\n[7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and\nJ. Allan. UMass at TREC 2003: HARD and QA. In TREC, pages\n715-725, 2003.\n[8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu. Hierarchical\npresentation of expansion terms. In Proceedings of the 2002 ACM\nsymposium on applied computing, pages 645-649, 2002.\n[9] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of\ninformation retrieval: development and status. Technical Report 446,\nComputer Laboratory, University of Cambridge, 1998.\n[10] D. Kelly, V. D. Dollu, and X. Fu. The loquacious user: a\ndocument-independent source of terms for query expansion. In\nProceedings of the 28th annual international ACM SIGIR\nconference on research and development in information retrieval,\npages 457-464, 2005.\n[11] D. Kelly and X. Fu. Elicitation of term relevance feedback: an\ninvestigation of term source and context. In Proceedings of the 29th\nannual international ACM SIGIR conference on research and\ndevelopment in information retrieval, 2006.\n[12] J. Koenemann and N. Belkin. A case for interaction: A study of\ninteractive information retrieval behavior and effectiveness. In\nProceedings of the SIGCHI conference on human factors in\ncomputing systems, pages 205-212, 1996.\n[13] V. Lavrenko and W. B. Croft. Relevance-based language models. In\nResearch and Development in Information Retrieval, pages\n120-127, 2001.\n[14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon. Evaluation of the real\nand perceived value of automatic and interactive query expansion. In\nProceedings of the 27th annual international ACM SIGIR\nconference on research and development in information retrieval,\npages 526-527, 2004.\n[15] J. Ponte. A Language Modeling Approach to Information Retrieval.\nPhD thesis, University of Massachusetts at Amherst, 1998.\n[16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.\nOkapi at TREC-3. In Proceedings of the Third Text REtrieval\nConference, 1994.\n[17] J. Rocchio. Relevance feedback in information retrieval. In The\nSMART retrieval system, pages 313-323. 1971.\n[18] I. Ruthven. Re-examining the potential effectiveness of interactive\nquery expansion. In Proceedings of the 26th annual international\nACM SIGIR conference on research and development in informaion\nretrieval, pages 213-220, 2003.\n[19] G. Salton and C. Buckley. Improving retrieval performance by\nrelevance feedback. Journal of the American Society for Information\nScience, 41:288-297, 1990.\n[20] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for\npersonalized search. In Proceedings of the 14th ACM international\nconference on information and knowledge management, pages\n824-831, 2005.\n[21] X. Shen and C. Zhai. Active feedback in ad-hoc information\nretrieval. In Proceedings of the 28th annual international ACM\nSIGIR conference on research and development in information\nretrieval, pages 59-66, 2005.\n[22] A. Spink. Term relevance feedback and query expansion: relation to\ndesign. In Proceedings of the 17th annual international ACM SIGIR\nconference on research and development in information retrieval,\npages 81-90, 1994.\n[23] J. Xu and W. B. Croft. Query expansion using local and global\ndocument analysis. In Proceedings of the 19th annual international\nACM SIGIR conference on research and development in information\nretrieval, pages 4-11, 1996.\n[24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.\nMicrosoft cambridge at TREC-13: Web and HARD tracks. In\nProceedings of the 13th Text REtrieval Conference, 2004.\n[25] C. Zhai and J. Lafferty. Model-based feedback in the language\nmodeling approach to information retrieval. In Proceedings of the\ntenth international conference on information and knowledge\nmanagement, pages 403-410, 2001.\n[26] C. Zhai, A. Velivelli, and B. Yu. A cross-collection mixture model\nfor comparative text mining. In Proceedings of the tenth ACM\nSIGKDD international conference on knowledge discovery and data\nmining, pages 743-748, 2004.\n": ["term-based feedback", "information retrieval", "language modeling", "query expansion process", "query model", "interactive adhoc search", "retrieval performance", "probability", "kl-divergence", "presentation term", "query expansion", "interactive retrieval", ""]}