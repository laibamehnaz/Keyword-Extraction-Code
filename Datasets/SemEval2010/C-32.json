{"BuddyCache: High-Performance Object Storage\nfor Collaborative Strong-Consistency Applications\nin a WAN\n\u00e2\u02c6\u2014\nMagnus E. Bjornsson and Liuba Shrira\nDepartment of Computer Science\nBrandeis University\nWaltham, MA 02454-9110\n{magnus, liuba}@cs.brandeis.edu\nABSTRACT\nCollaborative applications provide a shared work \nenvironment for groups of networked clients collaborating on a \ncommon task. They require strong consistency for shared \npersistent data and efficient access to fine-grained objects. These\nproperties are difficult to provide in wide-area networks \nbecause of high network latency.\nBuddyCache is a new transactional caching approach that\nimproves the latency of access to shared persistent objects\nfor collaborative strong-consistency applications in \nhigh-latency network environments. The challenge is to improve\nperformance while providing the correctness and availability\nproperties of a transactional caching protocol in the presence\nof node failures and slow peers.\nWe have implemented a BuddyCache prototype and \nevaluated its performance. Analytical results, confirmed by\nmeasurements of the BuddyCache prototype using the \nmultiuser 007 benchmark indicate that for typical Internet \nlatencies, e.g. ranging from 40 to 80 milliseconds round trip time\nto the storage server, peers using BuddyCache can reduce by\nup to 50% the latency of access to shared objects compared\nto accessing the remote servers directly.\nCategories and Subject Descriptors\nC.2.4 [Computer Systems Organization]: Distributed\nSystems\nGeneral Terms\nDesign, Performance\n1. INTRODUCTION\nImprovements in network connectivity erode the \ndistinction between local and wide-area computing and, \nincreasingly, users expect their work environment to follow them\nwherever they go. Nevertheless, distributed applications\nmay perform poorly in wide-area network environments. \nNetwork bandwidth problems will improve in the foreseeable \nfuture, but improvement in network latency is fundamentally\nlimited. BuddyCache is a new object caching technique that\naddresses the network latency problem for collaborative \napplications in wide-area network environment.\nCollaborative applications provide a shared work \nenvironment for groups of networked users collaborating on a \ncommon task, for example a team of engineers jointly \noverseeing a construction project. Strong-consistency collaborative\napplications, for example CAD systems, use client/server\ntransactional object storage systems to ensure consistent \naccess to shared persistent data. Up to now however, users\nhave rarely considered running consistent network storage\nsystems over wide-area networks as performance would be\nunacceptable [24]. For transactional storage systems, the\nhigh cost of wide-area network interactions to maintain data\nconsistency is the main cost limiting the performance and\ntherefore, in wide-area network environments, collaborative\napplications have been adapted to use weaker consistency\nstorage systems [22]. Adapting an application to use weak\nconsistency storage system requires significant effort since\nthe application needs to be rewritten to deal with a \ndifferent storage system semantics. If shared persistent objects\ncould be accessed with low-latency, a new field of distributed\nstrong-consistency applications could be opened.\nCooperative web caching [10, 11, 15] is a well-known \napproach to reducing client interaction with a server by \nallowing one client to obtain missing objects from a another client\ninstead of the server. Collaborative applications seem a \nparticularly good match to benefit from this approach since one\nof the hard problems, namely determining what objects are\ncached where, becomes easy in small groups typical of \ncollaborative settings. However, cooperative web caching \ntechniques do not provide two important properties needed by\ncollaborative applications, strong consistency and efficient\n26\naccess to fine-grained objects. Cooperative object caching\nsystems [2] provide these properties. However, they rely on\ninteraction with the server to provide fine-grain cache \ncoherence that avoids the problem of false sharing when accesses\nto unrelated objects appear to conflict because they occur\non the same physical page. Interaction with the server \nincreases latency. The contribution of this work is extending\ncooperative caching techniques to provide strong consistency\nand efficient access to fine-grain objects in wide-area \nenvironments.\nConsider a team of engineers employed by a construction\ncompany overseeing a remote project and working in a shed\nat the construction site. The engineers use a collaborative\nCAD application to revise and update complex project \ndesign documents. The shared documents are stored in \ntransactional repository servers at the company home site. The\nengineers use workstations running repository clients. The\nworkstations are interconnected by a fast local Ethernet but\nthe network connection to the home repository servers is\nslow. To improve access latency, clients fetch objects from\nrepository servers and cache and access them locally. A\ncoherence protocol ensures that client caches remain \nconsistent when objects are modified. The performance problem\nfacing the collaborative application is coordinating with the\nservers consistent access to shared objects.\nWith BuddyCache, a group of close-by collaborating clients,\nconnected to storage repository via a high-latency link, can\navoid interactions with the server if needed objects, updates\nor coherency information are available in some client in the\ngroup.\nBuddyCache presents two main technical challenges. One\nchallenge is how to provide efficient access to shared \nfinegrained objects in the collaborative group without imposing\nperformance overhead on the entire caching system. The\nother challenge is to support fine-grain cache coherence in\nthe presence of slow and failed nodes.\nBuddyCache uses a redirection approach similar to one\nused in cooperative web caching systems [11]. A \nredirector server, interposed between the clients and the remote\nservers, runs on the same network as the collaborating group\nand, when possible, replaces the function of the remote\nservers. If the client request can not be served locally, the\nredirector forwards it to a remote server. When one of the\nclients in the group fetches a shared object from the \nrepository, the object is likely to be needed by other clients. \nBuddyCache redirects subsequent requests for this object to the\ncaching client. Similarly, when a client creates or modifies\na shared object, the new data is likely to be of potential\ninterest to all group members. BuddyCache uses \nredirection to support peer update, a lightweight application-level\nmulticast technique that provides group members with \nconsistent access to the new data committed within the \ncollaborating group without imposing extra overhead outside the\ngroup.\nNevertheless, in a transactional system, redirection \ninterferes with shared object availability. Solo commit, is a\nvalidation technique used by BuddyCache to avoid the \nundesirable client dependencies that reduce object availability\nwhen some client nodes in the group are slow, or clients fail\nindependently. A salient feature of solo commit is \nsupporting fine-grained validation using inexpensive coarse-grained\ncoherence information.\nSince redirection supports the performance benefits of \nreducing interaction with the server but introduces extra \nprocessing cost due to availability mechanisms and request \nforwarding, this raises the question is the cure worse than the\ndisease? We designed and implemented a BuddyCache\nprototype and studied its performance benefits and costs\nusing analytical modeling and system measurements. We\ncompared the storage system performance with and without\nBuddyCache and considered how the cost-benefit balance is\naffected by network latency.\nAnalytical results, supported by measurements based on\nthe multi-user 007 benchmark, indicate that for typical \nInternet latencies BuddyCache provides significant performance\nbenefits, e.g. for latencies ranging from 40 to 80 milliseconds\nround trip time, clients using the BuddyCache can reduce\nby up to 50% the latency of access to shared objects \ncompared to the clients accessing the repository directly. These\nstrong performance gains could make transactional object\nstorage systems more attractive for collaborative \napplications in wide-area environments.\n2. RELATED WORK\nCooperative caching techniques [20, 16, 13, 2, 28] provide\naccess to client caches to avoid high disk access latency in\nan environment where servers and clients run on a fast local\narea network. These techniques use the server to provide\nredirection and do not consider issues of high network \nlatency.\nMultiprocessor systems and distributed shared memory\nsystems [14, 4, 17, 18, 5] use fine-grain coherence techniques\nto avoid the performance penalty of false sharing but do not\naddress issues of availability when nodes fail.\nCooperative Web caching techniques, (e.g. [11, 15]) \ninvestigate issues of maintaining a directory of objects cached\nin nearby proxy caches in wide-area environment, using \ndistributed directory protocols for tracking cache changes. This\nwork does not consider issues of consistent concurrent \nupdates to shared fine-grained objects.\nCheriton and Li propose MMO [12] a hybrid web \ncoherence protocol that combines invalidations with updates\nusing multicast delivery channels and receiver-reliable \nprotocol, exploiting locality in a way similar to BuddyCache.\nThis multicast transport level solution is geared to the single\nwriter semantics of web objects. In contrast, BuddyCache\nuses application level multicast and a sender-reliable \ncoherence protocol to provide similar access latency \nimprovements for transactional objects. Application level multicast\nsolution in a middle-ware system was described by \nPendarakis, Shi and Verma in [27]. The schema supports small\nmulti-sender groups appropriate for collaborative \napplications and considers coherence issues in the presence of \nfailures but does not support strong consistency or fine-grained\nsharing.\nYin, Alvisi, Dahlin and Lin [32, 31] present a \nhierarchical WAN cache coherence scheme. The protocol uses\nleases to provide fault-tolerant call-backs and takes \nadvantage of nearby caches to reduce the cost of lease extensions.\nThe study uses simulation to investigate latency and fault\ntolerance issues in hierarchical avoidance-based coherence\nscheme. In contrast, our work uses implementation and\nanalysis to evaluate the costs and benefits of redirection\nand fine grained updates in an optimistic system. \nAnderson, Eastham and Vahdat in WebFS [29] present a global\nfile system coherence protocol that allows clients to choose\n27\non per file basis between receiving updates or invalidations.\nUpdates and invalidations are multicast on separate \nchannels and clients subscribe to one of the channels. The \nprotocol exploits application specific methods e.g. last-writer-wins\npolicy for broadcast applications, to deal with concurrent\nupdates but is limited to file systems.\nMazieres studies a bandwidth saving technique [24] to \ndetect and avoid repeated file fragment transfers across a WAN\nwhen fragments are available in a local cache. BuddyCache\nprovides similar bandwidth improvements when objects are\navailable in the group cache.\n3. BUDDYCACHE\nHigh network latency imposes performance penalty for\ntransactional applications accessing shared persistent \nobjects in wide-area network environment. This section \ndescribes the BuddyCache approach for reducing the network\nlatency penalty in collaborative applications and explains\nthe main design decisions.\nWe consider a system in which a distributed transactional\nobject repository stores objects in highly reliable servers,\nperhaps outsourced in data-centers connected via \nhigh-bandwidth reliable networks. Collaborating clients interconnected\nvia a fast local network, connect via high-latency, possibly\nsatellite, links to the servers at the data-centers to access\nshared persistent objects. The servers provide disk storage\nfor the persistent objects. A persistent object is owned by\na single server. Objects may be small (order of 100 bytes\nfor programming language objects [23]). To amortize the\ncost of disk and network transfer objects are grouped into\nphysical pages.\nTo improve object access latency, clients fetch the objects\nfrom the servers and cache and access them locally. A \ntransactional cache coherence protocol runs at clients and servers\nto ensure that client caches remain consistent when objects\nare modified. The performance problem facing the \ncollaborating client group is the high latency of coordinating \nconsistent access to the shared objects.\nBuddyCache architecture is based on a request \nredirection server, interposed between the clients and the remote\nservers. The interposed server (the redirector) runs on the\nsame network as the collaborative group and, when possible,\nreplaces the function of the remote servers. If the client \nrequest can be served locally, the interaction with the server is\navoided. If the client request can not be served locally, \nredirector forwards it to a remote server. Redirection approach\nhas been used to improve the performance of web caching\nprotocols. BuddyCache redirector supports the correctness,\navailability and fault-tolerance properties of transactional\ncaching protocol [19]. The correctness property ensures \nonecopy serializability of the objects committed by the client\ntransactions. The availability and fault-tolerance properties\nensure that a crashed or slow client does not disrupt any\nother client\"s access to persistent objects.\nThe three types of client server interactions in a \ntransactional caching protocol are the commit of a transaction, the\nfetch of an object missing in a client cache, and the exchange\nof cache coherence information. BuddyCache avoids \ninteractions with the server when a missing object, or cache \ncoherence information needed by a client is available within the\ncollaborating group. The redirector always interacts with\nthe servers at commit time because only storage servers \nprovide transaction durability in a way that ensures committed\nClient\nRedirector\nClient\nClient\nBuddy Group\nClient\nRedirector\nClient\nClient\nBuddy Group\nServers\nFigure 1: BuddyCache.\ndata remains available in the presence of client or redirector\nfailures. Figure 1 shows the overall BuddyCache \narchitecture.\n3.1 Cache Coherence\nThe redirector maintains a directory of pages cached at\neach client to provide cooperative caching [20, 16, 13, 2,\n28], redirecting a client fetch request to another client that\ncaches the requested object. In addition, redirector manages\ncache coherence.\nSeveral efficient transactional cache coherence protocols [19]\nexist for persistent object storage systems. Protocols make\ndifferent choices in granularity of data transfers and \ngranularity of cache consistency. The current best-performing\nprotocols use page granularity transfers when clients fetch\nmissing objects from a server and object granularity \ncoherence to avoid false (page-level) conflicts. The \ntransactional caching taxonomy [19] proposed by Carey, Franklin\nand Livny classifies the coherence protocols into two main\ncategories according to whether a protocol avoids or detects\naccess to stale objects in the client cache. The BuddyCache\napproach could be applied to both categories with different\nperformance costs and benefits in each category.\nWe chose to investigate BuddyCache in the context of\nOCC [3], the current best performing detection-based \nprotocol. We chose OCC because it is simple, performs well in\nhigh-latency networks, has been implemented and we had\naccess to the implementation. We are investigating \nBuddyCache with PSAA [33], the best performing \navoidancebased protocol. Below we outline the OCC protocol [3]. The\nOCC protocol uses object-level coherence. When a client \nrequests a missing object, the server transfers the containing\npage. Transaction can read and update locally cached \nobjects without server intervention. However, before a \ntransaction commits it must be validated; the server must make\nsure the validating transaction has not read a stale version\nof some object that was updated by a successfully \ncommitted or validated transaction. If validation fails, the \ntransaction is aborted. To reduce the number and cost of aborts,\n28\nHelper Requester\nA:p\nFetch pPeer fetch p\nPage p\nRedirector\nFigure 2: Peer fetch\na server sends background object invalidation messages to\nclients caching the containing pages. When clients receive\ninvalidations they remove stale objects from the cache and\nsend background acknowledgments to let server know about\nthis.\nSince invalidations remove stale objects from the client\ncache, invalidation acknowledgment indicates to the server\nthat a client with no outstanding invalidations has read \nupto-date objects. An unacknowledged invalidation indicates\na stale object may have been accessed in the client cache.\nThe validation procedure at the server aborts a client \ntransaction if a client reads an object while an invalidation is\noutstanding.\nThe acknowledged invalidation mechanism supports \nobject-level cache coherence without object-based directories\nor per-object version numbers. Avoiding per-object \noverheads is very important to reduce performance penalties [3]\nof managing many small objects, since typical objects are\nsmall. An important BuddyCache design goal is to \nmaintain this benefit.\nSince in BuddyCache a page can be fetched into a client\ncache without server intervention (as illustrated in figure 2),\ncache directories at the servers keep track of pages cached in\neach collaborating group rather than each client. Redirector\nkeeps track of pages cached in each client in a group. Servers\nsend to the redirector invalidations for pages cached in the\nentire group. The redirector propagates invalidations from\nservers to affected clients. When all affected clients \nacknowledge invalidations, redirector can propagate the group \nacknowledgment to the server.\n3.2 Light-weight Peer Update\nWhen one of the clients in the collaborative group creates\nor modifies shared objects, the copies cached by any other\nclient become stale but the new data is likely to be of \npotential interest to the group members. The goal in BuddyCache\nis to provide group members with efficient and consistent\naccess to updates committed within the group without \nimposing extra overhead on other parts of the storage system.\nThe two possible approaches to deal with stale data are\ncache invalidations and cache updates. Cache coherence\nstudies in web systems (e.g. [7]) DSM systems (e.g. [5]),\nand transactional object systems (e.g. [19]) compare the\nbenefits of update and invalidation. The studies show the\nCommitting\nClient\nServer\nRedirector\nx2. Store x\n6. Update x\n3. Commit x\n4. Commit OK\n5. Commit OK1. Commit x\nFigure 3: Peer update.\nbenefits are strongly workload-dependent. In general, \ninvalidation-based coherence protocols are efficient since \ninvalidations are small, batched and piggybacked on other\nmessages. Moreover, invalidation protocols match the \ncurrent hardware trend for increasing client cache sizes. Larger\ncaches are likely to contain much more data than is actively\nused. Update-based protocols that propagate updates to\nlow-interest objects in a wide-area network would be \nwasteful. Nevertheless, invalidation-based coherence protocols\ncan perform poorly in high-latency networks [12] if the \nobject\"s new value is likely to be of interest to another group\nmember. With an invalidation-based protocol, one \nmember\"s update will invalidate another member\"s cached copy,\ncausing the latter to perform a high-latency fetch of the new\nvalue from the server.\nBuddyCache circumvents this well-known bandwidth vs.\nlatency trade-off imposed by update and invalidation \nprotocols in wide-area network environments. It avoids the\nlatency penalty of invalidations by using the redirector to\nretain and propagate updates committed by one client to\nother clients within the group. This avoids the bandwidth\npenalty of updates because servers propagate invalidations\nto the redirectors. As far as we know, this use of localized\nmulticast in BuddyCache redirector is new and has not been\nused in earlier caching systems.\nThe peer update works as follows. An update commit \nrequest from a client arriving at the redirector contains the\nobject updates. Redirector retains the updates and \npropagates the request to the coordinating server. After the \ntransaction commits, the coordinator server sends a commit reply\nto the redirector of the committing client group. The \nredirector forwards the reply to the committing client, and also\npropagates the retained committed updates to the clients\ncaching the modified pages (see figure 3). Since the groups\noutside the BuddyCache propagate invalidations, there is no\nextra overhead outside the committing group.\n3.3 Solo commit\nIn the OCC protocol, clients acknowledge server \ninvalidations (or updates) to indicate removal of stale data. The\nstraightforward group acknowledgement protocol where\nredirector collects and propagates a collective \nacknowledge29\nRedirector\ncommit ok\nABORT\nClient 1 Client 2 Server\ncommit (P(x))\ncommit (P(x))\nok + inv(P(x))\ninv(P(x))\ncommit(P(x))\ncommit(P(x))\nack(P(x))\nack(P(x))\nFigure 4: Validation with Slow Peers\nment to the server, interferes with the availability property\nof the transactional caching protocol [19] since a client that\nis slow to acknowledge an invalidation or has failed can \ndelay a group acknowledgement and prevent another client in\nthe group from committing a transaction. E.g. an engineer\nthat commits a repeated revision to the same shared design\nobject (and therefore holds the latest version of the object)\nmay need to abort if the group acknowledgement has not\npropagated to the server.\nConsider a situation depicted in figure 4 where Client1\ncommits a transaction T that reads the latest version of\nan object x on page P recently modified by Client1. If the\ncommit request for T reaches the server before the collective\nacknowledgement from Client2 for the last modification of x\narrives at the server, the OCC validation procedure \nconsiders x to be stale and aborts T (because, as explained above,\nan invalidation unacknowledged by a client, acts as \nindication to the server that the cached object value is stale at the\nclient).\nNote that while invalidations are not required for the \ncorrectness of the OCC protocol, they are very important for\nthe performance since they reduce the performance \npenalties of aborts and false sharing. The asynchronous \ninvalidations are an important part of the reason OCC has \ncompetitive performance with PSAA [33], the best performing\navoidance-based protocol [3].\nNevertheless, since invalidations are sent and processed\nasynchronously, invalidation processing may be arbitrarily\ndelayed at a client. Lease-based schemes (time-out based)\nhave been proposed to improve the availability of \nhierarchical callback-based coherence protocols [32] but the \nasynchronous nature of invalidations makes the lease-based \napproaches inappropriate for asynchronous invalidations.\nThe Solo commit validation protocol allows a client with\nup-to-date objects to commit a transaction even if the group\nacknowledgement is delayed due to slow or crashed peers.\nThe protocol requires clients to include extra information\nwith the transaction read sets in the commit message, to\nindicate to the server the objects read by the transaction\nare up-to-date.\nObject version numbers could provide a simple way to\ntrack up-to-date objects but, as mentioned above, \nmaintaining per object version numbers imposes unacceptably high\noverheads (in disk storage, I/O costs and directory size) on\nthe entire object system when objects are small [23]. \nInstead, solo commit uses coarse-grain page version numbers\nto identify fine-grain object versions. A page version number\nis incremented at a server when at transaction that modifies\nobjects on the page commits. Updates committed by a \nsingle transaction and corresponding invalidations are therefore\nuniquely identified by the modified page version number.\nPage version numbers are propagated to clients in fetch\nreplies, commit replies and with invalidations, and clients\ninclude page version numbers in commit requests sent to\nthe servers. If a transaction fails validation due to missing\ngroup acknowledgement, the server checks page version\nnumbers of the objects in the transaction read set and allows\nthe transaction to commit if the client has read from the\nlatest page version.\nThe page version numbers enable independent commits\nbut page version checks only detect page-level conflicts. To\ndetect object-level conflicts and avoid the problem of false\nsharing we need the acknowledged invalidations. Section 4\ndescribes the details of the implementation of solo commit\nsupport for fine-grain sharing.\n3.4 Group Configuration\nThe BuddyCache architecture supports multiple \nconcurrent peer groups. Potentially, it may be faster to access\ndata cached in another peer group than to access a remote\nserver. In such case extending BuddyCache protocols to\nsupport multi-level peer caching could be worthwhile. We\nhave not pursued this possibility for several reasons.\nIn web caching workloads, simply increasing the \npopulation of clients in a proxy cache often increases the \noverall cache hit rate [30]. In BuddyCache applications, \nhowever, we expect sharing to result mainly from explicit client\ninteraction and collaboration, suggesting that inter-group\nfetching is unlikely to occur. Moreover, measurements from\nmulti-level web caching systems [9] indicate that a \nmultilevel system may not be advantageous unless the network\nconnection between the peer groups is very fast. We are\nprimarily interested in environments where closely \ncollaborating peers have fast close-range connectivity, but the \nconnection between peer groups may be slow. As a result, we\ndecided that support for inter-group fetching in BuddyCache\nis not a high priority right now.\nTo support heterogenous resource-rich and resource-poor\npeers, the BuddyCache redirector can be configured to run\neither in one of the peer nodes or, when available, in a \nseparate node within the site infrastructure. Moreover, in a\nresource-rich infrastructure node, the redirector can be \nconfigured as a stand-by peer cache to receive pages fetched by\nother peers, emulating a central cache somewhat similar to\na regional web proxy cache. From the BuddyCache cache\ncoherence protocol point of view, however, such a stand-by\npeer cache is equivalent to a regular peer cache and therefore\nwe do not consider this case separately in the discussion in\nthis paper.\n4. IMPLEMENTATION\nIn this section we provide the details of the BuddyCache\nimplementation. We have implemented BuddyCache in the\nThor client/server object-oriented database [23]. Thor \nsupports high performance access to distributed objects and\ntherefore provides a good test platform to investigate \nBuddyCache performance.\n30\n4.1 Base Storage System\nThor servers provide persistent storage for objects and\nclients cache copies of these objects. Applications run at\nthe clients and interact with the system by making calls on\nmethods of cached objects. All method calls occur within\natomic transactions. Clients communicate with servers to\nfetch pages or to commit a transaction.\nThe servers have a disk for storing persistent objects, a\nstable transaction log, and volatile memory. The disk is\norganized as a collection of pages which are the units of\ndisk access. The stable log holds commit information and\nobject modifications for committed transactions. The server\nmemory contains cache directory and a recoverable modified\nobject cache called the MOB. The directory keeps track of\nwhich pages are cached by which clients. The MOB holds\nrecently modified objects that have not yet been written\nback to their pages on disk. As MOB fills up, a background\nprocess propagates modified objects to the disk [21, 26].\n4.2 Base Cache Coherence\nTransactions are serialized using optimistic concurrency\ncontrol OCC [3] described in Section 3.1. We provide some\nof the relevant OCC protocol implementation details. The\nclient keeps track of objects that are read and modified by its\ntransaction; it sends this information, along with new copies\nof modified objects, to the servers when it tries to commit\nthe transaction. The servers determine whether the commit\nis possible, using a two-phase commit protocol if the \ntransaction used objects at multiple servers. If the transaction\ncommits, the new copies of modified objects are appended\nto the log and also inserted in the MOB. The MOB is \nrecoverable, i.e. if the server crashes, the MOB is reconstructed\nat recovery by scanning the log.\nSince objects are not locked before being used, a \ntransaction commit can cause caches to contain obsolete objects.\nServers will abort a transaction that used obsolete objects.\nHowever, to reduce the probability of aborts, servers notify\nclients when their objects become obsolete by sending them\ninvalidation messages; a server uses its directory and the\ninformation about the committing transaction to determine\nwhat invalidation messages to send. Invalidation messages\nare small because they simply identify obsolete objects. \nFurthermore, they are sent in the background, batched and \npiggybacked on other messages.\nWhen a client receives an invalidation message, it removes\nobsolete objects from its cache and aborts the current \ntransaction if it used them. The client continues to retain pages\ncontaining invalidated objects; these pages are now \nincomplete with holes in place of the invalidated objects. \nPerforming invalidation on an object basis means that false\nsharing does not cause unnecessary aborts; keeping \nincomplete pages in the client cache means that false sharing does\nnot lead to unnecessary cache misses. Clients acknowledge\ninvalidations to indicate removal of stale data as explained in\nSection 3.1. Invalidation messages prevent some aborts, and\naccelerate those that must happen - thus wasting less work\nand o\u00ef\u00ac\u201eoading detection of aborts from servers to clients.\nWhen a transaction aborts, its client restores the cached\ncopies of modified objects to the state they had before the\ntransaction started; this is possible because a client makes\na copy of an object the first time it is modified by a \ntransaction.\n4.3 Redirection\nThe redirector runs on the same local network as the peer\ngroup, in one of the peer nodes, or in a special node within\nthe infrastructure. It maintains a directory of pages \navailable in the peer group and provides fast centralized fetch\nredirection (see figure 2) between the peer caches. To \nimprove performance, clients inform the redirector when they\nevict pages or objects by piggybacking that information on\nmessages sent to the redirector.\nTo ensure up-to-date objects are fetched from the group\ncache the redirector tracks the status of the pages. A cached\npage is either complete in which case it contains consistent\nvalues for all the objects, or incomplete, in which case some\nof the objects on a page are marked invalid. Only complete\npages are used by the peer fetch. The protocol for \nmaintaining page status when pages are updated and invalidated is\ndescribed in Section 4.4.\nWhen a client request has to be processed at the servers,\ne.g., a complete requested page is unavailable in the peer\ngroup or a peer needs to commit a transaction, the redirector\nacts as a server proxy: it forwards the request to the server,\nand then forwards the reply back to the client. In addition,\nin response to invalidations sent by a server, the redirector\ndistributes the update or invalidation information to clients\ncaching the modified page and, after all clients acknowledge,\npropagates the group acknowledgment back to the server\n(see figure 3). The redirector-server protocol is, in effect, the\nclient-server protocol used in the base Thor storage system,\nwhere the combined peer group cache is playing the role of\na single client cache in the base system.\n4.4 Peer Update\nThe peer update is implemented as follows. An update\ncommit request from a client arriving at the redirector \ncontains the object updates. Redirector retains the updates\nand propagates the request to the coordinator server. After\na transaction commits, using a two phase commit if needed,\nthe coordinator server sends a commit reply to the redirector\nof the committing client group. The redirector forwards the\nreply to the committing client. It waits for the invalidations\nto arrive to propagate corresponding retained (committed)\nupdates to the clients caching the modified pages (see \nfigure 3.)\nParticipating servers that are home to objects modified by\nthe transaction generate object invalidations for each cache\ngroup that caches pages containing the modified objects \n(including the committing group). The invalidations are sent\nlazily to the redirectors to ensure that all the clients in the\ngroups caching the modified objects get rid of the stale data.\nIn cache groups other than the committing group, \nredirectors propagates the invalidations to all the clients caching\nthe modified pages, collect the client acknowledgments and\nafter completing the collection, propagate collective \nacknowledgments back to the server.\nWithin the committing client group, the arriving \ninvalidations are not propagated. Instead, updates are sent to clients\ncaching those objects\" pages, the updates are acknowledged\nby the client, and the collective acknowledgment is \npropagated to the server.\nAn invalidation renders a cached page unavailable for peer\nfetch changing the status of a complete page p into an \nincomplete. In contrast, an update of a complete page \npreserves the complete page status. As shown by studies of the\n31\nfragment reconstruction [2], such update propagation allows\nto avoid the performance penalties of false sharing. That is,\nwhen clients within a group modify different objects on the\nsame page, the page retains its complete status and remains\navailable for peer fetch. Therefore, the effect of peer update\nis similar to eager fragment reconstruction [2].\nWe have also considered the possibility of allowing a peer\nto fetch an incomplete page (with invalid objects marked\naccordingly) but decided against this possibility because of\nthe extra complexity involved in tracking invalid objects.\n4.5 Vcache\nThe solo commit validation protocol allows clients with\nup-to-date objects to commit independently of slower (or\nfailed) group members. As explained in Section 3.3, the solo\ncommit protocol allows a transaction T to pass validation if\nextra coherence information supplied by the client indicates\nthat transaction T has read up-to-date objects. Clients use\npage version numbers to provide this extra coherence \ninformation. That is, a client includes the page version number\ncorresponding to each object in the read object set sent in\nthe commit request to the server. Since a unique page \nversion number corresponds to each committed object update,\nthe page version number associated with an object allows\nthe validation procedure at the server to check if the client\ntransaction has read up-to-date objects.\nThe use of coarse-grain page versions to identify object\nversions avoids the high penalty of maintaining persistent\nobject versions for small objects, but requires an extra \nprotocol at the client to maintain the mapping from a cached \nobject to the identifying page version (ObjectToVersion). The\nmain implementation issue is concerned with maintaining\nthis mapping efficiently.\nAt the server side, when modifications commit, servers\nassociate page version numbers with the invalidations. At\nvalidation time, if an unacknowledged invalidation is \npending for an object x read by a transaction T, the validation\nprocedure checks if the version number for x in T\"s read set\nmatches the version number for highest pending invalidation\nfor x, in which case the object value is current, otherwise T\nfails validation.\nWe note again that the page version number-based checks,\nand the invalidation acknowledgment-based checks are \ncomplimentary in the solo commit validation and both are needed.\nThe page version number check allows the validation to \nproceed before invalidation acknowledgments arrive but by itself\na page version number check detects page-level conflicts and\nis not sufficient to support fine-grain coherence without the\nobject-level invalidations.\nWe now describe how the client manages the mapping \nObjectToVersion. The client maintains a page version number\nfor each cached page. The version number satisfies the \nfollowing invariant V P about the state of objects on a page:\nif a cached page P has a version number v, then the value\nof an object o on a cached page P is either invalid or it \nreflects at least the modifications committed by transactions\npreceding the transaction that set P\"s version number to v.\nNew object values and new page version numbers arrive\nwhen a client fetches a page or when a commit reply or \ninvalidations arrive for this page. The new object values modify\nthe page and, therefore, the page version number needs to\nbe updated to maintain the invariant V P. A page version\nnumber that arrives when a client fetches a page, replaces\nObject Version\nx 8\nRedirector Server 1Client 1\ncom(P(x,6),Q(y,9))\ncom(P(x,6),Q(y,9))\nok(P(x,8),Q(y,10))\nok(P(x,8),Q(y,10))\ninv(Q(s,11))\ninv(Q(s,11))\ninv(P(r,7)\ninv(P(r,7)\nServer 2\nFigure 5: Reordered Invalidations\nthe page version number for this page. Such an update\npreserves the invariant V P. Similarly, an in-sequence page\nversion number arriving at the client in a commit or \ninvalidation message advances the version number for the entire\ncached page, without violating V P. However, invalidations\nor updates and their corresponding page version numbers\ncan also arrive at the client out of sequence, in which case\nupdating the page version number could violate V P. For\nexample, a commit reply for a transaction that updates \nobject x on page P in server S1, and object y on page Q in\nserver S2, may deliver a new version number for P from the\ntransaction coordinator S1 before an invalidation generated\nfor an earlier transaction that has modified object r on page\nP arrives from S1 (as shown in figure 5).\nThe cache update protocol ensures that the value of any\nobject o in a cached page P reflects the update or \ninvalidation with the highest observed version number. That is,\nobsolete updates or invalidations received out of sequence\ndo not affect the value of an object.\nTo maintain the ObjectToVersion mapping and the \ninvariant V P in the presence of out-of-sequence arrival of page\nversion numbers, the client manages a small version number\ncache vcache that maintains the mapping from an object into\nits corresponding page version number for all reordered \nversion number updates until a complete page version number\nsequence is assembled. When the missing version numbers\nfor the page arrive and complete a sequence, the version\nnumber for the entire page is advanced.\nThe ObjectToVersion mapping, including the vcache and\npage version numbers, is used at transaction commit time to\nprovide version numbers for the read object set as follows.\nIf the read object has an entry in the vcache, its version\nnumber is equal to the highest version number in the vcache\nfor this object. If the object is not present in the vcache, its\nversion number is equal the version number of its containing\ncached page. Figure 6 shows the ObjectToVersion mapping\nin the client cache, including the page version numbers for\npages and the vcache.\nClient can limit vcache size as needed since re-fetching a\npage removes all reordered page version numbers from the\nvcache. However, we expect version number reordering to\nbe uncommon and therefore expect the vcache to be very\nsmall.\n5. BUDDYCACHE FAILOVER\nA client group contains multiple client nodes and a \nredi32\nVersionPageObject Version\nVCache\nClient Cache\nClient\nPage Cache\nFigure 6: ObjectToVersion map with vcache\nrector that can fail independently. The goal of the failover\nprotocol is to reconfigure the BuddyCache in the case of a\nnode failure, so that the failure of one node does not disrupt\nother clients from accessing shared objects. Moreover, the\nfailure of the redirector should allow unaffected clients to\nkeep their caches intact.\nWe have designed a failover protocols for BuddyCache\nbut have not implemented it yet. The appendix outlines the\nprotocol.\n6. PERFORMANCE EVALUATION\nBuddyCache redirection supports the performance \nbenefits of avoiding communication with the servers but \nintroduces extra processing cost due to availability mechanisms\nand request forwarding. Is the cure worse then the \ndisease? To answer the question, we have implemented a \nBuddyCache prototype for the OCC protocol and conducted \nexperiments to analyze the performance benefits and costs over\na range of network latencies.\n6.1 Analysis\nThe performance benefits of peer fetch and peer update\nare due to avoided server interactions. This section presents\na simple analytical performance model for this benefit. The\navoided server interactions correspond to different types of\nclient cache misses. These can be cold misses, invalidation\nmisses and capacity misses. Our analysis focuses on cold\nmisses and invalidation misses, since the benefit of avoiding\ncapacity misses can be derived from the cold misses. \nMoreover, technology trends indicate that memory and storage\ncapacity will continue to grow and therefore a typical \nBuddyCache configuration is likely not to be cache limited.\nThe client cache misses are determined by several \nvariables, including the workload and the cache configuration.\nOur analysis tries, as much as possible, to separate these\nvariables so they can be controlled in the validation \nexperiments.\nTo study the benefit of avoiding cold misses, we consider\ncold cache performance in a read-only workload (no \ninvalidation misses). We expect peer fetch to improve the latency\ncost for client cold cache misses by fetching objects from\nnearby cache. We evaluate how the redirection cost affects\nthis benefit by comparing and analyzing the performance\nof an application running in a storage system with \nBuddyCache and without (called Base).\nTo study the benefit of avoiding invalidation misses, we\nconsider hot cache performance in a workload with \nmodifications (with no cold misses). In hot caches we expect\nBuddyCache to provide two complementary benefits, both\nof which reduce the latency of access to shared modified \nobjects. Peer update lets a client access an object modified by\na nearby collaborating peer without the delay imposed by\ninvalidation-only protocols. In groups where peers share a\nread-only interest in the modified objects, peer fetch allows\na client to access a modified object as soon as a collaborating\npeer has it, which avoids the delay of server fetch without\nthe high cost imposed by the update-only protocols.\nTechnology trends indicate that both benefits will remain\nimportant in the foreseeable future. The trend toward \nincrease in available network bandwidth decreases the cost\nof the update-only protocols. However, the trend toward\nincreasingly large caches, that are updated when cached \nobjects are modified, makes invalidation-base protocols more\nattractive.\nTo evaluate these two benefits we consider the \nperformance of an application running without BuddyCache with\nan application running BuddyCache in two configurations.\nOne, where a peer in the group modifies the objects, and\nanother where the objects are modified by a peer outside\nthe group.\nPeer update can also avoid invalidation misses due to\nfalse-sharing, introduced when multiple peers update \ndifferent objects on the same page concurrently. We do not \nanalyze this benefit (demonstrated by earlier work [2]) because\nour benchmarks do not allow us to control object layout,\nand also because this benefit can be derived given the cache\nhit rate and workload contention.\n6.1.1 The Model\nThe model considers how the time to complete an \nexecution with and without BuddyCache is affected by \ninvalidation misses and cold misses.\nConsider k clients running concurrently accessing uniformly\na shared set of N pages in BuddyCache (BC) and Base. Let\ntfetch(S), tredirect(S), tcommit(S), and tcompute(S) be the\ntime it takes a client to, respectively, fetch from server, peer\nfetch, commit a transaction and compute in a transaction,\nin a system S, where S is either a system with BuddyCache\n(BC) or without (Base). For simplicity, our model assumes\nthe fetch and commit times are constant. In general they\nmay vary with the server load, e.g. they depend on the total\nnumber of clients in the system.\nThe number of misses avoided by peer fetch depends on k,\nthe number of clients in the BuddyCache, and on the client\nco-interest in the shared data. In a specific BuddyCache \nexecution it is modeled by the variable r, defined as a number\nof fetches arriving at the redirector for a given version of\npage P (i.e. until an object on the page is invalidated).\nConsider an execution with cold misses. A client starts\nwith a cold cache and runs read-only workload until it \naccesses all N pages while committing l transactions. We \nassume there are no capacity misses, i.e. the client cache is\nlarge enough to hold N pages. In BC, r cold misses for\npage P reach the redirector. The first of the misses fetches\nP from the server, and the subsequent r \u00e2\u02c6\u2019 1 misses are \nredirected. Since each client accesses the entire shared set r = k.\nLet Tcold(Base) and Tcold(BC) be the time it takes to\ncomplete the l transactions in Base and BC.\n33\nTcold(Base) = N \u00e2\u02c6\u2014 tfetch(Base)\n+(tcompute + tcommit(Base)) \u00e2\u02c6\u2014 l (1)\nTcold(BC) =\nN \u00e2\u02c6\u2014\n1\nk\n\u00e2\u02c6\u2014 tfetch(BC) + (1 \u00e2\u02c6\u2019\n1\nk\n) \u00e2\u02c6\u2014 tredirect\n+(tcompute + tcommit(BC)) \u00e2\u02c6\u2014 l (2)\nConsider next an execution with invalidation misses. A\nclient starts with a hot cache containing the working set of N\npages. We focus on a simple case where one client (writer)\nruns a workload with modifications, and the other clients\n(readers) run a read-only workload.\nIn a group containing the writer (BCW ), peer update\neliminates all invalidation misses. In a group containing\nonly readers (BCR), during a steady state execution with\nuniform updates, a client transaction has missinv \ninvalidation misses. Consider the sequence of r client misses on\npage P that arrive at the redirector in BCR between two\nconsequent invalidations of page P. The first miss goes to\nthe server, and the r \u00e2\u02c6\u2019 1 subsequent misses are redirected.\nUnlike with cold misses, r \u00e2\u2030\u00a4 k because the second \ninvalidation disables redirection for P until the next miss on P\ncauses a server fetch.\nAssuming uniform access, a client invalidation miss has a\nchance of 1/r to be the first miss (resulting in server fetch),\nand a chance of (1 \u00e2\u02c6\u2019 1/r) to be redirected.\nLet Tinval(Base), Tinval(BCR) and Tinval(BCW ) be the\ntime it takes to complete a single transaction in the Base,\nBCR and BCW systems.\nTinval(Base) = missinv \u00e2\u02c6\u2014 tfetch(Base)\n+tcompute + tcommit(Base) (3)\nTinval(BCR) = missinv \u00e2\u02c6\u2014 (\n1\nr\n\u00e2\u02c6\u2014 tfetch(BCR)\n+(1 \u00e2\u02c6\u2019\n1\nr\n) \u00e2\u02c6\u2014 tredirect(BCR))\n+tcompute + tcommit(BCR) (4)\nTinval(BCW ) = tcompute + tcommit(BCW ) (5)\nIn the experiments described below, we measure the \nparameters N, r, missinv, tfetch(S), tredirect(S), tcommit(S),\nand tcompute(S). We compute the completion times derived\nusing the above model and derive the benefits. We then\nvalidate the model by comparing the derived values to the\ncompletion times and benefits measured directly in the \nexperiments.\n6.2 Experimental Setup\nBefore presenting our results we describe our experimental\nsetup. We use two systems in our experiments. The Base\nsystem runs Thor distributed object storage system [23] with\nclients connecting directly to the servers. The Buddy system\nruns our implementation of BuddyCache prototype in Thor,\nsupporting peer fetch, peer update, and solo commit, but\nnot the failover.\nOur workloads are based on the multi-user OO7 \nbenchmark [8]; this benchmark is intended to capture the \ncharacteristics of many different multi-user CAD/CAM/CASE \napplications, but does not model any specific application. We\nuse OO7 because it is a standard benchmark for measuring\nobject storage system performance. The OO7 database \ncontains a tree of assembly objects with leaves pointing to three\ncomposite parts chosen randomly from among 500 such \nobjects. Each composite part contains a graph of atomic parts\nlinked by connection objects; each atomic part has 3 \noutgoing connections. We use a medium database that has 200\natomic parts per composite part. The multi-user database\nallocates for each client a private module consisting of one\ntree of assembly objects, and adds an extra shared module\nthat scales proportionally to the number of clients.\nWe expect a typical BuddyCache configuration not to be\ncache limited and therefore focus on workloads where the\nobjects in the client working set fit in the cache. Since the\ngoal of our study is to evaluate how effectively our \ntechniques deal with access to shared objects, in our study we\nlimit client access to shared data only. This allows us to\nstudy the effect our techniques have on cold cache and cache\nconsistency misses and isolate as much as possible the effect\nof cache capacity misses.\nTo keep the length of our experiments reasonable, we use\nsmall caches. The OO7 benchmark generates database \nmodules of predefined size. In our implementation of OO7, the\nprivate module size is about 38MB. To make sure that the\nentire working set fits into the cache we use a single private\nmodule and choose a cache size of 40MB for each client. The\nOO7 database is generated with modules for 3 clients, only\none of which is used in our experiments as we explain above.\nThe objects in the database are clustered in 8K pages, which\nare also the unit of transfer in the fetch requests.\nWe consider two types of transaction workloads in our\nanalysis, read-only and read-write. In OO7 benchmark,\nread-only transactions use the T1 traversal that performs\na depth-first traversal of entire composite part graph. Write\ntransactions use the T2b traversal that is identical to T1\nexcept that it modifies all the atomic parts in a single \ncomposite. A single transaction includes one traversal and there\nis no sleep time between transactions. Both read-only and\nread-write transactions always work with data from the same\nmodule. Clients running read-write transactions don\"t \nmodify in every transaction, instead they have a 50% probability\nof running read-only transactions.\nThe database was stored by a server on a 40GB IBM\n7200RPM hard drive, with a 8.5 average seek time and 40\nMB/sec data transfer rates. In Base system clients \nconnect directly to the database. In Buddy system clients \nconnect to the redirector that connects to the database. We\nrun the experiments with 1-10 clients in Base, and one or\ntwo 1-10 client groups in Buddy. The server, the clients\nand the redirectors ran on a 850MHz Intel Pentium III \nprocessor based PC, 512MB of memory, and Linux Red Hat\n6.2. They were connected by a 100Mb/s Ethernet. The\nserver was configured with a 50MB cache (of which 6MB\nwere used for the modified object buffer), the client had a\n40MB cache. The experiments ran in Utah experimental\ntestbed emulab.net [1].\n34\nLatency [ms]\nBase Buddy\n3 group 5 group 3 group 5 group\nFetch 1.3 1.4 2.4 2.6\nCommit 2.5 5.5 2.4 5.7\nTable 1: Commit and Server fetch\nOperation Latency [ms]\nPeerFetch 1.8 - 5.5\n\u00e2\u02c6\u2019AlertHelper 0.3 - 4.6\n\u00e2\u02c6\u2019CopyUnswizzle 0.24\n\u00e2\u02c6\u2019CrossRedirector 0.16\nTable 2: Peer fetch\n6.3 Basic Costs\nThis section analyzes the basic cost of the requests in the\nBuddy system during the OO7 runs.\n6.3.1 Redirection\nFetch and commit requests in the BuddyCache cross the\nredirector, a cost not incurred in the Base system. For a\nrequest redirected to the server (server fetch) the extra cost\nof redirection includes a local request from the client to \nredirector on the way to and from the server. We evaluate this\nlatency overhead indirectly by comparing the measured \nlatency of the Buddy system server fetch or commit request\nwith the measured latency of the corresponding request in\nthe Base system.\nTable 1 shows the latency for the commit and server fetch\nrequests in the Base and Buddy system for 3 client and 5\nclient groups in a fast local area network. All the numbers\nwere computed by averaging measured request latency over\n1000 requests. The measurements show that the redirection\ncost of crossing the redirector in not very high even in a\nlocal area network. The commit cost increases with the\nnumber of clients since commits are processed sequentially.\nThe fetch cost does not increase as much because the server\ncache reduces this cost. In a large system with many groups,\nhowever, the server cache becomes less efficient.\nTo evaluate the overheads of the peer fetch, we measure\nthe peer fetch latency (PeerFetch) at the requesting client\nand break down its component costs. In peer fetch, the cost\nof the redirection includes, in addition to the local network\nrequest cost, the CPU processing latency of crossing the\nredirector and crossing the helper, the latter including the\ntime to process the help request and the time to copy, and\nunswizzle the requested page.\nWe directly measured the time to copy and unswizzle the\nrequested page at the helper, (CopyUnswizzle), and timed\nthe crossing times using a null crossing request. Table 2\nsummarizes the latencies that allows us to break down the\npeer fetch costs. CrossRedirector, includes the CPU latency\nof crossing the redirector plus a local network round-trip\nand is measured by timing a round-trip null request issued\nby a client to the redirector. AlertHelper, includes the time\nfor the helper to notice the request plus a network \nroundtrip, and is measured by timing a round-trip null request\nissued from an auxiliary client to the helper client. The\nlocal network latency is fixed and less than 0.1 ms.\nThe AlertHelper latency which includes the elapsed time\nfrom the help request arrival until the start of help request\nprocessing is highly variable and therefore contributes to\nthe high variability of the PeerFetch time. This is because\nthe client in Buddy system is currently single threaded and\ntherefore only starts processing a help request when blocked\nwaiting for a fetch- or commit reply. This overhead is not\ninherent to the BuddyCache architecture and could be \nmitigated by a multi-threaded implementation in a system with\npre-emptive scheduling.\n6.3.2 Version Cache\nThe solo commit allows a fast client modifying an object\nto commit independently of a slow peer. The solo \ncommit mechanism introduces extra processing at the server\nat transaction validation time, and extra processing at the\nclient at transaction commit time and at update or \ninvalidation processing time.\nThe server side overheads are minimal and consist of a\npage version number update at commit time, and a version\nnumber comparison at transaction validation time.\nThe version cache has an entry only when invalidations or\nupdates arrive out of order. This may happen when a \ntransaction accesses objects in multiple servers. Our experiments\nrun in a single server system and therefore, the commit time\noverhead of version cache management at the client does not\ncontribute in the results presented in the section below. To\ngauge these client side overheads in a multiple server \nsystem, we instrumented the version cache implementation to\nrun with a workload trace that included reordered \ninvalidations and timed the basic operations.\nThe extra client commit time processing includes a version\ncache lookup operation for each object read by the \ntransaction at commit request preparation time, and a version\ncache insert operation for each object updated by a \ntransaction at commit reply processing time, but only if the \nupdated page is missing some earlier invalidations or updates.\nIt is important that the extra commit time costs are kept\nto a minimum since client is synchronously waiting for the\ncommit completion. The measurements show that in the\nworst case, when a large number of invalidations arrive out\nof order, and about half of the objects modified by T2a (200\nobjects) reside on reordered pages, the cost of updating the\nversion cache is 0.6 ms. The invalidation time cost are \ncomparable, but since invalidations and updates are processed\nin the background this cost is less important for the overall\nperformance. We are currently working on optimizing the\nversion cache implementation to further reduce these costs.\n6.4 Overall Performance\nThis section examines the performance gains seen by an\napplication running OO7 benchmark with a BuddyCache in\na wide area network.\n6.4.1 Cold Misses\nTo evaluate the performance gains from avoiding cold\nmisses we compare the cold cache performance of OO7 \nbenchmark running read-only workload in the Buddy and Base\nsystems. We derive the times by timing the execution of the\nsystems in the local area network environment and \nsubstituting 40 ms and 80 ms delays for the requests crossing the\nredirector and the server to estimate the performance in the\nwide-area-network. Figures 7 and 8 show the overall time to\ncomplete 1000 cold cache transactions. The numbers were\n35\n0\n5 0\n100\n150\n200\n250\nBase Buddy Base Buddy Base Buddy\n3 Clients 5 Clients 10 Clients\n[ms]\nCPU Commit Server Fetch Peer Fetch\nFigure 7: Breakdown for cold read-only 40ms RTT\n0\n5 0\n100\n150\n200\n250\n300\n350\n400\nBase Buddy Base Buddy Base Buddy\n3 Clients 5 Clients 10 Clients\n[ms]\nCPU Commit Server Fetch Peer Fetch\nFigure 8: Breakdown for cold read-only 80ms RTT\nobtained by averaging the overall time of each client in the\ngroup.\nThe results show that in a 40 ms network Buddy \nsystem reduces significantly the overall time compared to the\nBase system, providing a 39% improvement in a three client\ngroup, 46% improvement in the five client group and 56%\nimprovement in the ten client case.\nThe overall time includes time spent performing client\ncomputation, direct fetch requests, peer fetches, and commit\nrequests.\nIn the three client group, Buddy and Base incur almost\nthe same commit cost and therefore the entire performance\nbenefit of Buddy is due to peer fetch avoiding direct fetches.\nIn the five and ten client group the server fetch cost for\nindividual client decreases because with more clients faulting\nin a fixed size shared module into BuddyCache, each client\nneeds to perform less server fetches.\nFigure 8 shows the overall time and cost break down in\nthe 80 ms network. The BuddyCache provides similar \nperformance improvements as with the 40ms network. Higher\nnetwork latency increases the relative performance \nadvantage provided by peer fetch relative to direct fetch but this\nbenefit is offset by the increased commit times.\nFigure 9 shows the relative latency improvement provided\nby BuddyCache (computed as the overall measured time\ndifference between Buddy and Base relative to Base) as a\n-10%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n1 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100\nLatency [ms]\n3 Clients 3 Clients (Perf model) 5 Clients\n5 Clients (Perf model) 10 Clients 10 FEs (perf model)\nFigure 9: Cold miss benefit\n0\n2 0\n4 0\n6 0\n8 0\n100\n120\n140\nBase Buddy Reader Buddy Writer\n[ms]\nCPU Commit Server Fetch Peer Fetch\nFigure 10: Breakdown for hot read-write 40ms RTT\nfunction of network latency, with a fixed server load. The\ncost of the extra mechanism dominates BuddyCache benefit\nwhen network latency is low. At typical Internet latencies\n20ms-60ms the benefit increases with latency and levels off\naround 60ms with significant (up to 62% for ten clients)\nimprovement.\nFigure 9 includes both the measured improvement and the\nimprovement derived using the analytical model.Remarkably,\nthe analytical results predict the measured improvement\nvery closely, albeit being somewhat higher than the \nempirical values. The main reason why the simplified model works\nwell is it captures the dominant performance component,\nnetwork latency cost.\n6.4.2 Invalidation Misses\nTo evaluate the performance benefits provided by \nBuddyCache due to avoided invalidation misses, we compared the\nhot cache performance of the Base system with two \ndifferent Buddy system configurations. One of the Buddy system\nconfigurations represents a collaborating peer group \nmodifying shared objects (Writer group), the other represents a\ngroup where the peers share a read-only interest in the \nmodified objects (Reader group) and the writer resides outside\nthe BuddyCache group.\nIn each of the three systems, a single client runs a \nreadwrite workload (writer) and three other clients run read-only\nworkload (readers). Buddy system with one group \ncontain36\n0\n5 0\n100\n150\n200\n250\n300\nBase Buddy Reader Buddy Writer\n[ms]\nCPU Commit Server Fetch Peer Fetch\nFigure 11: Breakdown for hot read-write 80ms RTT\ning a single reader and another group containing two readers\nand one writer models the Writer group. Buddy system with\none group containing a single writer and another group \nrunning three readers models the Reader group. In Base, one\nwriter and three readers access the server directly. This\nsimple configuration is sufficient to show the impact of \nBuddyCache techniques.\nFigures 10 and 11 show the overall time to complete 1000\nhot cache OO7 read-only transactions. We obtain the \nnumbers by running 2000 transactions to filter out cold misses\nand then time the next 1000 transactions. Here again, the\nreported numbers are derived from the local area network\nexperiment results.\nThe results show that the BuddyCache reduces \nsignificantly the completion time compared to the Base system.\nIn a 40 ms network, the overall time in the Writer group\nimproves by 62% compared to Base. This benefit is due\nto peer update that avoids all misses due to updates. The\noverall time in the Reader group improves by 30% and is\ndue to peer fetch that allows a client to access an \ninvalidated object at the cost of a local fetch avoiding the delay\nof fetching from the server. The latter is an important \nbenefit because it shows that on workloads with updates, peer\nfetch allows an invalidation-based protocol to provide some\nof the benefits of update-based protocol.\nNote that the performance benefit delivered by the peer\nfetch in the Reader group is approximately 50% less than the\nperformance benefit delivered by peer update in the Writer\ngroup. This difference is similar in 80ms network.\nFigure 12 shows the relative latency improvement \nprovided by BuddyCache in Buddy Reader and Buddy Writer\nconfigurations (computed as the overall time difference \nbetween BuddyReader and Base relative to Base, and Buddy\nWriter and Base relative to Base) in a hot cache experiment\nas a function of increasing network latency, for fixed server\nload.\nThe peer update benefit dominates overhead in Writer\nconfiguration even in low-latency network (peer update \nincurs minimal overhead) and offers significant 44-64% \nimprovement for entire latency range.\nThe figure includes both the measured improvement and\nthe improvement derived using the analytical model. As\nin cold cache experiments, here the analytical results \npredict the measured improvement closely. The difference is\n-10%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n1 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100\nLatency [ms]\nBenefits[%]\nBuddy Reader Buddy Reader (perf model)\nBuddy Writer Buddy Writer (perf model)\nFigure 12: Invalidation miss benefit\nminimal in the \"writer group\", and somewhat higher in the\n\"reader group\" (consistent with the results in the cold cache\nexperiments). As in cold cache case, the reason why the \nsimplified analytical model works well is because it captures the\ncosts of network latency, the dominant performance cost.\n7. CONCLUSION\nCollaborative applications provide a shared work \nenvironment for groups of networked clients collaborating on a \ncommon task. They require strong consistency for shared \npersistent data and efficient access to fine-grained objects. These\nproperties are difficult to provide in wide-area network \nbecause of high network latency.\nThis paper described BuddyCache, a new transactional \ncooperative caching [20, 16, 13, 2, 28] technique that improves\nthe latency of access to shared persistent objects for \ncollaborative strong-consistency applications in high-latency \nnetwork environments. The technique improves performance\nyet provides strong correctness and availability properties\nin the presence of node failures and slow clients.\nBuddyCache uses redirection to fetch missing objects \ndirectly from group members caches, and to support peer \nupdate, a new lightweight application-level multicast \ntechnique that gives group members consistent access to the new\ndata committed within the collaborating group without \nimposing extra overhead outside the group. Redirection, \nhowever, can interfere with object availability. Solo commit, is\na new validation technique that allows a client in a group\nto commit independently of slow or failed peers. It \nprovides fine-grained validation using inexpensive coarse-grain\nversion information.\nWe have designed and implemented BuddyCache \nprototype in Thor distributed transactional object storage \nsystem [23] and evaluated the benefits and costs of the system\nover a range of network latencies. Analytical results, \nsupported by the system measurements using the multi-user 007\nbenchmark indicate, that for typical Internet latencies \nBuddyCache provides significant performance benefits, e.g. for\nlatencies ranging from 40 to 80 milliseconds round trip time,\nclients using the BuddyCache can reduce by up to 50% the\nlatency of access to shared objects compared to the clients\naccessing the repository directly.\nThe main contributions of the paper are:\n1. extending cooperative caching techniques to support\n37\nfine-grain strong-consistency access in high-latency \nenvironments,\n2. an implementation of the system prototype that yields\nstrong performance gains over the base system,\n3. analytical and measurement based performance \nevaluation of the costs and benefits of the new techniques\ncapturing the dominant performance cost, high \nnetwork latency.\n8. ACKNOWLEDGMENTS\nWe are grateful to Jay Lepreau and the staff of Utah \nexperimental testbed emulab.net [1], especially Leigh Stoller,\nfor hosting the experiments and the help with the testbed.\nWe also thank Jeff Chase, Maurice Herlihy, Butler Lampson\nand the OOPSLA reviewers for the useful comments that\nimproved this paper.\n9. REFERENCES\n[1] \"emulab.net\", the Utah Network Emulation Facility.\nhttp://www.emulab.net.\n[2] A. Adya, M. Castro, B. Liskov, U. Maheshwari, and\nL. Shrira. Fragment Reconstruction: Providing Global\nCache Coherence in a Transactional Storage System.\nProceedings of the International Conference on\nDistributed Computing Systems, May 1997.\n[3] A. Adya, R. Gruber, B. Liskov, and U. Maheshwari.\nEfficient optimistic concurrencty control using loosely\nsynchronized clocks. In Proceedings of the ACM\nSIGMOD International Conference on Management of\nData, May 1995.\n[4] C. Amza, A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu,\nR. Rajamony, W. Yu, and W. Zwaenepoel.\nTreadmarks: Shared memory computing on networks\nof workstations. IEEE Computer, 29(2), February\n1996.\n[5] C. Anderson and A. Karlin. Two Adaptive Hybrid\nCache Coherency Protocols. In Proceedings of the 2nd\nIEEE Symposium on High-Performance Computer\nArchitecture (HPCA \"96), February 1996.\n[6] M. Baker. Fast Crash Recovery in Distributed File\nSystems. PhD thesis, University of California at\nBerkeley, 1994.\n[7] P. Cao and C. Liu. Maintaining Strong Cache\nConsistency in the World Wide Web. In 17th\nInternational Conference on Distributed Computing\nSystems., April 1998.\n[8] M. Carey, D. J. Dewitt, C. Kant, and J. F. Naughton.\nA Status Report on the OO7 OODBMS Benchmarking\nEffort. In Proceedings of OOPSLA, October 1994.\n[9] A. Chankhunthod, M. Schwartz, P. Danzig,\nK. Worrell, and C. Neerdaels. A Hierarchical Internet\nObject Cache. In USENIX Annual Technical\nConference, January 1995.\n[10] J. Chase, S. Gadde, and M. Rabinovich. Directory\nStructures for Scalable Internet Caches. Technical\nReport CS-1997-18, Dept. of Computer Science, Duke\nUniversity, November 1997.\n[11] J. Chase, S. Gadde, and M. Rabinovich. Not All Hits\nAre Created Equal: Cooperative Proxy Caching Over\na Wide-Area Network. In Third International WWW\nCaching Workshop, June 1998.\n[12] D. R. Cheriton and D. Li. Scalable Web Caching of\nFrequently Updated Objects using Reliable Multicast.\n2nd USENIX Symposium on Internet Technologies\nand Systems, October 1999.\n[13] M. D. Dahlin, R. Y. Wang, T. E. Anderson, and D. A.\nPatterson. Cooperative caching: Using remote client\nmemory to improve file system performance.\nProceedings of the USENIX Conference on Operating\nSystems Design and Implementation, November 1994.\n[14] S. Dwarkadas, H. Lu, A.L. Cox, R. Rajamony, and\nW. Zwaenepoel. Combining Compile-Time and\nRun-Time Support for Efficient Software Distributed\nShared Memory. In Proceedings of IEEE, Special Issue\non Distributed Shared Memory, March 1999.\n[15] Li Fan, Pei Cao, Jussara Almeida, and Andrei Broder.\nSummary Cache: A Scalable Wide-Area Web Cache\nSharing Protocol. In Proceedings of ACM SIGCOMM,\nSeptember 1998.\n[16] M. Feeley, W. Morgan, F. Pighin, A. Karlin, and\nH. Levy. Implementing Global Memory Management\nin a Workstation Cluster. Proceedings of the 15th\nACM Symposium on Operating Systems Principles,\nDecember 1995.\n[17] M. J. Feeley, J. S. Chase, V. R. Narasayya, and H. M.\nLevy. Integrating Coherency and Recoverablity in\nDistributed Systems. In Proceedings of the First\nUsenix Symposium on Operating sustems Design and\nImplementation, May 1994.\n[18] P. Ferreira and M. Shapiro et al. PerDiS: Design,\nImplementation, and Use of a PERsistent DIstributed\nStore. In Recent Advances in Distributed Systems,\nLNCS 1752, Springer-Verlag, 1999.\n[19] M. J. Franklin, M. Carey, and M. Livny. Transactional\nClient-Server Cache Consistency: Alternatives and\nPerformance. In ACM Transactions on Database\nSystems, volume 22, pages 315-363, September 1997.\n[20] Michael Franklin, Michael Carey, and Miron Livny.\nGlobal Memory Management for Client-Server DBMS\nArchitectures. In Proceedings of the 19th Intl.\nConference on Very Large Data Bases (VLDB),\nAugust 1992.\n[21] S. Ghemawat. The Modified Object Buffer: A Storage\nManagement Technique for Object-Oriented\nDatabases. PhD thesis, Massachusetts Institute of\nTechnology, 1997.\n[22] L. Kawell, S. Beckhardt, T. Halvorsen, R. Ozzie, and\nI. Greif. Replicated document management in a group\ncommunication system. In Proceedings of the ACM\nCSCW Conference, September 1988.\n[23] B. Liskov, M. Castro, L. Shrira, and A. Adya.\nProviding Persistent Objects in Distributed Systems.\nIn Proceedings of the 13th European Conference on\nObject-Oriented Programming (ECOOP \"99), June\n1999.\n[24] A. Muthitacharoen, B. Chen, and D. Mazieres. A\nLow-bandwidth Network File System. In 18th ACM\nSymposium on Operating Systems Principles, October\n2001.\n[25] B. Oki and B. Liskov. Viewstamped Replication: A\nNew Primary Copy Method to Support\nHighly-Available Distributed Systems. In Proc. of\nACM Symposium on Principles of Distributed\n38\nComputing, August 1988.\n[26] J. O\"Toole and L. Shrira. Opportunistic Log: Efficient\nInstallation Reads in a Reliable Object Server. In\nUsenix Symposium on Operation Systems Design and\nImplementation, November 1994.\n[27] D. Pendarakis, S. Shi, and D. Verma. ALMI: An\nApplication Level Multicast Infrastructure. In 3rd\nUSENIX Symposium on Internet Technologies and\nSystems, March 2001.\n[28] P. Sarkar and J. Hartman. Efficient Cooperative\nCaching Using Hints. In Usenix Symposium on\nOperation Systems Design and Implementation,\nOctober 1996.\n[29] A. M. Vahdat, P. C. Eastham, and T. E Anderson.\nWebFS: A Global Cache Coherent File System.\nTechnical report, University of California, Berkeley,\n1996.\n[30] A. Wolman, G. Voelker, N. Sharma, N. Cardwell,\nA. Karlin, and H. Levy. On the Scale and Performance\nof Cooperative Web Proxy Caching. In 17th ACM\nSymposium on Operating Systems Principles,\nDecember 1999.\n[31] J. Yin, L. Alvisi, M. Dahlin, and C. Lin. Hierarchical\nCache Consistency in a WAN. In USENIX Symposium\non Internet Technologies and Systems, October 1999.\n[32] J. Yin, L. Alvisi, M. Dahlin, and C. Lin. Volume\nLeases for Consistency in Large-Scale Systems. IEEE\nTransactions on Knowledge and Data Engineering,\n11(4), July/August 1999.\n[33] M. Zaharioudakis, M. J. Carey, and M. J. Franklin.\nAdaptive, Fine-Grained Sharing in a Client-Server\nOODBMS: A Callback-Based Approach. ACM\nTransactions on Database Systems, 22:570-627,\nDecember 1997.\n10. APPENDIX\nThis appendix outlines the BuddyCache failover protocol.\nTo accommodate heterogeneous clients including \nresourcepoor hand-helds we do not require the availability of \npersistent storage in the BuddyCache peer group. The \nBuddyCache design assumes that the client caches and the \nredirector data structures do not survive node failures.\nA failure of a client or a redirector is detected by a \nmembership protocol that exchanges periodic I am alive \nmessages between group members and initiates a failover \nprotocol. The failover determines the active group participants,\nre-elects a redirector if needed, reinitializes the BuddyCache\ndata structures in the new configuration and restarts the\nprotocol. The group reconfiguration protocol is similar to\nthe one presented in [25]. Here we describe how the failover\nmanages the BuddyCache state.\nTo restart the BuddyCache protocol, the failover needs\nto resynchronize the redirector page directory and \nclientserver request forwarding so that active clients can continue\nrunning transactions using their caches. In the case of a\nclient failure, the failover removes the crashed client pages\nfrom the directory. Any response to an earlier request \ninitiated by the failed client is ignored except a commit reply, in\nwhich case the redirector distributes the retained committed\nupdates to active clients caching the modified pages.\nIn the case of a redirector failure, the failover protocol\nreinitializes sessions with the servers and clients, and \nrebuilds the page directory using a protocol similar to one\nin [6]. The newly restarted redirector asks the active group\nmembers for the list of pages they are caching and the \nstatus of these pages, i.e. whether the pages are complete or\nincomplete.\nRequests outstanding at the redirector at the time of the\ncrash may be lost. A lost fetch request will time out at the\nclient and will be retransmitted. A transaction running at\nthe client during a failover and committing after the failover\nis treated as a regular transaction, a transaction trying to\ncommit during a failover is aborted by the failover \nprotocol. A client will restart the transaction and the commit\nrequest will be retransmitted after the failover. \nInvalidations, updates or collected update acknowledgements lost at\nthe crashed redirector could prevent the garbage collection\nof pending invalidations at the servers or the vcache in the\nclients. Therefore, servers detecting a redirector crash \nretransmit unacknowledged invalidations and commit replies.\nUnique version numbers in invalidations and updates ensure\nthat duplicate retransmitted requests are detected and \ndiscarded.\nSince the transaction validation procedure depends on the\ncache coherence protocol to ensure that transactions do not\nread stale data, we now need to argue that BuddyCache\nfailover protocol does not compromise the correctness of the\nvalidation procedure. Recall that BuddyCache transaction\nvalidation uses two complementary mechanisms, page \nversion numbers and invalidation acknowledgements from the\nclients, to check that a transaction has read up-to-date data.\nThe redirector-based invalidation (and update) \nacknowledgement propagation ensures the following invariant. When\na server receives an acknowledgement for an object o \nmodification (invalidation or update) from a client group, any\nclient in the group caching the object o has either installed\nthe latest value of object o, or has invalidated o. \nTherefore, if a server receives a commit request from a client for a\ntransaction T reading an object o after a failover in the client\ngroup, and the server has no unacknowledged invalidation\nfor o pending for this group, the version of the object read\nby the transaction T is up-to-date independently of client\nor redirector failures.\nNow consider the validation using version numbers. The\ntransaction commit record contains a version number for\neach object read by the transaction. The version number\nprotocol maintains the invariant V P that ensures that the\nvalue of object o read by the transaction corresponds to the\nhighest version number for o received by the client. The \ninvariant holds since the client never applies an earlier \nmodification after a later modification has been received. \nRetransmition of invalidations and updates maintains this invariant.\nThe validation procedure checks that the version number in\nthe commit record matches the version number in the \nunacknowledged outstanding invalidation. It is straightforward\nto see that since this check is an end-to-end client-server\ncheck it is unaffected by client or redirector failure.\nThe failover protocol has not been implemented yet.\n39\n": ["object storage system", "collaborative strong-consistency application", "wide-area network", "cooperative web caching", "fine-grain sharing", "transaction", "fault-tolerance property", "buddycache", "dominant performance cost", "optimistic system", "peer fetch", "multi-user oo7 benchmark", "cooperative cache", "fine-grain share", "fault-tolerance", ""]}