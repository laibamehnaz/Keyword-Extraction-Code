{"Realistic Cognitive Load Modeling for Enhancing Shared\nMental Models in Human-Agent Collaboration\nXiaocong Fan\nCollege of Information Sciences and Technology\nThe Pennsylvania State University\nUniversity Park, PA 16802\nzfan@ist.psu.edu\nJohn Yen\nCollege of Information Sciences and Technology\nThe Pennsylvania State University\nUniversity Park, PA 16802\njyen@ist.psu.edu\nABSTRACT\nHuman team members often develop shared expectations to\npredict each other\"s needs and coordinate their behaviors.\nIn this paper the concept Shared Belief Map is proposed\nas a basis for developing realistic shared expectations among\na team of Human-Agent-Pairs (HAPs). The establishment\nof shared belief maps relies on inter-agent information \nsharing, the effectiveness of which highly depends on agents\" \nprocessing loads and the instantaneous cognitive loads of their\nhuman partners. We investigate HMM-based cognitive load\nmodels to facilitate team members to share the right \ninformation with the right party at the right time. The shared\nbelief map concept and the cognitive/processing load models\nhave been implemented in a cognitive agent \narchitectureSMMall. A series of experiments were conducted to evaluate\nthe concept, the models, and their impacts on the evolving\nof shared mental models of HAP teams.\nCategories and Subject Descriptors\nI.2.11 [Artificial Intelligence]: Distributed Artificial \nIntelligence-Intelligent agents, Multiagent systems\nGeneral Terms\nDesign, Experimentation, Human Factors\n1. INTRODUCTION\nThe entire movement of agent paradigm was spawned,\nat least in part, by the perceived importance of fostering\nhuman-like adjustable autonomy. Human-centered \nmultiagent teamwork has thus attracted increasing attentions in\nmulti-agent systems field [2, 10, 4]. Humans and autonomous\nsystems (agents) are generally thought to be \ncomplementary: while humans are limited by their cognitive capacity in\ninformation processing, they are superior in spatial, \nheuristic, and analogical reasoning; autonomous systems can \ncontinuously learn expertise and tacit problem-solving \nknowledge from humans to improve system performance. In short,\nhumans and agents can team together to achieve better \nperformance, given that they could establish certain mutual\nawareness to coordinate their mixed-initiative activities.\nHowever, the foundation of human-agent collaboration\nkeeps being challenged because of nonrealistic modeling of\nmutual awareness of the state of affairs. In particular, few\nresearchers look beyond to assess the principles of modeling\nshared mental constructs between a human and his/her \nassisting agent. Moreover, human-agent relationships can go\nbeyond partners to teams. Many informational processing\nlimitations of individuals can be alleviated by having a group\nperform tasks. Although groups also can create additional\ncosts centered on communication, resolution of conflict, and\nsocial acceptance, it is suggested that such limitations can\nbe overcome if people have shared cognitive structures for \ninterpreting task and social requirements [8]. Therefore, there\nis a clear demand for investigations to broaden and deepen\nour understanding on the principles of shared mental \nmodeling among members of a mixed human-agent team.\nThere are lines of research on multi-agent teamwork, both\ntheoretically and empirically. For instance, Joint Intention\n[3] and SharedPlans [5] are two theoretical frameworks for\nspecifying agent collaborations. One of the drawbacks is\nthat, although both have a deep philosophical and \ncognitive root, they do not accommodate the modeling of \nhuman team members. Cognitive studies suggested that teams\nwhich have shared mental models are expected to have \ncommon expectations of the task and team, which allow them\nto predict the behavior and resource needs of team \nmembers more accurately [14, 6]. Cannon-Bowers et al. [14]\nexplicitly argue that team members should hold compatible\nmodels that lead to common expectations. We agree on\nthis and believe that the establishment of shared \nexpectations among human and agent team members is a critical\nstep to advance human-centered teamwork research.\nIt has to be noted that the concept of shared expectation\ncan broadly include role assignment and its dynamics, \nteamwork schemas and progresses, communication patterns and\nintentions, etc. While the long-term goal of our research is\nto understand how shared cognitive structures can enhance\nhuman-agent team performance, the specific objective of the\nwork reported here is to develop a computational cognitive\n395\n978-81-904262-7-5 (RPS) c 2007 IFAAMAS\ncapacity model to facilitate the establishment of shared \nexpectations. In particular, we argue that to favor \nhumanagent collaboration, an agent system should be designed to\nallow the estimation and prediction of human teammates\"\n(relative) cognitive loads, and use that to offer improvised,\nunintrusive help. Ideally, being able to predict the \ncognitive/processing capacity curves of teammates could allow\na team member to help the right party at the right time,\navoiding unbalanced work/cognitive loads among the team.\nThe last point is on the modeling itself. Although an\nagent\"s cognitive model of its human peer is not necessarily\nto be descriptively accurate, having at least a realistic model\ncan be beneficial in offering unintrusive help, bias \nreduction, as well as trustable and self-adjustable autonomy. For\nexample, although humans\" use of cognitive simplification\nmechanisms (e.g., heuristics) does not always lead to errors\nin judgment, it can lead to predictable biases in responses\n[8]. It is feasible to develop agents as cognitive aids to \nalleviate humans\" biases, as long as an agent can be trained\nto obtain a model of a human\"s cognitive inclination. With\na realistic human cognitive model, an agent can also better\nadjust its automation level. When its human peer is \nbecoming overloaded, an agent can take over resource-consuming\ntasks, shifting the human\"s limited cognitive resources to\ntasks where a human\"s role is indispensable. When its \nhuman peer is underloaded, an agent can take the chance to\nobserve the human\"s operations to refine its cognitive model\nof the human. Many studies have documented that human\nchoices and behaviors do not agree with predictions from\nrational models. If agents could make recommendations in\nways that humans appreciate, it would be easier to establish\ntrust relationships between agents and humans; this in turn,\nwill encourage humans\" automation uses.\nThe rest of the paper is organized as follows. In Section\n2 we review cognitive load theories and measurements. A\nHMM-based cognitive load model is given in Section 3 to\nsupport resource-bounded teamwork among \nhuman-agentpairs. Section 4 describes the key concept shared belief\nmap as implemented in SMMall, and Section 5 reports the\nexperiments for evaluating the cognitive models and their\nimpacts on the evolving of shared mental models.\n2. COGNITIVE CAPACITY-OVERVIEW\nPeople are information processors. Most cognitive \nscientists [8] believe that human information-processing system\nconsists of an executive component and three main \ninformation stores: (a) sensory store, which receives and retains\ninformation for one second or so; (b) working (or \nshortterm) memory, which refers to the limited capacity to hold\n(approximately seven elements at any one time [9]), retain\n(for several seconds), and manipulate (two or three \ninformation elements simultaneously) information; and (c) \nlongterm memory, which has virtually unlimited capacity [1] and\ncontains a huge amount of accumulated knowledge organized\nas schemata. Cognitive load studies are, by and large, \nconcerned about working memory capacity and how to \ncircumvent its limitations in human problem-solving activities such\nas learning and decision making.\nAccording to the cognitive load theory [11], cognitive load\nis defined as a multidimensional construct representing the\nload that a particular task imposes on the performer. It\nhas a causal dimension including causal factors that can be\ncharacteristics of the subject (e.g. expertise level), the task\n(e.g. task complexity, time pressure), the environment (e.g.\nnoise), and their mutual relations. It also has an \nassessment dimension reflecting the measurable concepts of \nmental load (imposed exclusively by the task and environmental\ndemands), mental effort (the cognitive capacity actually \nallocated to the task), and performance.\nLang\"s information-processing model [7] consists of three\nmajor processes: encoding, storage, and retrieval. The \nencoding process selectively maps messages in sensory stores\nthat are relevant to a person\"s goals into working memory;\nthe storage process consolidates the newly encoded \ninformation into chunks, and form associations and schema to \nfacilitate subsequent recalls; the retrieval process searches the\nassociated memory network for a specific element/schema\nand reactivates it into working memory. The model \nsuggests that processing resources (cognitive capacity) are \nindependently allocated to the three processes. In addition,\nworking memory is used both for holding and for \nprocessing information [1]. Due to limited capacity, when greater\neffort is required to process information, less capacity \nremains for the storage of information. Hence, the allocation\nof the limited cognitive resources has to be balanced in \norder to enhance human performance. This comes to the issue\nof measuring cognitive load, which has proven difficult for\ncognitive scientists.\nCognitive load can be assessed by measuring mental load,\nmental effort, and performance using rating scales, \npsychophysiological (e.g. measures of heart activity, brain \nactivity, eye activity), and secondary task techniques [12]. \nSelfratings may appear questionable and restricted, especially\nwhen instantaneous load needs to be measured over time.\nAlthough physiological measures are sometimes highly \nsensitive for tracking fluctuating levels of cognitive load, costs\nand work place conditions often favor task- and \nperformancebased techniques, which involve the measure of a secondary\ntask as well as the primary task under consideration. \nSecondary task techniques are based on the assumption that\nperformance on a secondary task reflects the level of \ncognitive load imposed by a primary task [15]. From the resource\nallocation perspective, assuming a fixed cognitive capacity,\nany increase in cognitive resources required by the primary\ntask must inevitably decrease resources available for the \nsecondary task [7]. Consequently, performance in a secondary\ntask deteriorates as the difficulty or priority of the primary\ntask increases. The level of cognitive load can thus be \nmanifested by the secondary task performance: the subject is\ngetting overloaded if the secondary task performance drops.\nA secondary task can be as simple as detecting a visual or\nauditory signal but requires sustained attention. Its \nperformance can be measured in terms of reaction time, accuracy,\nand error rate. However, one important drawback of \nsecondary task performance, as noted by Paas [12], is that it\ncan interfere considerably with the primary task \n(competing for limited capacity), especially when the primary task is\ncomplex. To better understand and measure cognitive load,\nXie and Salvendy [16] introduced a conceptual framework,\nwhich distinguishes instantaneous load, peak load, \naccumulated load, average load, and overall load. It seems that\nthe notation of instantaneous load, which represents the \ndynamics of cognitive load over time, is especially useful for\nmonitoring the fluctuation trend so that free capacity can\nbe exploited at the most appropriate time to enhance the\noverall performance in human-agent collaborations.\n396 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\nAgent n\nHuman\nHuman-Agent Pair n\nAgent 1\nHuman\nHuman-Agent Pair 1\nTeammates\nAgent\nProcessing\nModel\nAgent\nComm\nModel\nHuman\nPartner\nHAI\nAgent\nProcessing\nModel\nAgent\nComm\nModel\nHuman\nPartner\nHAI\nTeammates\nFigure 1: Human-centered teamwork model.\n3. HUMAN-CENTERED TEAMWORK MODEL\nPeople are limited information processors, and so are \nintelligent agent systems; this is especially true when they act\nunder hard or soft timing constraints imposed by the domain\nproblems. In respect to our goal to build realistic \nexpectations among teammates, we take two important steps.\nFirst, agents are resource-bounded; their processing \ncapacity is limited by computing resources, inference \nknowledge, concurrent tasking capability, etc. We withdraw the\nassumption that an agent knows all the information/intentions\ncommunicated from other teammates. Instead, we contend\nthat due to limited processing capacity, an agent may only\nhave opportunities to process (make sense of) a portion of\nthe incoming information, with the rest ignored. Taking\nthis approach will largely change the way in which an agent\nviews (models) the involvement and cooperativeness of its\nteammates in a team activity. In other words, the \nestablishment of shared mental models regarding team members\"\nbeliefs, intentions, and responsibilities can no longer rely on\ninter-agent communication only. This being said, we are not\ndropping the assumption that teammates are trustable.\nWe still stick to this, only that team members cannot \novertrust each other; an agent has to consider the possibility that\nits information being shared with others might not be as\neffective as expected due to the recipients\" limited \nprocessing capacities. Second, human teammates are bounded by\ntheir cognitive capacities. As far as we know, the research\nreported here is the first attempt in the area of \nhumancentered multi-agent teamwork that really considers \nbuilding and using human\"s cognitive load model to facilitate\nteamwork involving both humans and agents.\nWe use Hi, Ai to denote Human-Agent-Pair (HAP) i.\n3.1 Computational Cognitive Capacity Model\nAn intelligent agent being a cognitive aid, it is desirable\nthat the model of its human partner implemented within\nthe agent is cognitively-acceptable, if not descriptively \naccurate. Of course, building a cognitive load model that is\ncognitively-acceptable is not trivial; there exist a variety of\ncognitive load theories and different measuring techniques.\nWe here choose to focus on the performance variables of\nsecondary tasks, given the ample evidence supporting \nsecondary task performance as a highly sensitive and reliable\ntechnique for measuring human\"s cognitive load [12]. It\"s\nworth noting that just for the purpose of estimating a \nhuman subject\"s cognitive load, any artificial task (e.g, pressing\na button in response to unpredictable stimuli) can be used\nas a secondary task to force the subject to go through. \nHowever, in a realistic application, we have to make sure that the\nselected secondary task interacts with the primary task in\nmeaningful ways, which is not easy and often depends on the\ndomain problem at hand. For example, in the experiment\nbelow, we used the number of newly available information\ncorrectly recalled as the secondary task, and the \neffective0 1 2 3 4\nnegligibly slightly fairly heavily overly\n0.4 0.4 0.4 0.4 0.6\n0.4\n0.2 0.1\n0.2\n0.3\n0.2\n0.2\n0.1\n0.1\n0.25\n0.25\n0.1\n0.2\n0.2\n0 1 2 3 4 5 6 7 8 \u00e2\u2030\u00a5 9\nB =\n0\n1\n2\n3\n4\n\u00e2\u017d\u00a1\n\u00e2\u017d\u00a2\n\u00e2\u017d\u00a2\n\u00e2\u017d\u00a2\n\u00e2\u017d\u00a2\n\u00e2\u017d\u00a3\n0 0 0 0 0 0.02 0.03 0.05 0.1 0.8\n0 0 0 0 0 0.05 0.05 0.1 0.7 0.1\n0 0 0 0 0.01 0.02 0.45 0.4 0.1 0.02\n0.02 0.03 0.05 0.15 0.4 0.3 0.03 0.02 0 0\n0.1 0.3 0.3 0.2 0.1 0 0 0 0 0\n\u00e2\u017d\u00a4\n\u00e2\u017d\u00a5\n\u00e2\u017d\u00a5\n\u00e2\u017d\u00a5\n\u00e2\u017d\u00a5\n\u00e2\u017d\u00a6\nFigure 2: A HMM Cognitive Load Model.\nness of information sharing as the primary task. This is\nrealistic to intelligence workers because in time stress \nsituations they have to know what information to share in order\nto effectively establish an awareness of the global picture.\nIn the following, we adopt the Hidden Markov Model\n(HMM) approach [13] to model human\"s cognitive \ncapacity. It is thus assumed that at each time step the secondary\ntask performance of a human subject in a team composed\nof human-agent-pairs (HAP) is observable to all the team\nmembers. Human team members\" secondary task \nperformance is used for estimating their hidden cognitive loads.\nA HMM is denoted by \u00ce\u00bb = N, V, A, B, \u00cf\u20ac , where N is a\nset of hidden states, V is a set of observation symbols, A\nis a set of state transition probability distributions, B is a\nset of observation symbol probability distributions (one for\neach hidden state), and \u00cf\u20ac is the initial state distribution.\nWe consider a 5-state HMM model of human cognitive\nload as follows (Figure 2). The hidden states are 0 \n(negligiblyloaded), 1 (slightly-loaded), 2 (fairly-loaded), 3 \n(heavilyloaded), and 4 (overly loaded). The observable states are\ntied with secondary task performance, which, in this study,\nis measured in terms of the number of items correctly \nrecalled. According to Miller\"s 7\u00c2\u00b12 rule, the observable states\ntake integer values from 0 to 9 ( the state is 9 when the\nnumber of items correctly recalled is no less than 9). For\nthe example B Matrix given in Fig. 2, it is very likely that\nthe cognitive load of the subject is negligibly when the\nnumber of items correctly recalled is larger than 9.\nHowever, to determine the current hidden load status of\na human partner is not trivial. The model might be \noversensitive if we only consider the last-step secondary task\nperformance to locate the most likely hidden state. There\nis ample evidence suggesting that human cognitive load is\na continuous function over time and does not manifest \nsudden shifts unless there is a fundamental changes in tasking\ndemands. To address this issue, we place a constraint on\nthe state transition coefficients: no jumps of more than 2\nstates are allowed. In addition, we take the position that,\na human subject is very likely overloaded if his secondary\ntask performance is mostly low in recent time steps, while\nhe is very likely not overloaded if his secondary task \nperformance is mostly high recently. This leads to the following\nWindowed-HMM approach.\nGiven a pre-trained HMM \u00ce\u00bb of human cognitive load and\nthe recent observation sequence Ot of length w, let \nparameter w be the effective window size, \u00ce\u00b5\u00ce\u00bb\nt be the estimated\nhidden state at time step t. First apply the HMM to the\nobservation sequence to find the optimal sequence of hidden\nstates S\u00ce\u00bb\nt = s1s2 \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 sw (Viterbi algorithm). Then, compute\nthe estimated hidden state \u00ce\u00b5\u00ce\u00bb\nt for the current time step, \nviewing it as a function of S\u00ce\u00bb\nt . We consider all the hidden states\nin S\u00ce\u00bb\nt , weighted by their respective distance to \u00ce\u00b5\u00ce\u00bb\nt\u00e2\u02c6\u20191 (the \nestimated state of the last step): the closer of a state in S\u00ce\u00bb\nt\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 397\nto \u00ce\u00b5\u00ce\u00bb\nt\u00e2\u02c6\u20191, the higher probability of the state being \u00ce\u00b5\u00ce\u00bb\nt . \u00ce\u00b5\u00ce\u00bb\nt is\nset to be the state with the highest probability (note that a\nstate may have multiple appearances in S\u00ce\u00bb\nt ). More formally,\nthe probability of state s \u00e2\u02c6\u02c6 S being \u00ce\u00b5\u00ce\u00bb\nt is given by:\np\u00ce\u00bb(s, t) =\ns=sj \u00e2\u02c6\u02c6S\u00ce\u00bb\nt\n\u00ce\u00b7(sj)e\u00e2\u02c6\u2019|sj \u00e2\u02c6\u2019\u00ce\u00b5\u00ce\u00bb\nt\u00e2\u02c6\u20191|\n, (1)\nwhere \u00ce\u00b7(sj) = ej\n/ w\nk=1 ek\nis the weight of sj \u00e2\u02c6\u02c6 S\u00ce\u00bb\nt (the\nmost recent hidden state has the most significant influence\nin predicting the next state). The estimated state for the\ncurrent step is the state with maximum likelihood:\n\u00ce\u00b5\u00ce\u00bb\nt = argmax\ns\u00e2\u02c6\u02c6S\u00ce\u00bb\nt\np\u00ce\u00bb(s, t) (2)\n3.2 Agent Processing Load Model\nAccording to schema theory [11], multiple elements of \ninformation can be chunked as single elements in cognitive\nschemas. A schema can hold a huge amount of information,\nyet is processed as a single unit. We adapt this idea and \nassume that agent i\"s estimation of agent j\"s processing load\nat time step t is a function of two factors: the number of\nchunks cj(t) and the total number sj(t) of information \nbeing considered by agent j. If cj(t) and sj(t) are observable\nto agent i, agent i can employ a Windowed-HMM approach\nas described in Section 3.1 to model and estimate agent j\"s\ninstantaneous processing load.\nIn the study reported below, we also used 5-state HMM\nmodels for agent processing load. With the 5 hidden states\nsimilar to the HMM models adopted for human cognitive\nload, we employed multivariate Gaussian observation \nprobability distributions for the hidden states.\n3.3 HAP\"s Processing Load Model\nAs discussed above, a Human-Agent-Pair (HAP) is viewed\nas a unit when teaming up with other HAPs. The processing\nload of a HAP can thus be modeled as the co-effect of the\nprocessing load of the agent and the cognitive load of the\nhuman partner as captured by the agent.\nSuppose agent Ai has models for its processing load and\nits human partner Hi\"s cognitive load. Denote the agent\nprocessing load and human cognitive load of HAP Hi, Ai\nat time step t by \u00ce\u00bci\nt and \u00ce\u00bdi\nt, respectively. Agent Ai can use \u00ce\u00bci\nt\nand \u00ce\u00bdi\nt to estimate the load of Hi, Ai as a whole. Similarly,\nif \u00ce\u00bcj\nt and \u00ce\u00bdj\nt are observable to agent Ai, it can estimate the\nload of Hj, Aj . For model simplicity, we still used 5-state\nHMM models for HAP processing load, with the estimated\nhidden states of the corresponding agent processing load and\nhuman cognitive load as the input observation vectors.\nBuilding a load estimation model is the means. The goal\nis to use the model to enhance information sharing \nperformance so that a team can form better shared mental models\n(e.g., to develop inter-agent role expectations in their \ncollaboration), which is the key to high team performance.\n3.4 Load-Sensitive Information Processing\nEach agent has to adopt a certain strategy to process the\nincoming information. As far as resource-bounded agents\nare concerned, it is of no use for an agent to share \ninformation with teammates who are already overloaded; they\nsimply do not have the capacity to process the information.\nConsider the incoming information processing strategy as\nshown in Table 1. Agent Ai (of HAPi) ignores all the \nincoming information when it is overloaded, and processes all the\nincoming information when it is negligibly loaded. When it\nTable 1: Incoming information processing strategy\nHAPi Load Strategy\nOverly Ignore all shared info\nHeavily Consider every teammate A \u00e2\u02c6\u02c6 [1, 1\nq\n|Q| ],\nrandomly process half amount of info from A;\nIgnore info from any teammate B \u00e2\u02c6\u02c6 ( 1\nq\n|Q|, |Q|]\nFairly Process half of shared info from any teammate\nSlightly Process all info from any A \u00e2\u02c6\u02c6 [1, 1\nq\n|Q| ];\nFor any teammate B \u00e2\u02c6\u02c6 ( 1\nq\n|Q|, |Q|]\nrandomly process half amount of info from B\nNegligibly Process all shared info\nHAPj Process all info from HAPj if it is overloaded\n*Q is a FIFO queue of agents from whom this HAP has received\ninformation at the current step; q is a constant known to all.\nis heavily loaded, Ai randomly processes half of the messages\nfrom those agents which are the first 1/q teammates \nappeared in its communication queue; when it is fairly loaded,\nAi randomly processes half of the messages from any \nteammates; when it is slightly loaded, Ai processes all the \nmessages from those agents which are the first 1/q teammates\nappeared in its communication queue, and randomly \nprocesses half of the messages from other teammates.\nTo further encourage sharing information at the right time,\nthe last row of Table 1 says that HAPi , if having not sent\ninformation to HAPj who is currently overloaded, will \nprocess all the information from HAPj . This can be justified\nfrom resource allocation perspective: an agent can reallocate\nits computing resource reserved for communication to \nenhance its capacity of processing information. This strategy\nfavors never sending information to an overloaded \nteammate, and it suggests that estimating and exploiting \nothers\" loads can be critical to enable an agent to share the\nright information with the right party at the right time.\n4. SYSTEM IMPLEMENTATION\nSMMall (Shared Mental Models for all) is a cognitive\nagent architecture developed for supporting human-centric\ncollaborative computing. It stresses human\"s role in team\nactivities by means of novel collaborative concepts and \nmultiple representations of context woven through all aspects of\nteam work. Here we describe two components pertinent to\nthe experiment reported in Section 5: multi-party \ncommunication and shared mental maps (a complete description of\nthe SMMall system is beyond the scope of this paper).\n4.1 Multi-Party Communication\nMulti-party communication refers to conversations \ninvolving more than two parties. Aside from the speaker, the \nlisteners involved in a conversation can be classified into \nvarious roles such as addressees (the direct listeners), auditors\n(the intended listeners), overhearers (the unintended but \nanticipated listeners), and eavesdroppers (the unanticipated\nlisteners). Multi-party communication is one of the \ncharacteristics of human teams. SMMall agents, which can form\nHuman-Agent-Pairs with human partners, support \nmultiparty communication with the following features.\n1. SMMall supports a collection of multi-party \nperformatives such as MInform (multi-party inform), MAnnounce\n(multi-party announce), and MAsk (multi-party ask). The\nlisteners of a multi-party performative can be addressees, \nauditors, and overhearers, which correspond to \u00e2\u20ac\u02dcto\", \u00e2\u20ac\u02dccc\", and\n\u00e2\u20ac\u02dcbcc\" in e-mail terms, respectively.\n2. SMMall supports channelled-communication. There\n398 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\nare three built-in channels: agentTalk channel (inter-agent\nactivity-specific communication), control channel (meta \ncommunication for team coordination), and world channel \n(communication with the external world). An agent can fully\ntune to a channel to collect messages sent (or cc, bcc)\nto it. An agent can also partially tune to a channel to get\nstatistic information about the messages communicated over\nthe channel. This is particularly useful if an agent wants to\nknow the communication load imposed on a teammate.\n4.2 Shared Belief Map & Load Display\nA concept shared belief map has been proposed and \nimplemented into SMMall; this responds to the need to seek\ninnovative perspectives or concepts that allow group \nmembers to effectively represent and reason about shared mental\nmodels at different levels of abstraction. As described in \nSection 5, humans and agents interacted through shared belief\nmaps in the evaluation of HMM-based load models.\nA shared belief map is a table with color-coded \ninfo-cellscells associated with information. Each row captures the\nbelief model of one team member, and each column \ncorresponds to a specific information type (all columns together\ndefine the boundary of the information space being \nconsidered). Thus, info-cell Cij of a map encodes all the beliefs\n(instances) of information type j held by agent i. Color\ncoding applies to each info-cell to indicate the number of\ninformation instances held by the corresponding agent.\nThe concept of shared belief map helps maintain and\npresent a human partner with a synergy view of the shared\nmental models evolving within a team. Briefly, SMMall has\nimplemented the concept with the following features:\n1. A context menu can be popped up for each info-cell\nto view and share the associated information instances. It\nallows selective (selected subset) or holistic info-sharing.\n2. Mixed-initiative info-sharing: both agents and human\npartners can initiate a multi-party conversation. It also \nallows third-party info-sharing, say, A shares the information\nheld by B with C.\n3. Information types that are semantically related (e.g.,\nby inference rules) can be closely organized. Hence, nearby\ninfo-cells can form meaningful plateaus (or contour lines) of\nsimilar colors. Colored plateaus indicate those sections of a\nshared mental model that bear high overlapping degrees.\n4. The perceptible color (hue) difference manifested from\na shared belief map indicates the information difference among\nteam members, and hence visually represents the potential\ninformation needs of each team member (See Figure 3).\nSMMall has also implemented the HMM-based models\n(Section 3) to allow an agent to estimate its human \npartner\"s and other team members\" cognitive/processing loads.\nAs shown in Fig. 3, below the shared belief map there is\na load display for each team member. There are 2 curves in\na display: the blue (dark) one plots human\"s instantaneous\ncognitive loads and the red one plots the processing loads of\na HAP as a whole. If there are n team members, each agent\nneeds to maintain 2n HMM-based models to support the\nload displays. The human partner of a HAP can adjust her\ncognitive load at runtime, as well as monitor another HAP\"s\nagent processing load and its probability of processing the\ninformation she sends at the current time step. Thus, the\nmore closely a HAP can estimate the actual processing loads\nof other HAPs, the better information sharing performance\nthe HAP can achieve.\nFigure 3: Shared Mental Map Display\nIn sum, shared belief maps allow the inference of who\nneeds what, and load displays allow the judgment of when\nto share information. Together they allow us to investigate\nthe impact of sharing the right info. with the right party at\nthe right time on the evolving of shared mental models.\n4.3 Metrics for Shared Mental Models\nWe here describe how we measure team performance in\nour experiment. We use mental model overlapping \npercentage (MMOP) as the base to measure shared mental\nmodels. MMOP of a group is defined as the intersection of\nall the individual mental states relative to the union of \nindividual mental states of the group. Formally, given a group\nof k agents G = {Ai|1 \u00e2\u2030\u00a4 i \u00e2\u2030\u00a4 k}, let Bi = {Iim|1 \u00e2\u2030\u00a4 m \u00e2\u2030\u00a4 n}\nbe the beliefs (information) held by agent Ai, where each\nIim is a set of information of the same type, and n (the size\nof information space) is fixed for the agents in G, then\nMMOP(G) =\n100\nn\n1\u00e2\u2030\u00a4m\u00e2\u2030\u00a4n\n(\n| \u00e2\u02c6\u00a91\u00e2\u2030\u00a4i\u00e2\u2030\u00a4k Iim|\n| \u00e2\u02c6\u00aa1\u00e2\u2030\u00a4i\u00e2\u2030\u00a4k Iim|\n). (3)\nFirst, a shared mental model can be measured in terms of\nthe distance of averaged subgroup MMOPs to the MMOP\nof the whole group. Without losing generality, we define\npaired SMM distance (subgroups of size 2) D2 as:\nD2 (G) =\n1\u00e2\u2030\u00a4i<j\u00e2\u2030\u00a4k\n(MMOP({Ai, Aj}) \u00e2\u02c6\u2019 MMOP(G))2\n. (4)\nThe MMOP of a subgroup is always larger than the MMOP\nof the whole group. Intuitively, the larger distance from the\nMMOP of a subgroup to that of the whole group, the more\noverlapping mental models the subgroup shares. This \nnotion can be used to measure the tightness of an emerging\nsubgroup or guide the process of team coalition.\nSecond, due to communication or information processing\nlimits, each individual\"s subjective measure of the group\"s\nMMOP can be very different from the group\"s MMOP \nmeasured objectively from external. A shared mental model can\nthus be measured in terms of the closeness of individuals\"\nmeasure of the group\"s MMOP to the objective measure. Let\nMMOP(G) and M MOPi(G) be the objective measure and\nagent Ai\"s subjective measure of the group\"s shared mental\nmodels, respectively. We define SMM deviation D\u00e2\u0160\u00a5 as:\nD\u00e2\u0160\u00a5(G) =\n1\u00e2\u2030\u00a4i\u00e2\u2030\u00a4k\n(MMOPi(G) \u00e2\u02c6\u2019 MMOP(G))2\n. (5)\nObviously, D\u00e2\u0160\u00a5 measures the coherency of the whole group:\nthe smaller the better.\nThird, shared mental models evolve over time. A shared\nmental model can be measured in terms of the \nstableness of the instantaneous measures of MMOP, D , or D\u00e2\u0160\u00a5\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 399\nover time. High performing teams can often maintain their\nshared mental models such that the MMOP is stable at an\nacceptable level as activities proceed, while the MMOP \nmeasure of the shared mental models of a low-performing teams\ncan fluctuate or decrease notably over time.\n5. EXPERIMENT EVALUATION\nIn this section we describe the experiments conducted to\nevaluate the load estimation models and the shared belief\nmap concept for developing team shared mental models.\n5.1 Problem Description\nThe members of a HAP team (i.e., a team composed\nof Human-Agent-Pairs) are situated in a dynamic \nenvironment. Due to their different (maybe overlapping) \nobservability, at each time step they may get different situational\ninformation. The goal of a HAP team is to selectively share\ninformation in a timely manner to develop global situation\nawareness (say, for making critical decisions).\nEach run of the experiment has 45 time steps; each time\nstep lasts 15 seconds. A time step starts with certain \ninfocells of the shared belief map being flashed quickly (for 2\ns). The flashed cells are exactly those with newly available\ninformation that should be shared at that time step. An\ninfo-cell is frozen at the end of a time step: the associated\ninformation is no longer sharable. This requires that the\nnewly available information be shared in time, not later.\nThe human and agent of a HAP assume different roles.\nAn agent governs group communication and processes \nmessages to update the shared belief map on its display. An\nagent, with a pre-trained HMM-based cognitive load model\nfor its human partner and a processing load model for each\nof the other HAPs in the team, also estimates and displays\ntheir instantaneous loads. Human subjects need to perform\na primary task and a secondary task. The secondary task of\na human subject is to remember and mark the cells being\nflashed (not necessarily in the exact order) by left mouse\nclicks. Secondary task performance at step t is thus \nmeasured as the number of cells marked (remembered) correctly\nat t, which is taken as the observable state of the \nHMMbased cognitive load model of that human subject. The\nprimary task of a human subject is to build a shared mental\nmodel of the dynamic situation by sharing the right \ninformation with the right party at the right time. To share the\ninformation associated with an info-cell, a human subject\nneeds to click the right mouse button on the cell, and select\nthe receiving teammate(s) from the popup menu.\nThere are costs associated with information sharing. \nCommunications among HAPs is done by the corresponding \nSMMall agents, which have both limited capacity nin for \nprocessing incoming information and limited outgoing \ncommunication capacity nout. Thus, depending on the current HAP\nload, an agent may randomly ignore part or all of the \nincoming information (having no effect on the establishment of\nshared mental models). On the other hand, each time step\nat most nout number of information-sharing commands can\nbe effective; more than that contribute nothing to the \nestablishment of shared mental models. Sending information\nto an overloaded teammate will waste the capacity that \notherwise can be used to share information with a less loaded\nteammate. This means that at each time step a human \nsubject has to carefully go through three cognitive decisions:\nwhether the information under consideration needs to be\nshared (i.e., whether it is associated with an info-cell just\nflashed), whether a team member is the right party to share\nthe information with (i.e., whether it really needs the \ninformation), and whether this is the right time to share (i.e.,\nwhether the team member is currently overloaded).\nThe above description applies to HAP teams. For teams\ncomposed of SMMall agents only, the agents will take all the\nroles played by an agent or a human partner in HAP teams.\n5.2 Experiment Design\nTo investigate the impacts of the HMM-based load models\non the evolution of shared mental models (SMM), we \nconducted experiments for both Agent teams and HAP teams,\nwhere agent teams involve agent processing load models\nonly, HAP teams involve models of HAP processing load\n(i.e., the co-effect of agent processing load and human \ncognitive load). To get insights on how load predictions and\nmulti-party communication may affect the performance of\nforming SMM, we designed 3 Agent teams (TA1, TA2, TA3)\nand 3 HAP teams (TH1, TH2, TH3), where all agents adopt\nthe strategy in Table 1 to send and process information.\nWhen sharing information, teams of type 1 (TA1, TH1)\nignore load predictions; teams of type 2 (TA2, TH2) \nconsider load predictions; teams of type 3 (TA3, TH3) follow\nload predictions more strictly in the sense that the agents\nwill further group the receivers of a multi-party message\n(MInform) by their loads and split the message into \nmultiple messages with their receivers having the same load.\nFor example, given that agents A1, A2, A3 have load 1, 2,\n1, respectively. An agent A0 in a team of TA2 may send\none multi-party message, while an agent A0 in a team of\nTA3 will send two messages (one to A2, one to A1 and A3).\nIn addition, we controlled agents\" outgoing communication\ncapacities by varying from 6, 8, to 10.\nDue to constraints on communication capacity and \nprocessing capacity, an agent can be inaccurate when tracking\nother teammates\" mental models. In order to measure the\nactual shared mental models, a special SMMall agent named\n\u00e2\u20ac\u02dcOmniAll\" was added to each team to monitor inter-agent\ncommunications and record the actual effects of \ninformation sharing on each agent\"s mental model. This realizes a\nway, as suggested by Klimoski [6], to measuring the degree of\noverlap in immediate, intermediate, and long-term situation\nawareness zones held by group members.\nWe also recorded instantaneous information sharing \nutility, which is defined as follows. At each time step, let T =\nT0, T1, T2, T3, T4 be a sequence of sets, where T0, T1, T2, T3,\nand T4 are sets of teammates whose current load states are\nnegligibly, slightly, fairly, heavily, and overly \nrespectively. Let S be the set of information-sharing \ncommands issued by a human partner at the current step. Let\nMi = {Tk \u00e2\u02c6\u02c6 T|k \u00e2\u2030\u00a4 i, Tk = \u00e2\u02c6\u2026}. Instantaneous info-sharing\nutility is defined as c\u00e2\u02c6\u02c6S s value(c)/|S|, where\ns value(c) =\n\u00e2\u017d\u00a7\n\u00e2\u017d\u00aa\u00e2\u017d\u00a8\n\u00e2\u017d\u00aa\u00e2\u017d\u00a9\n0 receiver(c) \u00e2\u02c6\u02c6 T4\n0 c is known to receiver(c)\n1/|Mi| receiver(c) \u00e2\u02c6\u02c6 Ti, i = 4\n(6)\nIn sum, this study involved 18 types of teams, each team\nhad 4 members, and each team type was tested by 10 domain\nscenarios. 30 human subjects were recruited for HAP teams.\nThe experiment results are plotted in Figures 4, 6, and 7.\nWe next present our findings in this study.\n5.3 Load Estimation Betters Info-sharing\n400 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\n1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45\n35\n40\n45\n50\n55\n60\n65\nEvolution of Shared Mental Models of Agent Teams\ntime step\nPercentageofSMM(%)\nTA1\u00e2\u02c6\u20196\nTA1\u00e2\u02c6\u20198\nTA1\u00e2\u02c6\u201910\nTA2\u00e2\u02c6\u20196\nTA2\u00e2\u02c6\u20198\nTA2\u00e2\u02c6\u201910\nTA3\u00e2\u02c6\u20196\nTA3\u00e2\u02c6\u20198\nTA3\u00e2\u02c6\u201910\n1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45\n14\n16\n18\n20\n22\n24\n26\n28\n30\ntime step\nPercentageofSMM(%)\nEvolution of Shared Mental Models of HAP Teams\nTH1\u00e2\u02c6\u20196\nTH1\u00e2\u02c6\u20198\nTH1\u00e2\u02c6\u201910\nTH2\u00e2\u02c6\u20196\nTH2\u00e2\u02c6\u20198\nTH2\u00e2\u02c6\u201910\nTH3\u00e2\u02c6\u20196\nTH3\u00e2\u02c6\u20198\nTH3\u00e2\u02c6\u201910\nNoSharing\nFigure 4: Evolution of SMMs.\nConsider teams of type 1 (TA1, TH1) and teams of type\n2 (TA2, TH2). First look at the performance of HAP teams\nin Fig. 4, we have: (1) For each team type, the performance\n(percentage of SMM overtime) averaged over 10 teams \nincreased as communication capacity increased \n(TH1-6<TH18<TH1-10, TH2-6<TH2-8<TH2-10). (2) The averaged \nperformance of TH2 teams performed consistently better than\nthe TH1 teams for each capacity setting (TH2-6>TH1-6,\nTH2-8>TH1-8, TH2-10>TH1-10), and the performance \ndifference of TH1 and TH2 teams increased as communication\ncapacity increased. This indicates that, other things being\nequal, the benefit of exploiting load estimation when sharing\ninformation becomes more significant when communication\ncapacity is larger. From Fig. 4 the same findings can be\nderived for the performance of agent teams.\nIn addition, the results also show that the SMMs of each\nteam type were maintained steadily at a certain level after\nabout 20 time steps. However, to maintain a SMM steadily\nat a certain level is a non-trivial team task. The performance\nof teams who did not share any information (the \u00e2\u20ac\u02dcNoSharing\"\ncurve in Fig. 4) decreased constantly as time proceeded.\n5.4 Multi-Party Communication for SMM\nWe now compare teams of type 2 and type 3 (which splits\nmulti-party messages by receivers\" loads). As plotted in Fig.\n4, for HAP teams, the performance of team type 2 for each\nfixed communication capacity was consistently better than\nteam type 3 (TH3-6\u00e2\u2030\u00a4TH2-6, TH3-8<TH2-8, \nTH3-10<TH210); the difference became more significant as \ncommunication capacity increased. These also hold for the Agent teams\n(upper one in Fig. 4). Actually, the performance of type 3\n(a) (b)\nA B C\nMInform I\nA B C\nInform I\nInform I Inform I\nFigure 5: Multi-party messages.\nagent teams was even worse (for each fixed capacity) than\nthe performance of type 1 agent teams.\nThis can be explained by the difference of two-party and\nmulti-party communications. In SMMall, in order to enable\nteam-level inference, each agent maintains an internal model\nof every team member\"s mental model (beliefs). According\nto the semantics of MInform (multi-party inform), when A\nMInforms I to others, assuming all the receivers and \noverhearers will accept I, A will update its internal model of\ntheir beliefs; each of the receivers (overhearers), upon \ngetting the message, will update its own beliefs as well as its\nmodel of the sender\"s and all the other receivers\" beliefs.\nCompared to two-party performatives, multi-party \nperformatives are preferable for forming shared mental models.\nAs illustrated in Fig. 5(a), agent A only needs to perform\nMInform once (with B and C being receivers) to achieve\nthe common knowledge of the shared belief about I (It \nconsumes 2 of A\"s communication resources, one for each \nreceiver). However, to achieve the same effects using Inform\n(Fig. 5(b)), it is hard to form team-level SMM especially\nwhen the team size is big (missing one Inform will nullify all\nothers\" efforts). Moreover, although agent A still consumes\n2, the whole team needs more resources (3 in this case).\nHowever, splitting multi-party messages by receivers\" loads\ndoes enhance subgroup SMMs. In Fig. 6 we plotted the\nother two measures of SMMs (distance and closeness as\ndefined in Sec. 4.3). For HAP teams, TH3>TH2>TH1\nholds in Fig. 6(c) (larger distances indicate better \nsubgroup SMMs), and TH3<TH2<TH1 holds in Fig. 6(d)\n(smaller deviations indicate higher coherency of the whole\nteam). Thus, HAP teams of type 3 achieved better subgroup\nSMMs, and their team members had higher coherent view\nof group SMMs than teams of other types. For agent teams,\nTH3>TH1>TH2 holds in Fig. 6(a), and TH2<TH3<TH1\nholds in Fig. 6(b). Thus, agent teams of type 3 achieved\nbetter subgroup SMMs, and their team members had much\nhigher coherent view of group SMMs than teams of type 1\n(although slightly worse than team type 2).\nHence, generally, multi-party communication encourages\nthe forming/evolving of team SMMs. When a group of\nagents can be partitioned into subteams, splitting messages\nby their loads can be more effective for subteam SMMs.\n5.5 The HMM-based Cognitive Model\nTo validate the HMM-based cognitive load model is \nextremely difficult because detecting the real, noise-resistant\ncognitive load of human beings is far beyond the current\ntechnology. As an indirect judgment, we plotted the \nregression fitted lines for the means of information sharing utility\nof HAP teams with and without load displays. For the HAP\nteams with load displays there is a strong negative linear\nrelationship between info-sharing utility and cognitive load\nlevels (Pearson correlation coefficient is \u00e2\u02c6\u2019\n\u00e2\u02c6\u0161\n0.899 = \u00e2\u02c6\u20190.948\nwith P-Value = 0.014). Because the info-sharing utility\nmeasure and the cognitive load measure are indicators of\nprimary task performance and secondary task performance,\nrespectively, their linear relationship complies with \ncognitive studies that secondary task performance can be used\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 401\nAgent Teams: Distance from Group SMM to Paired SMM\n(averaged)\n80\n90\n100\n110\n120\n130\n140\n150\n160\n1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45time step\nDistance\nAgent Teams: Summed Deviation of Group SMM from Individual\nPerspective\n0\n5\n10\n15\n20\n25\n30\n1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45time step\nDeviation\nTA1\nTA2\nTA3\nHAP Teams: Distance from Group SMM to Paired SMM (averaged)\n130\n140\n150\n160\n170\n180\n190\n200\n210\n1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45time step\nDistance\nHAP Teams: Summed Deviation of Group SMM from Individual\nPerspective\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45time step\nDeviation\nTH1\nTH2\nTH3\n(a) (b) (c) (d)\nFigure 6: The distance of subgroup SMMs and the closeness of individual views of team SMMs.\nto explain primary task performance. However, for teams\nwithout load displays there is a strong quadratic rather than\nlinear relationship (correlation coefficient is\n\u00e2\u02c6\u0161\n0.974 = 0.987\nwith P-Value = 0.015). This indicates that the information\nsharing performance of a HAP team can be significantly \naffected by both the human subject\"s own cognitive load, and\nthe awareness of other team members\" load. Knowing of \nothers\" load (by estimation) will reduce the quadratic relation\nto a linear relation.\nFigure 7: Info-sharing utility by cognitive loads.\n6. CONCLUSION\nRecent research attention on human-centered teamwork\nhighly demands the design of agent systems as cognitive\naids that can model and exploit human partners\" cognitive\ncapacities to offer help unintrusively. In this paper, we \ninvestigated several factors surrounding the challenging \nproblem of evolving shared mental models of teams composed of\nhuman-agent-pairs. The major contribution of this research\nincludes (1) HMM-based load models were proposed for an\nagent to estimate its human partner\"s cognitive load and\nother HAP teammates\" processing loads; (2) The shared \nbelief map concept was introduced and implemented. It allows\ngroup members to effectively represent and reason about\nshared mental models; (3) Experiments were conducted to\nevaluate the HMM-based cognitive/processing load models\nand the impacts of multi-party communication on the \nevolving of team SMMs. The usefulness of shared belief maps was\nalso demonstrated during the experiments.\n7. REFERENCES\n[1] A. D. Baddeley. Working memory. Science,\n255:556-559, 1992.\n[2] J. Bradshaw, M. Sierhuis, A. Acquisti, Y. Gawdiak,\nD. Prescott, R. Jeffers, N. Suri, and R. van Hoof.\nWhat we can learn about human-agent teamwork from\npractice. In Workshop on Teamwork and Coalition\nFormation at AAMAS\"02), Bologna, Italy, 2002.\n[3] P. R. Cohen and H. J. Levesque. Teamwork. Nous,\n25(4):487-512, 1991.\n[4] X. Fan, B. Sun, S. Sun, M. McNeese, and J. Yen.\nRPD-enabled agents teaming with humans for\nmulti-context decision making. In Proceedings of\nAAMAS\"06, pages 34-41. ACM Press, 2006.\n[5] B. Grosz and S. Kraus. Collaborative plans for\ncomplex group actions. Artificial Intelligence,\n86:269-358, 1996.\n[6] R. Klimoski and S. Mohammed. Team mental model:\nConstruct or metaphor? Journal of Management,\n20(2):403-437, 1994.\n[7] A. Lang. The limited capacity model of mediated\nmessage processing. J. of Comm., Winter:46-70, 2000.\n[8] R. G. Lord and K. J. Maher. Alternative information\nprocessing models and their implications for theory,\nresearch, and practice. The Academy of Management\nReview, 15(1):9-28, 1990.\n[9] G. A. Miller. The magical number seven, plus or\nminus two: some limits on our capacity for processing\ninformation. Psychological Review, 63:81-97, 1956.\n[10] E. Norling. Folk psychology for human modelling:\nExtending the BDI paradigm. In Proceedings of\nAAMAS\"04, pages 202-209, 2004.\n[11] F. Paas and J. V. Merrienboer. The efficiency of\ninstructional conditions: an approach to combine\nmental-effort and performance measures. Human\nFactors, 35:737-743, 1993.\n[12] F. Paas, J. E. Tuovinen, H. Tabbers, and P. W. M. V.\nGerven. Cognitive load measurement as a means to\nadvance cognitive load theory. Educational\nPsychologist, 38(1):63-71, 2003.\n[13] L. R. Rabiner. A tutorial on hidden markov models\nand selected applications in speech recognition.\nProceedings of the IEEE, 77:257-286, 1989.\n[14] W. Rouse, J. Cannon-Bowers, and E. Salas. The role\nof mental models in team performance in complex\nsystems. IEEE Trans. on Sys., man, and Cyber,\n22(6):1296-1308, 1992.\n[15] J. Sweller. Cognitive load during problem solving:\neffects on learning. Cog. Science, 12:257-285, 1988.\n[16] B. Xie and G. Salvendy. Prediction of mental workload\nin single and multiple task environments. International\nJournal of Cognitive Ergonomics, 4:213-242, 2000.\n402 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\n": ["shared belief map", "multiagent teamwork", "heuristic", "reasoning", "problem-solving", "collaboration", "teamwork", "expectation", "teamwork schema", "human-agent team performance", "cognitive load theory", "human performance", "resource allocation", "task performance", "info-sharing", "multi-party communication", "cognitive model", "human-center teamwork", "share belief map", ""]}