{"Implementation of a Dynamic Adjustment Mechanism with\nEfficient Replica Selection in Data Grid Environments\nChao-Tung Yang I-Hsien Yang Chun-Hsiang Chen Shih-Yu Wang\nHigh-Performance Computing Laboratory\nDepartment of Computer Science and Information Engineering\nTunghai University\nTaichung City, 40704, Taiwan R.O.C.\nctyang@thu.edu.tw g932813@thu.edu.tw\nABSTRACT\nThe co-allocation architecture was developed in order to enable\nparallel downloading of datasets from multiple servers. Several\nco-allocation strategies have been coupled and used to exploit rate\ndifferences among various client-server links and to address\ndynamic rate fluctuations by dividing files into multiple blocks of\nequal sizes. However, a major obstacle, the idle time of faster\nservers having to wait for the slowest server to deliver the final\nblock, makes it important to reduce differences in finishing time\namong replica servers. In this paper, we propose a dynamic \ncoallocation scheme, namely Recursive-Adjustment Co-Allocation\nscheme, to improve the performance of data transfer in Data Grids.\nOur approach reduces the idle time spent waiting for the slowest\nserver and decreases data transfer completion time. We also\nprovide an effective scheme for reducing the cost of reassembling\ndata blocks.\nCategories and Subject Descriptors\nC.2.4 [Distributed Systems]: Distributed applications.\nH.3.5 [Online Information Services]: Data sharing, Web-based\nservices.\nGeneral Terms\nManagement, Performance, Design, Experimentation.\n1. INTRODUCTION\nData Grids aggregate distributed resources for solving large-size\ndataset management problems. Most Data Grid applications\nexecute simultaneously and access large numbers of data files in\nthe Grid environment. Certain data-intensive scientific\napplications, such as high-energy physics, bioinformatics\napplications and virtual astrophysical observatories, entail huge\namounts of data that require data file management systems to\nreplicate files and manage data transfers and distributed data\naccess. The data grid infrastructure integrates data storage devices\nand data management services into the grid environment, which\nconsists of scattered computing and storage resources, perhaps\nlocated in different countries/regions yet accessible to users [12].\nReplicating popular content in distributed servers is widely used\nin practice [14, 17, 19]. Recently, large-scale, data-sharing\nscientific communities such as those described in [1, 5] used this\ntechnology to replicate their large datasets over several sites.\nDownloading large datasets from several replica locations may\nresult in varied performance rates, because the replica sites may\nhave different architectures, system loadings, and network\nconnectivity. Bandwidth quality is the most important factor\naffecting transfers between clients and servers since download\nspeeds are limited by the bandwidth traffic congestion in the links\nconnecting the servers to the clients.\nOne way to improve download speeds is to determine the best\nreplica locations using replica selection techniques [19]. This\nmethod selects the best servers to provide optimum transfer rates\nbecause bandwidth quality can vary unpredictably due to the\nsharing nature of the internet. Another way is to use co-allocation\ntechnology [17] to download data. Co-allocation of data transfers\nenables the clients to download data from multiple locations by\nestablishing multiple connections in parallel. This can improve\nthe performance compared to the single-server cases and alleviate\nthe internet congestion problem [17]. Several co-allocation\nstrategies were provided in previous work [17]. An idle-time\ndrawback remains since faster servers must wait for the slowest\nserver to deliver its final block. Therefore, it is important to\nreduce the differences in finishing time among replica servers.\nIn this paper, we propose a dynamic co-allocation scheme based\non co-allocation Grid data transfer architecture called \nRecursiveAdjustment Co-Allocation scheme that reduces the idle time spent\nwaiting for the slowest server and improves data transfer\nperformance [24]. Experimental results show that our approach is\nsuperior to previous methods and achieved the best overall\nperformance. We also discuss combination cost and provide an\neffective scheme for reducing it.\nThe remainder of this paper is organized as follows. Related\nbackground review and studies are presented in Section 2 and the\nco-allocation architecture and related work are introduced in\nSection 3. In Section 4, an efficient replica selection service is\nproposed by us. Our research approaches are outlined in Section 5,\nand experimental results and a performance evaluation of our\nscheme are presented in Section 6. Section 7 concludes this\nresearch paper.\n2. BACKGROUND\n2.1 Data Grid\nThe Data Grids enable the sharing, selection, and connection of a\nwide variety of geographically distributed computational and\nstorage resources for solving large-scale data intensive scientific\napplications (e.g., high energy physics, bioinformatics\napplications, and astrophysical virtual observatory). The term\nData Grid traditionally represents the network of distributed\nstorage resources, from archival systems to caches and databases,\nwhich are linked using a logical name space to create global,\npersistent identifiers and provide uniform access mechanisms [4].\nData Grids [1, 2, 16] federate a lot of storage resources. Large\ncollections of measured or computed data are emerging as\nimportant resources in many data intensive applications.\n2.1.1 Replica Management\nReplica management involves creating or removing replicas at a\ndata grid site [19]. In other words, the role of a replica manager is\nto create or delete replicas, within specified storage systems. Most\noften, these replicas are exact copies of the original files, created\nonly to harness certain performance benefits. A replica manager\ntypically maintains a replica catalog containing replica site\naddresses and the file instances. The replica management service\nis responsible for managing the replication of complete and partial\ncopies of datasets, defined as collections of files.\nThe replica management service is just one component in a Data\nGrid environment that provides support for high-performance,\ndata-intensive applications. A replica or location is a subset of a\ncollection that is stored on a particular physical storage system.\nThere may be multiple possibly overlapping subsets of a\ncollection stored on multiple storage systems in a Data Grid.\nThese Grid storage systems may use a variety of underlying\nstorage technologies and data movement protocols, which are\nindependent of replica management.\n2.1.2 Replica Catalog\nAs mentioned above, the purpose of the replica catalog is to\nprovide mappings between logical names for files or collections\nand one or more copies of the objects on physical storage systems.\nThe replica catalog includes optional entries that describe\nindividual logical files. Logical files are entities with globally\nunique names that may have one or more physical instances. The\ncatalog may optionally contain one logical file entry in the replica\ncatalog for each logical file in a collection.\nA Data Grid may contain multiple replica catalogs. For example,\na community of researchers interested in a particular research\ntopic might maintain a replica catalog for a collection of data sets\nof mutual interest. It is possible to create hierarchies of replica\ncatalogs to impose a directory-like structure on related logical\ncollections. In addition, the replica manager can perform access\ncontrol on entire catalogs as well as on individual logical files.\n2.1.3 Replica Selection\nThe purpose of replica selection [16] is to select a replica from\namong the sites which constitute a Data Grid [19]. The criteria of\nselection depend on characteristics of the application. By using\nthis mechanism, users of the Data Grid can easily manage replicas\nof data sets at their sites, with better performance. Much previous\neffort has been devoted to the replica selection problem. The\ncommon process of replica selection consists of three steps: data\npreparation, preprocessing and prediction. Then, applications can\nselect a replica according to its specific attributes. Replica\nselection is important to data-intensive applications, and it can\nprovide location transparency. When a user requests for accessing\na data set, the system determines an appropriate way to deliver the\nreplica to the user.\n2.2 Globus Toolkit and GridFTP\nThe Globus Project [9, 11, 16] provides software tools\ncollectively called The Globus Toolkit that makes it easier to\nbuild computational Grids and Grid-based applications. Many\norganizations use the Globus Toolkit to build computational Grids\nto support their applications. The composition of the Globus\nToolkit can be pictured as three pillars: Resource Management,\nInformation Services, and Data Management. Each pillar\nrepresents a primary component of the Globus Toolkit and makes\nuse of a common foundation of security. GRAM implements a\nresource management protocol, MDS implements an information\nservices protocol, and GridFTP implements a data transfer\nprotocol. They all use the GSI security protocol at the connection\nlayer [10, 11, 16, 13]. The Globus alliance proposed a common\ndata transfer and access protocol called GridFTP that provides\nsecure, efficient data movement in Grid environments [3]. This\nprotocol, which extends the standard FTP protocol, provides a\nsuperset of the features offered by the various Grid storage\nsystems currently in use.\nIn order to solve the appearing problems, the Data Grid\ncommunity tries to develop a secure, efficient data transport\nmechanism and replica management services. GridFTP is a\nreliable, secure and efficient data transport protocol which is\ndeveloped as a part of the Globus project. There is another key\ntechnology from Globus project, called replica catalog [16] which\nis used to register and manage complete and partial copies of data\nsets. The replica catalog contains the mapping information from a\nlogical file or collection to one or more physical files.\n2.3 Network Weather Service\nThe Network Weather Service (NWS) [22] is a generalized and\ndistributed monitoring system for producing short-term\nperformance forecasts based on historical performance\nmeasurements. The goal of the system is to dynamically\ncharacterize and forecast the performance deliverable at the\napplication level from a set of network and computational\nresources. A typical installation involves one nws_nameserver,\none or more nws_memory (which may reside on different\nmachines), and an nws_sensor running on each machine with\nresources which are to be monitored. The system includes sensors\nfor end-to-end TCP/IP performance (bandwidth and latency),\navailable CPU percentage, and available non-paged memory.\n798\n2.4 Sysstat Utilities\nThe Sysstat [15] utilities are a collection of performance\nmonitoring tools for the Linux OS. The Sysstat package\nincorporates the sar, mpstat, and iostat commands. The\nsar command collects and reports system activity information,\nwhich can also be saved in a system activity file for future\ninspection. The iostat command reports CPU statistics and I/O\nstatistics for tty devices and disks. The statistics reported by sar\nconcern I/O transfer rates, paging activity, process-related\nactivities, interrupts, network activity, memory and swap space\nutilization, CPU utilization, kernel activities, and tty statistics,\namong others. Uniprocessor (UP) and Symmetric multiprocessor\n(SMP) machines are fully supported.\n3. CO-ALLOCATION ARCHITECTURE\nAND RELATED WORK\nThe co-allocation architecture proposed in [17] consists of three\nmain components: an information service, a broker/co-allocator,\nand local storage systems. Figure 1 shows the co-allocation of\nGrid Data transfers, which is an extension of the basic template\nfor resource management [7] provided by Globus Toolkit.\nApplications specify the characteristics of desired data and pass\nthe attribute description to a broker. The broker queries available\nresources and gets replica locations from information services [6]\nand replica management services [19], and then gets a list of\nphysical locations for the desired files.\nFigure 1. Data Grid Co-Allocation Architecture [17]\nThe candidate replica locations are passed to a replica selection\nservice [19], which was presented in a previous work [23]. This\nreplica selection service provides estimates of candidate transfer\nperformance based on a cost model and chooses appropriate\namounts to request from the better locations. The co-allocation\nagent then downloads the data in parallel from the selected\nservers.\nIn these researches, GridFTP [1, 11, 16] was used to enable\nparallel data transfers. GridFTP is a high-performance, secure,\nreliable data transfer protocol optimized for high-bandwidth \nwidearea networks. Among its many features are security, parallel\nstreams, partial file transfers, third-party transfers, and reusable\ndata channels. Its partial file transfer ability allows files to be\nretrieved from data servers by specifying the start and end offsets\nof file sections.\nData grids consist of scattered computing and storage resources\nlocated in different countries/regions yet accessible to users [8].\nIn this study we used the grid middleware Globus Toolkit [16] as\nthe data grid infrastructure. The Globus Toolkit provides solutions\nfor such considerations as security, resource management, data\nmanagement, and information services. One of its primary\ncomponents is MDS [6, 11, 16, 25], which is designed to provide\na standard mechanism for discovering and publishing resource\nstatus and configuration information. It provides a uniform and\nflexible interface for data collected by lower-level information\nproviders in two modes: static (e.g., OS, CPU types, and system\narchitectures) and dynamic data (e.g., disk availability, memory\navailability, and loading). And it uses GridFTP [1, 11, 16], a\nreliable, secure, and efficient data transport protocol to provide\nefficient management and transfer of terabytes or petabytes of\ndata in a wide-area, distributed-resource environment.\nAs datasets are replicated within Grid environments for reliability\nand performance, clients require the abilities to discover existing\ndata replicas, and create and register new replicas. A Replica\nLocation Service (RLS) [4] provides a mechanism for discovering\nand registering existing replicas. Several prediction metrics have\nbeen developed to help replica selection. For instance, Vazhkudai\nand Schopf [18, 20, 21] used past data transfer histories to\nestimate current data transfer throughputs.\nIn our previous work [23, 24], we proposed a replica selection\ncost model and a replica selection service to perform replica\nselection. In [17], the author proposes co-allocation architecture\nfor co-allocating Grid data transfers across multiple connections\nby exploiting the partial copy feature of GridFTP. It also provides\nBrute-Force, History-Base, and Dynamic Load Balancing for\nallocating data block.\nBrute-Force Co-Allocation: Brute-Force Co-Allocation\nworks by dividing the file size equally among available\nflows. It does not address the bandwidth differences among\nthe various client-server links.\nHistory-based Co-Allocation: The History-based \nCoAllocation scheme keeps block sizes per flow proportional\nto predicted transfer rates.\nConservative Load Balancing: One of their dynamic \ncoallocation is Conservative Load Balancing. The\nConservative Load Balancing dynamic co-allocation\nstrategy divides requested datasets into k disjoint blocks\nof equal size. Available servers are assigned single blocks\nto deliver in parallel. When a server finishes delivering a\nblock, another is requested, and so on, till the entire file is\ndownloaded. The loadings on the co-allocated flows are\nautomatically adjusted because the faster servers will\ndeliver more quickly providing larger portions of the file.\nAggressive Load Balancing: Another dynamic \ncoallocation strategy, presented in [17], is the Aggressive\nLoad Balancing. The Aggressive Load Balancing dynamic\nco-allocation strategy presented in [17] adds functions that\nchange block size de-liveries by: (1) progressively\nincreasing the amounts of data requested from faster\nservers, and (2) reducing the amounts of data requested\nfrom slower servers or ceasing to request data from them\naltogether.\nThe co-allocation strategies described above do not handle the\nshortcoming of faster servers having to wait for the slowest server\nto deliver its final block. In most cases, this wastes much time and\ndecreases overall performance. Thus, we propose an efficient\napproach called Recursive-Adjustment Co-Allocation and based\n799\non a co-allocation architecture. It improves dynamic co-allocation\nand reduces waiting time, thus improving overall transfer\nperformance.\n4. AN EFFICIENT REPLICA SELECTION\nSERVICE\nWe constructed a replica selection service to enable clients to\nselect the better replica servers in Data Grid environments. See\nbelow for a detailed description.\n4.1 Replica Selection Scenario\nOur proposed replica selection model is illustrated in [23], which\nshows how a client identifies the best location for a desired\nreplica transfer. The client first logins in at a local site and\nexecutes the Data Grid platform application, which checks to see\nif the files are available at the local site. If they are present at the\nlocal site, the application accesses them immediately; otherwise,\nit passes the logical file names to the replica catalog server, which\nreturns a list of physical locations for all registered copies. The\napplication passes this list of replica locations to a replica\nselection server, which identifies the storage system destination\nlocations for all candidate data transfer operations.\nThe replica selection server sends the possible destination\nlocations to the information server, which provides performance\nmeasurements and predictions of the three system factors\ndescribed below. The replica selection server chooses better\nreplica locations according to these estimates and returns location\ninformation to the transfer application, which receives the replica\nthrough GridFTP. When the application finishes, it returns the\nresults to the user.\n4.2 System Factors\nDetermining the best database from many with the same\nreplications is a significant problem. In our model, we consider\nthree system factors that affect replica selection:\nNetwork bandwidth: This is one of the most significant\nData Grid factors since data files in Data Grid\nenvironments are usually very large. In other words, data\nfile transfer times are tightly dependent on network\nbandwidth situations. Because network bandwidth is an\nunstable dynamic factor, we must measure it frequently and\npredict it as accurately as possible. The Network Weather\nService (NWS) is a powerful toolkit for this purpose.\nCPU load: Grid platforms consist of numbers of\nheterogeneous systems, built with different system\narchitectures, e.g., cluster platforms, supercomputers, PCs.\nCPU loading is a dynamic system factor, and a heavy\nsystem CPU load will certainly affect data file downloads\nprocess from the site. The measurement of it is done by the\nGlobus Toolkit / MDS.\nI/O state: Data Grid nodes consist of different\nheterogeneous storage systems. Data files in Data Grids are\nhuge. If the I/O state of a site that we wish to download\nfiles from is very busy, it will directly affect data transfer\nperformance. We measure I/O states using sysstat [15]\nutilities.\n4.3 Our Replica Selection Cost Model\nThe target function of a cost model for distributed and replicated\ndata storage is the information score from the information service.\nWe listed some influencing factors for our cost model in the\npreceding section. However, we must express these factors in\nmathematical notation for further analysis. We assume node i is\nthe local site the user or application logs in on, and node j\npossesses the replica the user or application wants. The seven\nsystem parameters our replica selection cost model considers are:\nScorei-j: the score value represents how efficiently a user or\napplication at node i can acquire a replica from node j\nBW\njiP\n: percentage of bandwidth available from node i to\nnode j; current bandwidth divided by highest theoretical\nbandwidth\nBBW\n: network bandwidth weight defined by the Data Grid\nadministrator\nCPU\njP\n: percentage of node j CPU idle states\nWCPU\n: CPU load weight defined by the Data Grid\nadministrator\nOI\njP /\n: percentage of node j I/O idle states\nWI/O\n: I/O state weight defined by the Data Grid\nadministrator\nWe define the following general formula using these system\nfactors.\nOIOI\nj\nCPUCPU\nj\nBWBW\njiji WPWPWPScore //\n(1)\nThe three influencing factors in this formula: WBW\n, WCPU\n, and\nWI/O\ndescribe CPU, I/O, and network bandwidth weights, which\ncan be determined by Data Grid organization administrators\naccording to the various attributes of the storage systems in Data\nGrid nodes since some storage equipment does not affect CPU\nloading. After several experimental measurements, we determined\nthat network bandwidth is the most significant factor directly\ninfluencing data transfer times. When we performed data transfers\nusing the GridFTP protocol we discovered that CPU and I/O\nstatuses slightly affect data transfer performance. Their respective\nvalues in our Data Grid environment are 80%, 10%, and 10%.\n4.4 Co-Allocation Cost Analysis\nWhen clients download datasets using GridFTP co-allocation\ntechnology, three time costs are incurred: the time required for\nclient authentication to the GridFTP server, actual data\ntransmission time, and data block reassembly time.\nAuthentication Time: Before a transfer, the client must load\na Globus proxy and authenticate itself to the GridFTP\nserver with specified user credentials. The client then\nestablishes a control channel, sets up transfer parameters,\nand requests data channel creation. When the channel has\nbeen established, the data begins flowing.\nTransmission Time: Transmission time is measured from\nthe time when the client starts transferring to the time when\nall transmission jobs are finished, and it includes the time\n800\nrequired for resetting data channels between transfer\nrequests. Data pathways need be opened only once and\nmay handle many transfers before being closed. This\nallows the same data pathways to be used for multiple file\ntransfers. However, data channels must be explicitly reset\nbetween transfer requests. This is less time-costly.\nCombination Time: Co-allocation architecture exploits the\npartial copy feature of the GridFTP data movement tool to\nenable data transfers across multiple connections. With\npartial file transfer, file sections can be retrieved from data\nservers by specifying only the section start and end offsets.\nWhen these file sections are delivered, they may need to be\nreassembled; the reassembly operation incurs an additional\ntime cost.\n5. DYNAMIC CO-ALLOCATION\nSTRATEGY\nDynamic co-allocation, described above, is the most efficient\napproach to reducing the influence of network variations between\nclients and servers. However, the idle time of faster servers\nawaiting the slowest server to deliver the last block is still a major\nfactor affecting overall efficiency, which Conservative Load\nBalancing and Aggressive Load Balancing [17] cannot effectively\navoid. The approach proposed in the present paper, a dynamic\nallocation mechanism called Recursive-Adjustment \nCoAllocation can overcome this, and thus, improve data transfer\nperformance.\n5.1 Recursive-Adjustment Co-Allocation\nRecursive-Adjustment Co-Allocation works by continuously\nadjusting each replica server\"s workload to correspond to its \nrealtime bandwidth during file transfers. The goal is to make the\nexpected finish time of all servers the same. As Figure 2 shows,\nwhen an appropriate file section is first selected, it is divided into\nproper block sizes according to the respective server bandwidths.\nThe co-allocator then assigns the blocks to servers for transfer. At\nthis moment, it is expected that the transfer finish time will be\nconsistent at E(T1). However, since server bandwidths may\nfluctuate during segment deliveries, actual completion time may\nbe dissimilar (solid line, in Figure 2). Once the quickest server\nfinishes its work at time T1, the next section is assigned to the\nservers again. This allows each server to finish its assigned \nworkload by the expected time at E(T2). These adjustments are\nrepeated until the entire file transfer is finished.\nServer 1\nServer 2\nServer 3\nRound 1 Round 2\nE(T1) E(T2)T1\nFile A Section 1 Section 2 ... ...\n...\nFigure 2. The adjustment process\nThe Recursive-Adjustment Co-Allocation process is illustrated in\nFigure 3. When a user requests file A, the replica selection service\nresponds with the subset of all available servers defined by the\nmaximum performance matrix. The co-allocation service gets this\nlist of selected replica servers. Assuming n replica servers are\nselected, Si denotes server i such that 1 i n. A connection for\nfile downloading is then built to each server. The \nRecursiveAdjustment Co-Allocation process is as follows. A new section of\na file to be allocated is first defined. The section size, SEj, is:\nSEj = UnassignedFileSize , (0 < < 1) (2)\nwhere SEj denotes the section j such that 1 j k, assuming we\nallocate k times for the download process. And thus, there are k\nsections, while Tj denotes the time section j allocated.\nUnassignedFileSize is the portion of file A not yet distributed for\ndownloading; initially, UnassignedFileSize is equal to the total\nsize of file A. is the rate that determines how much of the\nsection remains to be assigned.\nFigure 3. The Recursive-Adjustment Co-Allocation process.\nIn the next step, SEj is divided into several blocks and assigned to\nn servers. Each server has a real-time transfer rate to the client\nof Bi, which is measured by the Network Weather Service (NWS)\n[18]. The block size per flow from SEj for each server i at time\nTj is:\ni\nn\ni\nii\nn\ni\niji zeUnFinishSiBBzeUnFinishSiSES -)(\n11\n(3)\nwhere UnFinishSizei denotes the size of unfinished transfer\nblocks that is assigned in previous rounds at server i.\nUnFinishSizei is equal to zero in first round. Ideally, depending to\nthe real time bandwidth at time Tj, every flow is expected to\nfinish its workload in future.\nThis fulfills our requirement to minimize the time faster servers\nmust wait for the slowest server to finish. If, in some cases,\nnetwork variations greatly degrade transfer rates, UnFinishSizei\nmay exceed\nn\ni\nii\nn\ni\nij BBzeUnFinishSiSE\n11\n*)( , which is the\ntotal block size expected to be transferred after Tj. In such cases,\nthe co-allocator eliminates the servers in advance and assigns SEj\nto other servers. After allocation, all channels continue\ntransferring data blocks. When a faster channel finishes its\nassigned data blocks, the co-allocator begins allocating an\nunassigned section of file A again. The process of allocating data\n801\nblocks to adjust expected flow finish time continues until the\nentire file has been allocated.\n5.2 Determining When to Stop Continuous\nAdjustment\nOur approach gets new sections from whole files by dividing\nunassigned file ranges in each round of allocation. These\nunassigned portions of the file ranges become smaller after each\nallocation. Since adjustment is continuous, it would run as an\nendless loop if not limited by a stop condition.\nHowever, when is it appropriate to stop continuous adjustment?\nWe provide two monitoring criteria, LeastSize and\nExpectFinishedTime, to enable users to define stop thresholds.\nWhen a threshold is reached, the co-allocation server stopped\ndividing the remainder of the file and assigns that remainder as\nthe final section. The LeastSize criterion specifies the smallest file\nwe want to process, and when the unassigned portion of\nUnassignedFileSize drops below the LeastSize specification,\ndivision stops. ExpectFinishedTime criterion specifies the\nremaining time transfer is expected to take. When the expected\ntransfer time of the unassigned portion of a file drops below the\ntime specified by ExpectFinishedTime, file division stops. The\nexpected rest time value is determined by:\n1\nn\ni\niBFileSizeUnAssigned (4)\nThese two criteria determine the final section size allocated.\nHigher threshold values will induce fewer divisions and yield\nlower co-allocation costs, which include establishing connections,\nnegotiation, reassembly, etc. However, although the total \ncoallocation adjustment time may be lower, bandwidth variations\nmay also exert more influence. By contrast, lower threshold\nvalues will induce more frequent dynamic server workload\nadjustments and, in the case of greater network fluctuations, result\nin fewer differences in server transfer finish time. However, lower\nvalues will also increase co-allocation times, and hence, increase\nco-allocation costs. Therefore, the internet environment,\ntransferred file sizes, and co-allocation costs should all be\nconsidered in determining optimum thresholds.\n5.3 Reducing the Reassembly Overhead\nThe process of reassembling blocks after data transfers using \ncoallocation technology results in additional overhead and decreases\noverall performance. The reassembly overhead is related to total\nblock size, and could be reduced by upgrading hardware\ncapabilities or using better software algorithms. We propose an\nefficient alternative reassembly mechanism to reduce the added\ncombination overhead after all block transmissions are finished. It\ndiffers from the conventional method in which the software starts\nassembly after all blocks have been delivered by starting to\nassemble blocks once the first deliveries finish. Of course, this\nmakes it necessary to maintain the original splitting order.\nCo-allocation strategies such as Conservative Load Balancing and\nRecursive-Adjustment Co-Allocation produce additional blocks\nduring file transfers and can benefit from enabling reassembly\nduring data transfers. If some blocks are assembled in advance,\nthe time cost for assembling the blocks remaining after all\ntransfers finish can be reduced.\n6. EXPERIMENTAL RESULTS AND\nANALYSIS\nIn this section, we discuss the performance of our \nRecursiveAdjustment Co-Allocation strategy. We evaluate four \ncoallocation schemes: (1) Brute-Force (Brute), (2) History-based\n(History), (3) Conservative Load Balancing (Conservative) and (4)\nRecursive-Adjustment Co-Allocation (Recursive). We analyze the\nperformance of each scheme by comparing their transfer finish\ntime, and the total idle time faster servers spent waiting for the\nslowest server to finish delivering the last block. We also analyze\nthe overall performances in the various cases.\nWe performed wide-area data transfer experiments using our\nGridFTP GUI client tool. We executed our co-allocation client\ntool on our testbed at Tunghai University (THU), Taichung City,\nTaiwan, and fetched files from four selected replica servers: one\nat Providence University (PU), one at Li-Zen High School (LZ),\none at Hsiuping Institute of Technology School (HIT), and one at\nDa-Li High School (DL). All these institutions are in Taiwan, and\neach is at least 10 Km from THU. Figure 4 shows our Data Grid\ntestbed. Our servers have Globus 3.0.2 or above installed.\nInternet\nTHU\nLi-Zen High\nSchool (LZ)\nHITCeleron 900 MHz\n256 MB RAM\n60 GB HD\nAMD Athlon(tm) XP 2400+\n1024 MB RAM\n120 GB HD\nPentium 4 2.8 GHz\n512 MB RAM\n80 GB HD\nPU\nDa-Li High\nSchool (DL)\nAthlon MP 2000 MHz *2\n1 GB RAM\n60 GB HD\nPentium 4 1.8 GHZ\n128 MB RAM\n40 GB HD\nPentium 4 2.5 GHZ\n512 MB RAM\n80 GB HD\nFigure 4. Our Data Grid testbed\nIn the following experiments, we set = 0.5, the LeastSize\nthreshold to 10MB, and experimented with file sizes of 10 MB,\n50MB, 100MB, 500MB, 1000MB, 2000MB, and 4000MB. For\ncomparison, we measured the performance of Conservative Load\nBalancing on each size using the same block numbers. Figure 5\nshows a snapshot of our GridFTP client tool. This client tool is\ndeveloped by using Java CoG. It allows easier and more rapid\napplication development by encouraging collaborative code reuse\nand avoiding duplication of effort among problem-solving\nenvironments, science portals, Grid middleware, and collaborative\npilots. Table 1 shows average transmission rates between THU\nand each replica server. These numbers were obtained by\ntransferring files of 500MB, 1000MB, and 2000MB from a single\nreplica server using our GridFTP client tool, and each number is\nan average over several runs.\nTable 1. GridFTP end-to-end transmission rate from THU to\nvarious servers\nServer Average transmission rate\nHIT 61.5 Mbps\nLZ 59.5 Mbps\nDL 32.1 Mbps\nPU 26.7 Mbps\n802\nFigure 5. Our GridFTP client tool\nWe analyzed the effect of faster servers waiting for the slowest\nserver to deliver the last block for each scheme. Figure 6(a) shows\ntotal idle time for various file sizes. Note that our \nRecursiveAdjustment Co-Allocation scheme achieved significant\nperformance improvements over other schemes for every file size.\nThese results demonstrate that our approach efficiently reduces\nthe differences in servers finish times. The experimental results\nshown in Figure 6(b) indicate that our scheme beginning block\nreassembly as soon as the first blocks have been completely\ndelivered reduces combination time, thus aiding co-allocation\nstrategies like Conservative Load Balancing and \nRecursiveAdjustment Co-Allocation that produce more blocks during data\ntransfers.\nFigure 7 shows total completion time experimental results in a\ndetailed cost structure view. Servers were at PU, DL, and HIT,\nwith the client at THU. The first three bars for each file size\ndenote the time to download the entire file from single server,\nwhile the other bars show co-allocated downloads using all three\nservers. Our co-allocation scheme finished the job faster than the\nother co-allocation strategies. Thus, we may infer that the main\ngains our technology offers are lower transmission and\ncombination times than other co-allocation strategies.\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n100 500 1000 1500 2000\nFile Size (MB)\nWaitTime(Sec)\nBrute3 History3 Conservative3 Recursive3\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n500 1000 1500 2000\nFile Size (MB)\nCombinationTime(Sec)\nBrute3 History3 Conservative3 Recursive3\nFigure 6. (a) Idle times for various methods; servers are at PU,\nDL, and HIT. (b) Combination times for various methods;\nservers are at PU, DL, and HIT.\nIn the next experiment, we used the Recursive-Adjustment \nCoAllocation strategy with various sets of replica servers and\nmeasured overall performances, where overall performance is:\nTotal Performance = File size/Total Completion Time (5)\nTable 2 lists all experiments we performed and the sets of replica\nservers used. The results in Figure 8(a) show that using \ncoallocation technologies yielded no improvement for smaller file\nsizes such as 10MB. They also show that in most cases, overall\nperformance increased as the number of co-allocated flows\nincreased. We observed that for our testbed and our co-allocation\ntechnology, overall performance reached its highest value in the\nREC3_2 case. However, in the REC4 case, when we added one\nflow to the set of replica servers, the performance did not increase.\nOn the contrary, it decreased. We can infer that the co-allocation\nefficiency reached saturation in the REC3_2 case, and that\nadditional flows caused additional overhead and reduced overall\nperformance. This means that more download flows do not\nnecessarily result in higher performance. We must choose\nappropriate numbers of flows to achieve optimum performance.\nWe show the detailed cost structure view for the case of REC3_2\nand the case of REC4 in Figure 8(b). The detailed cost consists of\nauthentication time, transfer time and combination time.\n0\n100\n200\n300\n400\n500\n600\nPU1\nDL1\nHIT1\nBRU3\nHIS3\nCON3\nREC3\nPU1\nDL1\nHIT1\nBRU3\nHIS3\nCON3\nREC3\nPU1\nDL1\nHIT1\nBRU3\nHIS3\nCON3\nREC3\nPU1\nDL1\nHIT1\nBRU3\nHIS3\nCON3\nREC3\n500 1000 1500 2000\nFile Size (MB)\nCompletionTime(Sec)\nAuthentication Time Transmission Time Combination Time\nFigure 7. Completion times for various methods; servers are\nat PU, DL, and HIT.\nTable 2. The sets of replica servers for all cases\nCase Servers\nPU1 PU\nDL1 DL\nREC2 PU, DL\nREC3_1 PU, DL, LZ\nREC3_2 PU, DL, HIT\nREC4 PU, DL, HIT, LZ\n0\n10\n20\n30\n40\n50\n60\n70\n10 50 100 500 1000 1500 2000\nFile Size (MB)\nOverallPerformance(Mbits)\nPU1 DL1 REC2 REC3_1 REC3_2 REC4\n0\n10\n20\n30\n40\n50\n60\n70\nREC3_2\nREC4\nREC3_2\nREC4\nREC3_2\nREC4\nREC3_2\nREC4\nREC3_2\nREC4\nREC3_2\nREC4\nREC3_2\nREC4\n10 50 100 500 1000 1500 2000\nFile Size (MB)\nOverallPerformance(Mbits)\nAuthentication Time Transmission Time Combination Time\nFigure 8. (a) Overall performances for various sets of servers.\n(b) Detailed cost structure view for the case of REC3_2 and\nthe case of REC4.\n7. CONCLUSIONS\nThe co-allocation architecture provides a coordinated agent for\nassigning data blocks. A previous work showed that the dynamic\nco-allocation scheme leads to performance improvements.\nHowever, it cannot handle the idle time of faster servers, which\nmust wait for the slowest server to deliver its final block. We\nproposed the Recursive-Adjustment Co-Allocation scheme to\nimprove data transfer performances using the co-allocation\narchitecture in [17]. In this approach, the workloads of selected\nreplica servers are continuously adjusted during data transfers,\nand we provide a function that enables users to define a final\n803\nblock threshold, according to their data grid environment.\nExperimental results show the effectiveness of our proposed\ntechnique in improving transfer time and reducing overall idle\ntime spent waiting for the slowest server. We also discussed the\nre-combination cost and provided an effective scheme for\nreducing it.\n8. REFERENCES\n[1] B. Allcock, J. Bester, J. Bresnahan, A. Chervenak, I. Foster,\nC. Kesselman, S. Meder, V. Nefedova, D. Quesnel, and S.\nTuecke, Data Management and Transfer in \nHighPerformance Computational Grid Environments, Parallel\nComputing, 28(5):749-771, May 2002.\n[2] B. Allcock, J. Bester, J. Bresnahan, A. Chervenak, I. Foster,\nC. Kesselman, S. Meder, V. Nefedova, D. Quesnel, and S.\nTuecke, Secure, Efficient Data Transport and Replica\nManagement for High-Performance Data-Intensive\nComputing, Proc. of the Eighteenth IEEE Symposium on\nMass Storage Systems and Technologies, pp. 13-28, 2001.\n[3] B. Allcock, S. Tuecke, I. Foster, A. Chervenak, and C.\nKesselman. Protocols and Services for Distributed \nDataIntensive Science. ACAT2000 Proceedings, pp. 161-163,\n2000.\n[4] A. Chervenak, E. Deelman, I. Foster, L. Guy, W. Hoschek,\nA. Iamnitchi, C. Kesselman, P. Kunszt, and M. Ripeanu,\nGiggle: A Framework for Constructing Scalable Replica\nLocation Services, Proc. of SC 2002, Baltimore, MD, 2002.\n[5] A. Chervenak, I. Foster, C. Kesselman, C. Salisbury, and S.\nTuecke, The Data Grid: Towards an Architecture for the\nDistributed Management and Analysis of Large Scientific\nDatasets, Journal of Network and Computer Applications,\n23:187-200, 2001.\n[6] K. Czajkowski, S. Fitzgerald, I. Foster, and C. Kesselman,\nGrid Information Services for Distributed Resource\nSharing, Proc. of the Tenth IEEE International Symposium\non High-Performance Distributed Computing (HPDC-10\"01),\n181-194, August 2001.\n[7] K. Czajkowski, I. Foster, and C. Kesselman. Resource \nCoAllocation in Computational Grids, Proc. of the Eighth\nIEEE International Symposium on High Performance\nDistributed Computing (HPDC-8\"99), August 1999.\n[8] F. Donno, L. Gaido, A. Ghiselli, F. Prelz, and M. Sgaravatto,\nDataGrid Prototype 1, TERENA Networking Conference,\n\nhttp://www.terena.nl/conferences/tnc2002/Papers/p5a2ghiselli.pdf, June 2002,\n[9] I. Foster, C. Kesselman, and S. Tuecke. The Anatomy of\nthe Grid: Enabling Scalable Virtual Organizations. Int. J. of\nSupercomputer Applications and High Performance\nComputing, 15(3), pp. 200-222, 2001.\n[10] I. Foster and C. Kesselman, Globus: A Metacomputing\nInfrastructure Toolkit, Intl J. Supercomputer Applications,\n11(2), pp. 115-128, 1997.\n[11] Global Grid Forum, http://www.ggf.org/\n[12] W. Hoschek, J. Jaen-Martinez, A. Samar, H. Stockinger, and\nK. Stockinger, Data Management in an International Data\nGrid Project, Proc. of First IEEE/ACM International\nWorkshop on Grid Computing - Grid 2000, Bangalore, India,\nDecember 2000.\n[13] IBM Red Books, Introduction to Grid Computing with\nGlobus, IBM Press,\nwww.redbooks.ibm.com/redbooks/pdfs/sg246895.pdf\n[14] H. Stockinger, A. Samar, B. Allcock, I. Foster, K. Holtman,\nand B. Tierney, File and Object Replication in Data Grids,\nJournal of Cluster Computing, 5(3):305-314, 2002.\n[15] SYSSTAT utilities home page,\nhttp://perso.wanadoo.fr/sebastien.godard/\n[16] The Globus Alliance, http://www.globus.org/\n[17] S. Vazhkudai, Enabling the Co-Allocation of Grid Data\nTransfers, Proc. of Fourth International Workshop on Grid\nComputing, pp. 41-51, November 2003.\n[18] S. Vazhkudai and J. Schopf, Using Regression Techniques\nto Predict Large Data Transfers, International Journal of\nHigh Performance Computing Applications (IJHPCA),\n17:249-268, August 2003.\n[19] S. Vazhkudai, S. Tuecke, and I. Foster, Replica Selection in\nthe Globus Data Grid, Proc. of the 1st International\nSymposium on Cluster Computing and the Grid (CCGRID\n2001), pp. 106-113, May 2001.\n[20] S. Vazhkudai, J. Schopf, Predicting Sporadic Grid Data\nTransfers, Proc. of 11th IEEE International Symposium on\nHigh Performance Distributed Computing (HPDC-11 \u00e2\u20ac\u02dc02),\npp. 188-196, July 2002.\n[21] S. Vazhkudai, J. Schopf, and I. Foster, Predicting the\nPerformance of Wide Area Data Transfers, Proc. of the\n16th International Parallel and Distributed Processing\nSymposium (IPDPS 2002), pp.34-43, April 2002, pp. 34 - 43.\n[22] R. Wolski, N. Spring, and J. Hayes, The Network Weather\nService: A Distributed Resource Performance Forecasting\nService for Metacomputing, Future Generation Computer\nSystems, 15(5-6):757-768, 1999.\n[23] Chao-Tung Yang, Chun-Hsiang Chen, Kuan-Ching Li, and\nChing-Hsien Hsu, Performance Analysis of Applying\nReplica Selection Technology for Data Grid Environments,\nPaCT 2005, Lecture Notes in Computer Science, vol. 3603,\npp. 278-287, Springer-Verlag, September 2005.\n[24] Chao-Tung Yang, I-Hsien Yang, Kuan-Ching Li, and \nChingHsien Hsu A Recursive-Adjustment Co-Allocation Scheme\nin Data Grid Environments, ICA3PP 2005 Algorithm and\nArchitecture for Parallel Processing, Lecture Notes in\nComputer Science, vol. 3719, pp. 40-49, Springer-Verlag,\nOctober 2005.\n[25] X. Zhang, J. Freschl, and J. Schopf, A Performance Study\nof Monitoring and Information Services for Distributed\nSystems, Proc. of 12th IEEE International Symposium on\nHigh Performance Distributed Computing (HPDC-12 \u00e2\u20ac\u02dc03),\npp. 270-282, August 2003.\n804\n": ["distributed resource", "data grid application", "replication", "co-allocation", "large dataset", "resource management protocol", "replica", "co-allocation strategy", "server", "performance", "grid computing", "data grid", "replica selection", "data transfer", "globus", "gridftp", ""]}