{"Real-Time Agent Characterization\nand Prediction\nH. Van Dyke Parunak, Sven Brueckner, Robert Matthews, John Sauter, Steve Brophy\nNewVectors LLC\n3520 Green Court, Suite 250\nAnn Arbor, MI 48105 USA\n+1 734 302 4684\n{van.parunak, sven.brueckner, robert.matthews, john.sauter, steve.brophy}@newvectors.net\nABSTRACT\nReasoning about agents that we observe in the world is \nchallenging. Our available information is often limited to observations of\nthe agent\"s external behavior in the past and present. To \nunderstand these actions, we need to deduce the agent\"s internal state,\nwhich includes not only rational elements (such as intentions and\nplans), but also emotive ones (such as fear). In addition, we often\nwant to predict the agent\"s future actions, which are constrained\nnot only by these inward characteristics, but also by the dynamics\nof the agent\"s interaction with its environment. BEE (Behavior\nEvolution and Extrapolation) uses a faster-than-real-time \nagentbased model of the environment to characterize agents\" internal\nstate by evolution against observed behavior, and then predict\ntheir future behavior, taking into account the dynamics of their\ninteraction with the environment.\nCategories and Subject Descriptors\nI.2.6 [Artificial Intelligence]: Learning - parameter learning.\nI.2.11 [Artificial Intelligence]: Distributed Artificial \nIntelligence- multiagent systems.\nGeneral Terms\nAlgorithms, Measurement, Experimentation.\n1. INTRODUCTION\nReasoning about agents that we observe in the world must\nintegrate two disparate levels. Our observations are often limited\nto the agent\"s external behavior, which can frequently be \nsummarized numerically as a trajectory in space-time (perhaps \npunctuated by actions from a fairly limited vocabulary). However, this\nbehavior is driven by the agent\"s internal state, which (in the case\nof a human) may involve high-level psychological and cognitive\nconcepts such as intentions and emotions. A central challenge in\nmany application domains is reasoning from external observations\nof agent behavior to an estimate of their internal state. Such \nreasoning is motivated by a desire to predict the agent\"s behavior.\nThis problem has traditionally been addressed under the \nrubric of plan recognition or plan inference. Work to date \nfocuses almost entirely on recognizing the rational state (as opposed\nto the emotional state) of a single agent (as opposed to an \ninteracting community), and frequently takes advantage of explicit \ncommunications between agents (as in managing conversational \nprotocols). Many realistic problems deviate from these conditions.\nIncreasing the number of agents leads to a combinatorial \nexplosion that can swamp conventional analysis.\nEnvironmental dynamics can frustrate agent intentions.\nThe agents often are trying to hide their intentions (and even\ntheir presence), rather than intentionally sharing information.\nAn agent\"s emotional state may be at least as important as its\nrational state in determining its behavior.\nDomains that exhibit these constraints can often be \ncharacterized as adversarial, and include military combat, competitive\nbusiness tactics, and multi-player computer games.\nBEE (Behavioral Evolution and Extrapolation) is a novel \napproach to recognizing the rational and emotional state of multiple\ninteracting agents based solely on their behavior, without recourse\nto intentional communications from them. It is inspired by \ntechniques used to predict the behavior of nonlinear dynamical \nsystems, in which a representation of the system is continually fit to\nits recent past behavior. For nonlinear dynamical systems, the \nrepresentation is a closed-form mathematical equation. In BEE, it is a\nset of parameters governing the behavior of software agents \nrepresenting the individuals being analyzed. The current version of\nBEE characterizes and predicts the behavior of agents \nrepresenting soldiers engaged in urban combat [8].\nSection 2 reviews relevant previous work. Section 3 \ndescribes the architecture of BEE. Section 4 reports results from\nexperiments with the system. Section 5 concludes. Further details\nthat cannot be included here for the sake of space are available in\nan on-line technical report [16].\n2. PREVIOUS WORK\nBEE bears comparison with previous research in AI (plan\nrecognition), Hidden Markov Models, and nonlinear dynamics\nsystems (trajectory prediction).\n2.1 Plan Recognition in AI\nAgent theory commonly describes an agent\"s cognitive state\nin terms of its beliefs, desires, and intentions (the so-called BDI\nmodel [5, 20]). An agent\"s beliefs are propositions about the state\nof the world that it considers true, based on its perceptions. Its\ndesires are propositions about the world that it would like to be\ntrue. Desires are not necessarily consistent with one another: an\nagent might desire both to be rich and not to work at the same\ntime. An agent\"s intentions, or goals, are a subset of its desires\nthat it has selected, based on its beliefs, to guide its future actions.\nUnlike desires, goals must be consistent with one another (or at\nleast believed to be consistent by the agent).\nAn agent\"s goals guide its actions. Thus one ought to be able\nto learn something about an agent\"s goals by observing its past\nactions, and knowledge of the agent\"s goals in turn enables \nconclusions about what the agent may do in the future.\nThis process of reasoning from an agent\"s actions to its goals\nis known as plan recognition or plan inference. This body of\nwork (surveyed recently at [3]) is rich and varied. It covers both\nsingle-agent and multi-agent (e.g., robot soccer team) plans, \nintentional vs. non-intentional actions, speech vs. non-speech \nbehavior, adversarial vs. cooperative intent, complete vs. incomplete\nworld knowledge, and correct vs. faulty plans, among other \ndimensions.\nPlan recognition is seldom pursued for its own sake. It \nusually supports a higher-level function. For example, in \nhumancomputer interfaces, recognizing a user\"s plan can enable the \nsystem to provide more appropriate information and options for user\naction. In a tutoring system, inferring the student\"s plan is a first\nstep to identifying buggy plans and providing appropriate \nremediation. In many cases, the higher-level function is predicting\nlikely future actions by the entity whose plan is being inferred.\nWe focus on plan recognition in support of prediction. An\nagent\"s plan is a necessary input to a prediction of its future \nbehavior, but hardly a sufficient one. At least two other influences,\none internal and one external, need to be taken into account.\nThe external influence is the dynamics of the environment,\nwhich may include other agents. The dynamics of the real world\nimpose significant constraints.\nThe environment may interfere with the desires of the agent [4,\n10].\nMost interactions among agents, and between agents and the\nworld, are nonlinear. When iterated, these can generate chaos\n(extreme sensitivity to initial conditions).\nA rational analysis of an agent\"s goals may enable us to \npredict what it will attempt, but any nontrivial plan with several steps\nwill depend sensitively at each step to the reaction of the \nenvironment, and our prediction must take this reaction into account\nas well. Actual simulation of futures is one way (the only one we\nknow now) to deal with the impact of environmental dynamics on\nan agent\"s actions.\nHuman agents are also subject to an internal\ninfluence. The agent\"s emotional state can \nmodulate its decision process and its focus of attention\n(and thus its perception of the environment). In\nextreme cases, emotion can lead an agent to\nchoose actions that from the standpoint of a \nlogical analysis may appear irrational.\nCurrent work on plan recognition for \nprediction focuses on the rational plan, and does not\ntake into account either external environmental\ninfluences or internal emotional biases. BEE \nintegrates all three elements into its predictions.\n2.2 Hidden Markov Models\nBEE is superficially similar to Hidden Markov Models\n(HMM\"s [19]). In both cases, the agent has hidden internal state\n(the agent\"s personality) and observable state (its outward \nbehavior), and we wish to learn the hidden state from the observable\nstate (by evolution in BEE, by the Baum-Welch algorithm [1] in\nHMM\"s) and then predict the agent\"s future behavior (by \nextrapolation via ghosts in BEE, by the forward algorithm in HMM\"s).\nBEE offers two important benefits over HMM\"s.\nFirst, a single agent\"s hidden variables do not satisfy the\nMarkov property. That is, their values at t + 1 depend not only on\ntheir values at t, but also on the hidden variables of other agents.\nOne could avoid this limitation by constructing a single HMM\nover the joint state space of all of the agents, but this approach is\ncombinatorially prohibitive. BEE combines the efficiency of \nindependently modeling individual agents with the reality of taking\ninto account interactions among them.\nSecond, Markov models assume that transition probabilities\nare stationary. This assumption is unrealistic in dynamic \nsituations. BEE\"s evolutionary process continually updates the agents\"\npersonalities based on actual observations, and thus automatically\naccounts for changes in the agents\" personalities.\n2.3 Real-Time Nonlinear Systems Fitting\nMany systems of interest can be described by a vector of real\nnumbers that changes as a function of time. The dimensions of the\nvector define the system\"s state space. One typically analyzes\nsuch systems as vector differential equations, e.g.,\n)(xf\ndt\nxd\n.\nWhen f is nonlinear, the system can be formally chaotic, and\nstarting points arbitrarily close to one another can lead to \ntrajectories that diverge exponentially rapidly. Long-range prediction of\nsuch a system is impossible. However, it is often useful to \nanticipate the system\"s behavior a short distance into the future. A \ncommon technique is to fit a convenient functional form for f to the\nsystem\"s trajectory in the recent past, then extrapolate this fit into\nthe future (Figure 1, [7]). This process is repeated constantly, \nproviding the user with a limited look-ahead.\nThis approach is robust and widely applied, but requires \nsystems that can efficiently be described with mathematical \nequations. BEE extends this approach to agent behaviors, which it fits\nto observed behavior using a genetic algorithm.\n3. ARCHITECTURE\nBEE predicts the future by observing the emergent behavior\nof agents representing the entities of interest in a fine-grained\nagent simulation. Key elements of the BEE \narchitecture include the model of an individual\nagent, the pheromone infrastructure through\nwhich agents interact, the information sources\nthat guide them, and the overall evolutionary\ncycle that they execute.\n3.1 Agent Model\nThe agents in BEE are inspired by two \nbodies of work: our previous work on fine-grained\nagents that coordinate their actions through \ndigital pheromones in a shared environment [2, 13,\n17, 18, 21], and the success of previous \nagentbased combat modeling.\nDigital pheromones are scalar variables that\nagents deposit and sense at their current location\na\nc\nb\nd\na\nc\nb\nd\nFigure 1: Tracking a \nnonlinear dynamical system. a =\nsystem state space; b = system\ntrajectory over time; c = recent\nmeasurements of system state;\nd = short-range prediction.\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1427\nin the environment. Agents respond to local concentrations of\nthese variables tropistically, climbing or descending local \ngradients. Their movements change the deposit patterns. This feedback\nloop, together with processes of evaporation and propagation in\nthe environment, support complex patterns of interaction and \ncoordination among the agents [15]. Table 1 shows the BEE\"s \ncurrent pheromone flavors. For example, a living member of the \nadversary emits a RED-ALIVE pheromone, while roads emit a\nMOBILITY pheromone.\nOur soldier agents are inspired by EINSTein and MANA.\nEINSTein [6] represents an agent as a set of six weights, each in\n[-1, 1], describing the agent\"s response to six kinds of \ninformation. Four of these describe the number of alive friendly, alive\nenemy, injured friendly, and injured enemy troops within the\nagent\"s sensor range. The other two weights relate to the agent\"s\ndistance to its own flag and that of the adversary, representing\nobjectives that it seeks to protect and attack, respectively. A \npositive weight indicates attraction to the entity described by the\nweight, while a negative weight indicates repulsion.\nMANA [9] extends the concepts in EINSTein. Friendly and\nenemy flags are replaced by the waypoints pursued by each side.\nMANA includes low, medium, and high threat enemies. In \naddition, it defines a set of triggers (e.g., reaching a waypoint, being\nshot at, making contact with the enemy, being injured) that shift\nthe agent from one personality vector to another. A default state\ndefines the personality vector when no trigger state is active.\nThe personality vectors in MANA and EINSTein reflect both\nrational and emotive aspects of decision-making. The notion of\nbeing attracted or repelled by friendly or adversarial forces in\nvarious states of health is an important component of what we\ninformally think of as emotion (e.g., fear, compassion, \naggression), and the use of the term personality in both EINSTein and\nMANA suggests that the system designers are thinking \nanthropomorphically, though they do not use emotion to describe the\neffect they are trying to achieve. The notion of waypoints to\nwhich an agent is attracted reflects goal-oriented rationality.\nBEE uses an integrated rational-emotive personality model.\nA BEE agent\"s rationality is a vector of seven desires, which\nare values in [-1, +1]: ProtectRed (the adversary), ProtectBlue\n(friendly forces), ProtectGreen (civilians), ProtectKeySites,\nAvoidCombat, AvoidDetection, and Survive. Negative values\nreverse the sense suggested by the label. For example, a negative\nvalue of ProtectRed indicates a desire to harm Red, and an agent\nwith a high positive desire to ProtectRed will be attracted to \nREDALIVE, RED-CASUALTY, and MOBILITY pheromone, and\nwill move at maximum speed.\nThe emotive component of a BEE\"s personality is based on\nthe Ortony-Clore-Collins (OCC) framework [11], and is described\nin detail elsewhere [12]. OCC define emotions as valanced \nreactions to agents, states, or events in the environment. This notion\nof reaction is captured in MANA\"s trigger states. An important\nadvance in BEE\"s emotional model is the recognition that agents\nmay differ in how sensitive they are to triggers. For example,\nthreatening situations tend to stimulate the emotion of fear, but a\ngiven level of threat will produce more fear in a new recruit than\nin a seasoned veteran. Thus our model includes not only \nEmotions, but Dispositions. Each Emotion has a corresponding \nDisposition. Dispositions are relatively stable, and considered constant\nover the time horizon of a run of the BEE, while Emotions vary\nbased on the agent\"s disposition and the stimuli to which it is \nexposed.\nInterviews with military domain experts identified the two\nmost crucial emotions for combat behavior as Anger (with the\ncorresponding disposition Irritability) and Fear (whose disposition\nis Cowardice). Table 2 shows which pheromones trigger which\nemotions. For example, RED-CASUALTY pheromone stimulates\nboth Anger and Fear in a Red agent, but not in a Blue agent. \nEmotions are modeled as agent hormones (internal pheromones) that\nare augmented in the presence of the triggering environmental\ncondition and evaporate over time.\nA non-zero emotion modifies the agent\"s actions. Elevated\nlevel Anger increases movement likelihood, weapon firing \nlikelihood, and tendency toward an exposed posture. Elevated Fear\ndecreases these likelihoods.\nFigure 2 summarizes the BEE\"s personality model. The left\nside is a straightforward BDI model (we prefer the term goal to\nintention). The right side is the emotive component, where an\nappraisal of the agent\"s beliefs, moderated by the disposition,\nleads to an emotion that in turn influences the BDI analysis.\nTable 1. Pheromone flavors in BEE\nPheromone\nFlavor\nDescription\nRedAlive\nRedCasualty\nBlueAlive\nBlueCasualty\nGreenAlive\nGreenCasualty\nEmitted by a living or dead entity of the \nappropriate group (Red = enemy, Blue = friendly,\nGreen = neutral)\nWeaponsFire Emitted by a firing weapon\nKeySite\nEmitted by a site of particular importance to\nRed\nCover Emitted by locations that afford cover from fire\nMobility\nEmitted by roads and other structures that \nenhance agent mobility\nRedThreat\nBlueThreat\nDetermined by external process (see Section\n3.3)\nTable 2: Interactions of pheromones and dispositions/emotions\nDispositions/Emotions\nRed\nPerspective\nBlue\nPerspective\nGreen\nPerspective\nPheromone\nIrritability\n/Anger\nCowardice\n/Fear\nIrritability\n/Anger\nCowardice\n/Fear\nIrritability\n/Anger\nCowardice\n/FearRedAlive X X\nRedCasualty X X\nBlueAlive X X X X\nBlueCasualty X X\nGreenCasualty X X X X\nWeaponsFire X X X X X X\nKeySites X X\n1428 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\n3.2 The BEE Cycle\nBEE\"s major innovation is \nextending the nonlinear systems technique of\nSection 2.2 to agent behaviors. This\nsection describes this process at a high\nlevel, then details the multi-page\npheromone infrastructure that \nimplements it.\n3.2.1 Overview\nFigure 3 is an overview of \nBehavior Evolution and Extrapolation. Each\nactive entity in the battlespace has an\npersistent avatar that continuously \ngenerates a stream of ghost agents \nrepresenting itself. We call the combined\nmodeling entity consisting of avatar and ghosts a polyagent [14].\nGhosts live on a timeline indexed by that begins in the past\nand runs into the future. is offset with respect to the current time\nt. The timeline is divided into discrete pages, each representing\na successive value of . The avatar inserts the ghosts at the \ninsertion horizon. In our current system, the insertion horizon is at - t\n= -30, meaning that ghosts are inserted into a page representing\nthe state of the world 30 minutes ago. At the insertion horizon,\neach ghost\"s behavioral parameters (desires and dispositions) are\nsampled from distributions to explore alternative personalities of\nthe entity it represents.\nEach page between the insertion horizon and = t (now)\nrecords the historical state of the world at the point in the past to\nwhich it corresponds. As ghosts move from page to page, they\ninteract with this past state, based on their behavioral parameters.\nThese interactions mean that their fitness depends not just on their\nown actions, but also on the behaviors of the rest of the \npopulation, which is also evolving. Because advances faster than real\ntime, eventually = t (actual time). At this point, each ghost is\nevaluated based on its location compared with the actual location\nof its corresponding real-world entity.\nThe fittest ghosts have three functions.\n1. The personality of each entity\"s fittest ghost is reported to the\nrest of the system as the likely personality of that entity. This\ninformation enables us to characterize individual warriors as\nunusually cowardly or brave.\n2. The fittest ghosts breed genetically and their offspring return\nto the insertion horizon to continue the fitting process.\n3. The fittest ghosts for each entity form the basis for a\npopulation of ghosts that run past the avatar's present into the\nfuture. Each ghost that runs into the future explores a\ndifferent possible future of the battle, analogous to how some\npeople plan ahead by mentally simulating different ways that\na situation might unfold. Analysis of the behaviors of these\ndifferent possible futures yields predictions.\nThus BEE has three distinct notions of time, all of which\nmay be distinct from real-world time.\n1. Domain time t is the current time in the domain being\nmodeled. If BEE is applied to a real-world situation, this time\nis the same as real-world time. In our experiments, we apply\nBEE to a simulated battle, and domain time is the time stamp\npublished by the simulator. During actual runs, the simulator\nis often paused, so domain time runs slower than real time.\nWhen we replay logs from simulation runs, we can speed\nthem up so that domain time runs faster\nthan real time.\n2. BEE time for a page records the\ndomain time corresponding to the state\nof the world represented on that page,\nand is offset from the current domain\ntime.\n3. Shift time is incremented every time the\nghosts move from one page to the next.\nThe relation between shift time and real\ntime depends on the processing\nresources available.\n3.2.2 Pheromone Infrastructure\nBEE must operate very rapidly, to\nkeep pace with the ongoing battle. Thus\nwe use simple agents coordinated using pheromone mechanisms.\nWe have described the basic dynamics of our pheromone \ninfrastructure elsewhere [2]. This infrastructure runs on the nodes of a\ngraph-structured environment (in the case of BEE, a rectangular\nlattice). Each node maintains a scalar value for each flavor of\npheromone, and provides three functions:\nIt aggregates deposits from individual agents, fusing\ninformation across multiple agents and through time.\nIt evaporates pheromones over time, providing an innovative\nalternative to traditional truth maintenance. Traditionally,\nknowledge bases remember everything they are told unless\nthey have a reason to forget. Pheromone-based systems\nimmediately begin to forget everything they learn, unless it is\ncontinually reinforced. Thus inconsistencies automatically\nremove themselves within a known period.\nIt diffuses pheromones to nearby places, disseminating\ninformation for access by nearby agents.\nThe distribution of each pheromone flavor over the \nenvironment forms a field that represents some aspect of the state of the\nworld at an instant in time. Each page of the timeline is a \ncomplete pheromone field for the world at the BEE time represented\nby that page. The behavior of the pheromones on each page \ndepends on whether the page represents the past or the future.\nEnvironment\nBeliefs\nDesires\nGoal Emotion\nDisposition\nState Process\nAnalysis\nAction\nPerception\nAppraisal\nRational Emotive\nFigure 2: BEE\"s Integrated Rational and\nEmotive Personality Model\nGhost time\n=t(now)\nAvatar\nInsertion Horizon\nMeasure Ghost fitness\nPrediction Horizon\nObserve Ghost prediction\nGhosts\nReadPersonality\nReadPrediction\nEntity\nGhost time\n=t(now)\nAvatar\nInsertion Horizon\nMeasure Ghost fitness\nPrediction Horizon\nObserve Ghost prediction\nGhosts\nReadPersonality\nReadPrediction\nEntity\nFigure 3: Behavioral Emulation and Extrapolation. Each avatar\ngenerates a stream of ghosts that sample the personality space of its\nentity. They evolve against the entity\"s recent observed behavior, and\nthe fittest ghosts run into the future to generate predictions.\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1429\nIn pages representing the future ( > t), the usual pheromone\nmechanisms apply. Ghosts deposit pheromone each time they\nmove to a new page, and pheromones evaporate and propagate\nfrom one page to the next.\nIn pages representing the past ( t), we have an observed\nstate of the real world. This has two consequences for pheromone\nmanagement. First, we can generate the pheromone fields directly\nfrom the observed locations of individual entities, so there is no\nneed for the ghosts to make deposits. Second, we can adjust the\npheromone intensities based on the changed locations of entities\nfrom page to page, so we do not need to evaporate or propagate\nthe pheromones. Both of these simplifications reflect the fact that\nin our current system, we have complete knowledge of the past.\nWhen we introduce noise and uncertainty, we will probably need\nto introduce dynamic pheromones in the past as well as the future.\nExecution of the pheromone infrastructure proceeds on two\ntime scales, running in separate threads.\nThe first thread updates the book of pages each time the \ndomain time advances past the next page boundary. At each step,\nThe former now + 1page is replaced with a new current page,\nwhose pheromones correspond to the locations and strengths of\nobserved units;\nAn empty page is added at the prediction horizon;\nThe oldest page is discarded, since it has passed the insertion\nhorizon.\nThe second thread moves the ghosts from one page to the\nnext, as fast as the processor allows. At each step,\nGhosts reaching the = t page are evaluated for fitness and\nremoved or evolved;\nNew ghosts from the avatars and from the evolutionary process\nare inserted at the insertion horizon;\nA population of ghosts based on the fittest ghosts are inserted at\n= t to run into the future;\nGhosts that have moved beyond the prediction horizon are\nremoved;\nAll ghosts plan their next actions based on the pheromone field\nin the pages they currently occupy;\nThe system computes the next state of each page, including\nexecuting the actions elected by the ghosts, and (in future\npages) evaporating pheromones and recording new deposits\nfrom the recently arrived ghosts.\nGhost movement based on pheromone gradients is a simple\nprocess, so this system can support realistic agent populations\nwithout excessive computer load. In our current system, each \navatar generates eight ghosts per shift. Since there are about 50 \nentities in the battlespace (about 20 units each of Red and Blue and\nabout 5 of Green), we must support about 400 ghosts per page, or\nabout 24000 over the entire book.\nHow fast a processor do we need? Let p be the real-time \nduration of a page in seconds. If each page represents 60 seconds of\ndomain time, and we are replaying a simulation at 2x domain\ntime, p = 30. Let n be the number of pages between the insertion\nhorizon and = t. In our current system, n = 30. Then a shift rate\nof n/p shifts per second will permit ghosts to run from the \ninsertion horizon to the current time at least once before a new page is\ngenerated. Empirically, this level is a lower bound for reasonable\nperformance, and easily achievable on stock WinTel platforms.\n3.3 Information sources\nThe flexibility of the BEE\"s pheromone infrastructure \npermits the integration of numerous information sources as input to\nour characterizations of entity personalities and predictions of\ntheir future behavior. Our current system draws on three sources\nof information, but others can readily be added.\nReal-world observations.-Observations from the real\nworld are encoded into the pheromone field each increment of\nBEE time, as a new current page is generated. Table 1 identifies\nthe entities that generate each flavor of pheromone.\nStatistical estimates of threat regions.-Statistical \ntechniques1 estimate the level of threat to each force (Red or Blue),\nbased on the topology of the battlefield and the known disposition\nof forces. For example, a broad open area with no cover is \nthreatening, especially if the opposite force occupies its margins. The\nresults of this process are posted to the pheromone pages as\nRedThreat pheromone (representing a threat to red) and\nBlueThreat pheromone (representing a threat to Blue).\nAI-based plan recognition.-While plan recognition is not\nsufficient for effective prediction, it is a valuable input. We \ndynamically configure a Bayes net based on heuristics to identify\nthe likely goals that each entity may hold.2 The destinations of\nthese goals function as virtual pheromones. Ghosts include their\ndistance to such points in their action decisions, achieving the \nresult of gradient following without the computational expense of\nmaintaining a pheromone field.\n4. EXPERIMENTAL RESULTS\nWe have tested BEE in a series of experiments in which \nhuman wargamers make decisions that are played out in a battlefield\nsimulator. The commander for each side (Red and Blue) has at his\ndisposal a team of pucksters, human operators who set waypoints\nfor individual units in the simulator. Each puckster is responsible\nfor four to six units. The simulator moves the units, determines\nfiring actions, and resolves the outcome of conflicts. It is\nimportant to emphasize that this simulator is simply a surrogate\nfor a sensor feed from a real-world battlefield\n4.1 Fitting Dispositions\nTo test our ability to fit personalities based on behavior, one\nRed puckster responsible for four units is designated the \nemotional puckster. He selects two of his units to be cowardly\n(chickens) and two to be irritable (Rambos). He does not \ndisclose this assignment during the run. He moves each unit \naccording to the commander\"s orders until the unit encounters \ncircumstances that would trigger the emotion associated with the unit\"s\ndisposition. Then he manipulates chickens as though they are\nfearful (avoiding combat and moving away from Blue), and\nmoves Rambos into combat as quickly as possible. Our software\nreceives position reports on all units, every twenty seconds.\n1\nThis process, known as SAD (Statistical Anomaly Detection), is\ndeveloped by our colleagues Rafael Alonso, Hua Li, and John\nAsmuth at Sarnoff Corporation. Alonso and Li are now at SET\nCorporation.\n2\nThis process, known as KIP (Knowledge-based Intention \nProjection), is developed by our colleagues Paul Nielsen, Jacob\nCrossman, and Rich Frederiksen at Soar Technology.\n1430 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\nThe difference between the two disposition values \n(Irritability - Cowardice) of the fittest ghosts proves a better indicator of\nthe emotional state of the corresponding entity than either value\nby itself. Figure 4 shows the delta disposition for each of the eight\nfittest ghosts at each time step, plotted against the time in seconds,\nfor a unit played as a chicken. The values clearly trend negative.\nFigure 5 shows a similar plot for a Rambo. Rambos tend to die\nearly, and often do not give their ghosts enough time to evolve a\nclear picture of their personality, but in\nthis case the positive Delta Disposition is\nevident before the unit\"s demise.\nTo characterize a unit\"s personality,\nwe maintain a 800-second exponentially\nweighted moving average of the Delta\nDisposition, and declare the unit to be a\nchicken or Rambo if this value passes a\nnegative or positive threshold, respectively. Currently, this \nthreshold is set at 0.25. We are exploring additional filters. For example,\na rapid rate of increase enhances the likelihood of calling a\nRambo; units that seek to avoid detection and avoid combat are\nmore readily called chicken.\nTable 1 shows the detection results for emotional units in a\nrecent series of experiments. We never called a Rambo a chicken.\nIn the one case where we called a chicken a Rambo, logs show\nthat in fact the unit was being played aggressively, rushing toward\noncoming Blue forces. The brave die young, so we almost never\ndetect units played intentionally as Rambos.\nFigure 6 shows a comparison on a separate series of \nexperiments of our emotion detector compared with humans. Two \ncowards were played in each of eleven games. Human observers in\neach game were able to detect a total of 13 of the cowards. BEE\nwas able to detect cowards (= chickens) much earlier than the\nhuman, while missing only one chicken that the humans detected.\nIn addition to these results on units intentionally played as\nemotional, BEE sometimes detects other units as cowardly or\nbrave. Analysis of these units shows that these characterizations\nwere appropriate: units that flee in the face of enemy forces or\nweapons fire are detected as chickens, while those that stand their\nground or rush the adversary are denominated as Rambos.\n4.2 Integrated Predictions\nEach ghost that runs into the future generates a possible path\nthat its unit might follow. The paths in the resulting set over all\nghosts vary in how likely they are, the risk they pose to their own\nor the opposite side, and so forth. In the experiments reported\nhere, we select the future whose ghost receives the most guidance\nfrom pheromones in the environment at each step along the way.\nIn this sense, it is the most likely future. In these experiments, we\nreceive position reports only on units that have actually come\nwithin visual range of Blue units, or on average fewer than half of\nthe live Red units at any time.\nWe evaluate predictions spatially, comparing an entity\"s \nactual location with the location predicted\nfor it 15 minutes earlier. We compare\nBEE with two baselines: a \ngametheoretic predictor based on linguistic\ngeometry [22], and estimates by military\nofficers. In both cases, we use a CEP\n(circular error probable) measure of\naccuracy, the radius of the circle that one\nwould have to draw around each prediction to capture 50% of the\nactual unit locations. The higher the CEP measure, the worse the\naccuracy.\nFigure 7 compares our accuracy with that of the \ngametheoretic predictor. Each point gives the median CEP measure\nover all predictions in a single run. Points above the diagonal \nfavor BEE, while points below the line favor the game-theoretic\npredictor. In all but two missions, BEE is more accurate. In one\nmission, the two systems are comparable, while in one, the \ngameTable 1: Experimental Results on Fitting\nDisposition (16 runs)\nCalled\nCorrectly\nCalled\nIncorrectly\nNot\nCalled\nChickens 68% 5% 27%\nRambos 5% 0% 95%\nFigure 4: Delta Disposition for a Chicken\"s Ghosts.\nFigure 5: Delta Disposition for a Rambo.\nCowards Found vs Percent of Run Time\n0\n2\n4\n6\n8\n10\n12\n14\n0% 20% 40% 60% 80% 100%\nPercent of Run Time (Wall Clock)\nCowardsFound(outof22)\nHuman\nARM-A\nFigure 6: BEE vs. Human.\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1431\ntheoretic predictor is more accurate.\nIn 18 RAID runs, BEE generated 1405 predictions at each of\ntwo time horizons (0 and 15 minutes), while in 18 non-RAID\nruns, staff generated 102 predictions. Figure. 8 shows a \nbox-andwhisker plot of the CEP measures, in meters, of these predictions.\nThe box covers the inter-quartile range with a line at the median,\nwhiskers extend to the most distant data points within 1.5 of the\ninterquartile range from the edge of the box, squares show outliers\nwithin 3 interquartile ranges, and stars show more distant outliers.\nBEE\"s median score even at 15 minutes is lower than either Staff\nmedian. The Wilcoxon test shows that the difference between the\nH15 scores is significant at the 99.76% level, while that between\nthe H0 scores is significant at more than 99.999%.\n5. CONCLUSIONS\nIn many domains, it is important to reason from an entity\"s\nobserved behavior to an estimate of its internal state, and then to\nextrapolate that estimate to predict the entity\"s future behavior.\nBEE performs this task using a faster-than-real-time simulation of\nswarming agents, coordinated through digital pheromones. This\nsimulation integrates knowledge of threat regions, a cognitive\nanalysis of the agent\"s beliefs, desires, and intentions, a model of\nthe agent\"s emotional disposition and state, and the dynamics of\ninteractions with the environment. By evolving agents in this rich\nenvironment, we can fit their internal state to their observed \nbehavior. In realistic wargames, the system successfully detects \ndeliberately played emotions and makes reasonable predictions\nabout the entities\" future behaviors.\nBEE can only model internal state variables that impact the\nagent\"s external behavior. It cannot fit variables that the agent\ndoes not manifest externally, since the basis for the evolutionary\ncycle is a comparison of the outward behavior of the simulated\nagent with that of the real entity. This limitation is serious if our\npurpose is to understand the entity\"s internal state for its own\nsake. If our purpose of fitting agents is to predict their subsequent\nbehavior, the limitation is much less serious. State variables that\ndo not impact behavior, while invisible to a behavior-based \nanalysis, are irrelevant to a behavioral prediction.\nThe BEE architecture lends itself to extension in several\npromising directions.\nThe various inputs being integrated by the BEE are only an \nexample of the kinds of information that can be handled. The \nbasic principle of using a dynamical simulation to integrate a\nwide range of influences can be extended to other inputs as\nwell, requiring much less additional engineering than other\nmore traditional ways of reasoning about how different \nknowledge sources come together in impacting an agent\"s behavior.\nWith such a change in inputs, BEE could be applied more\nwidely than its current domain of adversarial reasoning in \nurban warfare. Potential applications of interest include computer\ngames, business strategy, and sensor fusion.\nOur initial limited repertoire of emotions is a small subset of\nthose that have been distinguished by psychologists, and that\nmight be useful for understanding and projecting behavior. We\nexpect to extend the set of emotions and supporting \ndispositions that BEE can detect.\nThe mapping between an agent\"s psychological (cognitive and\nemotional) state and its outward behavior is not one-to-one.\nSeveral different internal states might be consistent with a\ngiven observed behavior under one set of environmental \nconditions, but might yield distinct behaviors under other conditions.\nIf the environment in the recent past is one that confounds such\ndistinct internal states, we will be unable to distinguish them.\nAs long as the environment stays in this state, our predictions\nwill be accurate, whichever of the internal states we assign to\nthe agent. If the environment then shifts to one under which the\ndifferent internal states lead to different behaviors, using the\npreviously chosen internal state will yield inaccurate \npredictions. One way to address these concerns is to probe the real\nworld, perturbing it in ways that would stimulate distinct \nbehaviors from entities whose psychological state is otherwise \nindistinguishable. Such probing is an important intelligence \ntechnique. BEE\"s faster-than-real-time simulation may enable us to\nidentify appropriate probing actions, greatly increasing the \neffectiveness of intelligence efforts.\n6. ACKNOWLEDGEMENTS\nThis material is based in part upon work supported by the Defense\nAdvanced Research Projects Agency (DARPA) under Contract\nNo. NBCHC040153. Any opinions, findings and conclusions or\nrecommendations expressed in this material are those of the \nauthor(s) and do not necessarily reflect the views of the DARPA or\nthe Department of Interior-National Business Center (DOI-NBC).\nDistribution Statement A (Approved for Public Release, \nDistribution Unlimited).\n7. REFERENCES\n[1] Baum, L. E., Petrie, T., Soules, G., and Weiss, N. A \nmaximization technique occurring in the statistical analysis of \nprob50 100 150 200 250 300\nBEE Median Error\n50\n100\n150\n200\n250\n300\nGLnaideMrorrE\nFigure 7: Median errors for BEE vs. Linguistic Geometry on\neach run.-Squares are Defend missions, triangles are Move\nmissions, diamonds are Attack missions.\nRAID H0 Staff H0 RAID H15 Staff H15\n100\n200\n300\n400\n500\nFigure. 8: Box-and-whisker plots of RAID and Staff predictions\nat 0 and 15 minutes Horizons. Y-axis is CEP radius in meters;\nlower values indicate greater accuracy.\n1432 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\nabilistic functions of Markov chains. Ann. Math. Statist., 41,\n1: 1970, 164-171.\n[2] Brueckner, S. Return from the Ant: Synthetic Ecosystems for\nManufacturing Control. Thesis at Humboldt University \nBerlin, Department of Computer Science, 2000.\n[3] Carberry, S. Techniques for Plan Recognition. User \nModeling and User-Adapted Interaction, 11, 1-2: 2001, 31-48.\n[4] Ferber, J. and M\u00c3\u00bcller, J.-P. Influences and Reactions: a\nModel of Situated Multiagent Systems. In Proceedings of\nSecond International Conference on Multi-Agent Systems\n(ICMAS-96), AAAI, 1996, 72-79.\n[5] Haddadi, A. and Sundermeyer, K. Belief-Desire-Intention\nAgent Architectures. In G. M. P. O'Hare and N. R. Jennings,\nEditors, Foundations of Distributed Artificial Intelligence,\nJohn Wiley, New York, NY, 1996, 169-185.\n[6] Ilachinski, A. Artificial War: Multiagent-based Simulation of\nCombat. Singapore, World Scientific, 2004.\n[7] Kantz, H. and Schreiber, T. Nonlinear Time Series Analysis.\nCambridge, UK, Cambridge University Press, 1997.\n[8] Kott, A. Real-Time Adversarial Intelligence & Decision\nMaking (RAID). vol. 2005, DARPA, Arlington, VA, 2004.\nWeb Site.\n[9] Lauren, M. K. and Stephen, R. T. Map-Aware Non-uniform\nAutomata (MANA)-A New Zealand Approach to Scenario\nModelling. Journal of Battlefield Technology, 5, 1 (March):\n2002, 27ff.\n[10] Michel, F. Formalisme, m\u00c3\u00a9thodologie et outils pour la \nmod\u00c3\u00a9lisation et la simulation de syst\u00c3\u00a8mes multi-agents. Thesis at\nUniversit\u00c3\u00a9 des Sciences et Techniques du Languedoc, \nDepartment of Informatique, 2004.\n[11] Ortony, A., Clore, G. L., and Collins, A. The cognitive \nstructure of emotions. Cambridge, UK, Cambridge University\nPress, 1988.\n[12] Parunak, H. V. D., Bisson, R., Brueckner, S., Matthews, R.,\nand Sauter, J. Representing Dispositions and Emotions in\nSimulated Combat. In Proceedings of Workshop on Defence\nApplications of Multi-Agent Systems (DAMAS05, at\nAAMAS05), Springer, 2005, 51-65.\n[13] Parunak, H. V. D. and Brueckner, S. Ant-Like Missionaries\nand Cannibals: Synthetic Pheromones for Distributed Motion\nControl. In Proceedings of Fourth International Conference\non Autonomous Agents (Agents 2000), 2000, 467-474.\n[14] Parunak, H. V. D. and Brueckner, S. Modeling Uncertain\nDomains with Polyagents. In Proceedings of International\nJoint Conference on Autonomous Agents and Multi-Agent\nSystems (AAMAS'06), ACM, 2006.\n[15] Parunak, H. V. D., Brueckner, S., Fleischer, M., and Odell, J.\nA Design Taxonomy of Multi-Agent Interactions. In \nProceedings of Agent-Oriented Software Engineering IV,\nSpringer, 2003, 123-137.\n[16] Parunak, H. V. D., Brueckner, S., Matthews, R., Sauter, J.,\nand Brophy, S. Characterizing and Predicting Agents via\nMulti-Agent Evolution. Altarum Institute, Ann Arbor, MI,\n2005. http://www.newvectors.net/staff/parunakv/BEE.pdf.\n[17] Parunak, H. V. D., Brueckner, S., and Sauter, J. Digital\nPheromones for Coordination of Unmanned Vehicles. In\nProceedings of Workshop on Environments for Multi-Agent\nSystems (E4MAS 2004), Springer, 2004, 246-263.\n[18] Parunak, H. V. D., Brueckner, S. A., and Sauter, J. Digital\nPheromone Mechanisms for Coordination of Unmanned \nVehicles. In Proceedings of First International Conference on\nAutonomous Agents and Multi-Agent Systems (AAMAS\n2002), ACM, 2002, 449-450.\n[19] Rabiner, L. R. A Tutorial on Hidden Markov Models and\nSelected Applications in Speech Recognition. Proceedings of\nthe IEEE, 77, 2: 1989, 257-286.\n[20] Rao, A. S. and Georgeff, M. P. Modeling Rational Agents\nwithin a BDI Architecture. In Proceedings of International\nConference on Principles of Knowledge Representation and\nReasoning (KR-91), Morgan Kaufman, 1991, 473-484.\n[21] Sauter, J. A., Matthews, R., Parunak, H. V. D., and \nBrueckner, S. Evolving Adaptive Pheromone Path Planning \nMechanisms. In Proceedings of Autonomous Agents and \nMultiAgent Systems (AAMAS02), ACM, 2002, 434-440.\n[22] Stilman, B. Linguistic Geometry: From Search to \nConstruction. Boston, Kluwer, 2000.\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1433\n": ["agent reasoning", "external behavior", "internal state", "agent behavior prediction", "behavioral evolution and extrapolation", "nonlinear dynamical system", "agent's goal", "emotion", "pheromone flavor", "disposition", "future behavior", "plan recognition", "plan inference", "evolution", "prediction", "bdi", "swarm intelligence", "digital pheromone", "dynamics", ""]}