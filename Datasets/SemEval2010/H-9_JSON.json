{"Learn from Web Search Logs to Organize Search Results\nXuanhui Wang\nDepartment of Computer Science\nUniversity of Illinois at Urbana-Champaign\nUrbana, IL 61801\nxwang20@cs.uiuc.edu\nChengXiang Zhai\nDepartment of Computer Science\nUniversity of Illinois at Urbana-Champaign\nUrbana, IL 61801\nczhai@cs.uiuc.edu\nABSTRACT\nEffective organization of search results is critical for \nimproving the utility of any search engine. Clustering search results\nis an effective way to organize search results, which allows\na user to navigate into relevant documents quickly. \nHowever, two deficiencies of this approach make it not always\nwork well: (1) the clusters discovered do not necessarily\ncorrespond to the interesting aspects of a topic from the\nuser\"s perspective; and (2) the cluster labels generated are\nnot informative enough to allow a user to identify the right\ncluster. In this paper, we propose to address these two \ndeficiencies by (1) learning interesting aspects of a topic from\nWeb search logs and organizing search results accordingly;\nand (2) generating more meaningful cluster labels using past\nquery words entered by users. We evaluate our proposed\nmethod on a commercial search engine log data. Compared\nwith the traditional methods of clustering search results, our\nmethod can give better result organization and more \nmeaningful labels.\nCategories and Subject Descriptors: H.3.3 \n[Information Search and Retrieval]: Clustering, Search process\nGeneral Terms: Algorithm, Experimentation\n1. INTRODUCTION\nThe utility of a search engine is affected by multiple \nfactors. While the primary factor is the soundness of the \nunderlying retrieval model and ranking function, how to organize\nand present search results is also a very important factor\nthat can affect the utility of a search engine significantly.\nCompared with the vast amount of literature on retrieval\nmodels, however, there is relatively little research on how to\nimprove the effectiveness of search result organization.\nThe most common strategy of presenting search results is\na simple ranked list. Intuitively, such a presentation \nstrategy is reasonable for non-ambiguous, homogeneous search\nresults; in general, it would work well when the search \nresults are good and a user can easily find many relevant \ndocuments in the top ranked results.\nHowever, when the search results are diverse (e.g., due\nto ambiguity or multiple aspects of a topic) as is often the\ncase in Web search, the ranked list presentation would not\nbe effective; in such a case, it would be better to group the\nsearch results into clusters so that a user can easily navigate\ninto a particular interesting group. For example, the results\nin the first page returned from Google for the ambiguous\nquery jaguar (as of Dec. 2nd, 2006) contain at least four\ndifferent senses of jaguar (i.e., car, animal, software, and a\nsports team); even for a more refined query such as jaguar\nteam picture, the results are still quite ambiguous, \nincluding at least four different jaguar teams - a wrestling team, a\njaguar car team, Southwestern College Jaguar softball team,\nand Jacksonville Jaguar football team. Moreover, if a user\nwants to find a place to download a jaguar software, a query\nsuch as download jaguar is also not very effective as the\ndominating results are about downloading jaguar brochure,\njaguar wallpaper, and jaguar DVD. In these examples, a\nclustering view of the search results would be much more\nuseful to a user than a simple ranked list. Clustering is also\nuseful when the search results are poor, in which case, a user\nwould otherwise have to go through a long list sequentially\nto reach the very first relevant document.\nAs a primary alternative strategy for presenting search\nresults, clustering search results has been studied relatively\nextensively [9, 15, 26, 27, 28]. The general idea in virtually\nall the existing work is to perform clustering on a set of \ntopranked search results to partition the results into natural\nclusters, which often correspond to different subtopics of the\ngeneral query topic. A label will be generated to indicate\nwhat each cluster is about. A user can then view the labels\nto decide which cluster to look into. Such a strategy has\nbeen shown to be more useful than the simple ranked list\npresentation in several studies [8, 9, 26].\nHowever, this clustering strategy has two deficiencies which\nmake it not always work well:\nFirst, the clusters discovered in this way do not necessarily\ncorrespond to the interesting aspects of a topic from the\nuser\"s perspective. For example, users are often interested\nin finding either phone codes or zip codes when entering\nthe query area codes. But the clusters discovered by the\ncurrent methods may partition the results into local codes\nand international codes. Such clusters would not be very\nuseful for users; even the best cluster would still have a low\nprecision.\nSecond, the cluster labels generated are not informative\nenough to allow a user to identify the right cluster. There\nare two reasons for this problem: (1) The clusters are not\ncorresponding to a user\"s interests, so their labels would not\nbe very meaningful or useful. (2) Even if a cluster really\ncorresponds to an interesting aspect of the topic, the label\nmay not be informative because it is usually generated based\non the contents in a cluster, and it is possible that the user is\nnot very familiar with some of the terms. For example, the\nambiguous query jaguar may mean an animal or a car. A\ncluster may be labeled as panthera onca. Although this\nis an accurate label for a cluster with the animal sense of\njaguar, if a user is not familiar with the phrase, the label\nwould not be helpful.\nIn this paper, we propose a different strategy for \npartitioning search results, which addresses these two deficiencies\nthrough imposing a user-oriented partitioning of the search\nresults. That is, we try to figure out what aspects of a search\ntopic are likely interesting to a user and organize the results\naccordingly. Specifically, we propose to do the following:\nFirst, we will learn interesting aspects of similar topics\nfrom search logs and organize search results based on these\ninteresting aspects. For example, if the current query has\noccurred many times in the search logs, we can look at what\nkinds of pages viewed by the users in the results and what\nkind of words are used together with such a query. In case\nwhen the query is ambiguous such as jaguar we can expect\nto see some clear clusters corresponding different senses of\njaguar. More importantly, even if a word is not ambiguous\n(e.g., car), we may still discover interesting aspects such\nas car rental and car pricing (which happened to be\nthe two primary aspects discovered in our search log data).\nSuch aspects can be very useful for organizing future search\nresults about car. Note that in the case of car, \nclusters generated using regular clustering may not necessarily\nreflect such interesting aspects about car from a user\"s\nperspective, even though the generated clusters are \ncoherent and meaningful in other ways.\nSecond, we will generate more meaningful cluster labels\nusing past query words entered by users. Assuming that the\npast search logs can help us learn what specific aspects are\ninteresting to users given the current query topic, we could\nalso expect that those query words entered by users in the\npast that are associated with the current query can provide\nmeaningful descriptions of the distinct aspects. Thus they\ncan be better labels than those extracted from the ordinary\ncontents of search results.\nTo implement the ideas presented above, we rely on search\nengine logs and build a history collection containing the past\nqueries and the associated clickthroughs. Given a new query,\nwe find its related past queries from the history collection\nand learn aspects through applying the star clustering \nalgorithm [2] to these past queries and clickthroughs. We\ncan then organize the search results into these aspects using\ncategorization techniques and label each aspect by the most\nrepresentative past query in the query cluster.\nWe evaluate our method for result organization using logs\nof a commercial search engine. We compare our method\nwith the default search engine ranking and the traditional\nclustering of search results. The results show that our method\nis effective for improving search utility and the labels \ngenerated using past query words are more readable than those\ngenerated using traditional clustering approaches.\nThe rest of the paper is organized as follows. We first\nreview the related work in Section 2. In Section 3, we \ndescribe search engine log data and our procedure of building\na history collection. In Section 4, we present our approach\nin details. We describe the data set in Section 5 and the\nexperimental results are discussed in Section 6. Finally, we\nconclude our paper and discuss future work in Section 7.\n2. RELATED WORK\nOur work is closely related to the study of clustering\nsearch results. In [9, 15], the authors used Scatter/Gather\nalgorithm to cluster the top documents returned from a \ntraditional information retrieval system. Their results validate\nthe cluster hypothesis [20] that relevant documents tend to\nform clusters. The system Grouper was described in [26,\n27]. In these papers, the authors proposed to cluster the\nresults of a real search engine based on the snippets or the\ncontents of returned documents. Several clustering \nalgorithms are compared and the Suffix Tree Clustering \nalgorithm (STC) was shown to be the most effective one. They\nalso showed that using snippets is as effective as using whole\ndocuments. However, an important challenge of document\nclustering is to generate meaningful labels for clusters. To\novercome this difficulty, in [28], supervised learning \nalgorithms were studied to extract meaningful phrases from the\nsearch result snippets and these phrases were then used to\ngroup search results. In [13], the authors proposed to use\na monothetic clustering algorithm, in which a document is\nassigned to a cluster based on a single feature, to organize\nsearch results, and the single feature is used to label the\ncorresponding cluster. Clustering search results has also \nattracted a lot of attention in industry and commercial Web\nservices such as Vivisimo [22]. However, in all these works,\nthe clusters are generated solely based on the search results.\nThus the obtained clusters do not necessarily reflect users\"\npreferences and the generated labels may not be informative\nfrom a user\"s viewpoint.\nMethods of organizing search results based on text \ncategorization are studied in [6, 8]. In this work, a text \nclassifier is trained using a Web directory and search results are\nthen classified into the predefined categories. The authors\ndesigned and studied different category interfaces and they\nfound that category interfaces are more effective than list\ninterfaces. However predefined categories are often too \ngeneral to reflect the finer granularity aspects of a query.\nSearch logs have been exploited for several different \npurposes in the past. For example, clustering search queries to\nfind those Frequent Asked Questions (FAQ) is studied in [24,\n4]. Recently, search logs have been used for suggesting query\nsubstitutes [12], personalized search [19], Web site design [3],\nLatent Semantic Analysis [23], and learning retrieval \nranking functions [16, 10, 1]. In our work, we explore past query\nhistory in order to better organize the search results for \nfuture queries. We use the star clustering algorithm [2], which\nis a graph partition based approach, to learn interesting \naspects from search logs given a new query. Thus past queries\nare clustered in a query specific manner and this is another\ndifference from previous works such as [24, 4] in which all\nqueries in logs are clustered in an o\u00ef\u00ac\u201eine batch manner.\n3. SEARCH ENGINE LOGS\nSearch engine logs record the activities of Web users, which\nreflect the actual users\" needs or interests when conducting\nID Query URL Time\n1 win zip http://www.winzip.com xxxx\n1 win zip http://www.swinzip.com/winzip xxxx\n2 time zones http://www.timeanddate.com xxxx\n... ... ... ...\nTable 1: Sample entries of search engine logs. \nDifferent ID\"s mean different sessions.\nWeb search. They generally have the following \ninformation: text queries that users submitted, the URLs that they\nclicked after submitting the queries, and the time when they\nclicked. Search engine logs are separated by sessions. A\nsession includes a single query and all the URLs that a user\nclicked after issuing the query [24]. A small sample of search\nlog data is shown in Table 1.\nOur idea of using search engine logs is to treat these logs\nas past history, learn users\" interests using this history data\nautomatically, and represent their interests by \nrepresentative queries. For example, in the search logs, a lot of queries\nare related to car and this reflects that a large number of\nusers are interested in information about car. Different\nusers are probably interested in different aspects of car.\nSome are looking for renting a car, thus may submit a query\nlike car rental; some are more interested in buying a used\ncar, and may submit a query like used car; and others may\ncare more about buying a car accessory, so they may use a\nquery like car audio. By mining all the queries which are\nrelated to the concept of car, we can learn the aspects\nthat are likely interesting from a user\"s perspective. As an\nexample, the following is some aspects about car learned\nfrom our search log data (see Section 5).\n1. car rental, hertz car rental, enterprise car\nrental, ...\n2. car pricing, used car, car values, ...\n3. car accidents, car crash, car wrecks, ...\n4. car audio, car stereo, car speaker, ...\nIn order to learn aspects from search engine logs, we \npreprocess the raw logs to build a history data collection. As\nshown above, search engine logs consist of sessions. Each\nsession contains the information of the text query and the\nclicked Web page URLs, together with the time that the\nuser did the clicks. However, this information is limited\nsince URLs alone are not informative enough to tell the \nintended meaning of a submitted query accurately. To gather\nrich information, we enrich each URL with additional text\ncontent. Specifically, given the query in a session, we obtain\nits top-ranked results using the search engine from which we\nobtained our log data, and extract the snippets of the URLs\nthat are clicked on according to the log information in the\ncorresponding session. All the titles, snippets, and URLs of\nthe clicked Web pages of that query are used to represent\nthe session.\nDifferent sessions may contain the same queries. Thus\nthe number of sessions could be quite huge and the \ninformation in the sessions with the same queries could be \nredundant. In order to improve the scalability and reduce data\nsparseness, we aggregate all the sessions which contain \nexactly the same queries together. That is, for each unique\nquery, we build a pseudo-document which consists of all\nthe descriptions of its clicks in all the sessions aggregated.\nThe keywords contained in the queries themselves can be\nregarded as brief summaries of the pseudo-documents. All\nthese pseudo-documents form our history data collection,\nwhich is used to learn interesting aspects in the following\nsection.\n4. OUR APPROACH\nOur approach is to organize search results by aspects\nlearned from search engine logs. Given an input query, the\ngeneral procedure of our approach is:\n1. Get its related information from search engine logs.\nAll the information forms a working set.\n2. Learn aspects from the information in the working set.\nThese aspects correspond to users\" interests given the\ninput query. Each aspect is labeled with a \nrepresentative query.\n3. Categorize and organize the search results of the input\nquery according to the aspects learned above.\nWe now give a detailed presentation of each step.\n4.1 Finding Related Past Queries\nGiven a query q, a search engine will return a ranked list\nof Web pages. To know what the users are really interested\nin given this query, we first retrieve its past similar queries\nin our preprocessed history data collection.\nFormally, assume we have N pseudo-documents in our\nhistory data set: H = {Q1, Q2, ..., QN }. Each Qi \ncorresponds to a unique query and is enriched with clickthrough\ninformation as discussed in Section 3. To find q\"s related\nqueries in H, a natural way is to use a text retrieval \nalgorithm. Here we use the OKAPI method [17], one of the\nstate-of-the-art retrieval methods. Specifically, we use the\nfollowing formula to calculate the similarity between query\nq and pseudo-document Qi:\n\u00c2\u00a0\nw\u00e2\u02c6\u02c6q \u00c2\u00a1 Qi\nc(w, q) \u00c3\u2014 IDF(w) \u00c3\u2014\n(k1 + 1) \u00c3\u2014 c(w, Qi)\nk1((1 \u00e2\u02c6\u2019 b) + b |Qi|\navdl\n) + c(w, Qi)\nwhere k1 and b are OKAPI parameters set empirically, c(w, Qi)\nand c(w, q) are the count of word w in Qi and q respectively,\nIDF(w) is the inverse document frequency of word w, and\navdl is the average document length in our history \ncollection.\nBased on the similarity scores, we rank all the documents\nin H. The top ranked documents provide us a working set to\nlearn the aspects that users are usually interested in. Each\ndocument in H corresponds to a past query, and thus the\ntop ranked documents correspond to q\"s related past queries.\n4.2 Learning Aspects by Clustering\nGiven a query q, we use Hq = {d1, ..., dn} to represent the\ntop ranked pseudo-documents from the history collection\nH. These pseudo-documents contain the aspects that users\nare interested in. In this subsection, we propose to use a\nclustering method to discover these aspects.\nAny clustering algorithm could be applied here. In this\npaper, we use an algorithm based on graph partition: the\nstar clustering algorithm [2]. A good property of the star\nclustering in our setting is that it can suggest a good label\nfor each cluster naturally. We describe the star clustering\nalgorithm below.\n4.2.1 Star Clustering\nGiven Hq, star clustering starts with constructing a \npairwise similarity graph on this collection based on the vector\nspace model in information retrieval [18]. Then the clusters\nare formed by dense subgraphs that are star-shaped. These\nclusters form a cover of the similarity graph. Formally, for\neach of the n pseudo-documents {d1, ..., dn} in the collection\nHq, we compute a TF-IDF vector. Then, for each pair of\ndocuments di and dj (i = j), their similarity is computed\nas the cosine score of their corresponding vectors vi and vj ,\nthat is\nsim(di, dj ) = cos(vi, vj) =\nvi \u00c2\u00b7 vj\n|vi| \u00c2\u00b7 |vj |\n.\nA similarity graph G\u00cf\u0192 can then be constructed as follows\nusing a similarity threshold parameter \u00cf\u0192. Each document\ndi is a vertex of G\u00cf\u0192. If sim(di, dj) > \u00cf\u0192, there would be an\nedge connecting the corresponding two vertices. After the\nsimilarity graph G\u00cf\u0192 is built, the star clustering algorithm\nclusters the documents using a greedy algorithm as follows:\n1. Associate every vertex in G\u00cf\u0192 with a flag, initialized as\nunmarked.\n2. From those unmarked vertices, find the one which has\nthe highest degree and let it be u.\n3. Mark the flag of u as center.\n4. Form a cluster C containing u and all its neighbors\nthat are not marked as center. Mark all the selected\nneighbors as satellites.\n5. Repeat from step 2 until all the vertices in G\u00cf\u0192 are\nmarked.\nEach cluster is star-shaped, which consists a single center\nand several satellites. There is only one parameter \u00cf\u0192 in\nthe star clustering algorithm. A big \u00cf\u0192 enforces that the\nconnected documents have high similarities, and thus the\nclusters tend to be small. On the other hand, a small \u00cf\u0192 will\nmake the clusters big and less coherent. We will study the\nimpact of this parameter in our experiments.\nA good feature of the star clustering algorithm is that it\noutputs a center for each cluster. In the past query \ncollection Hq, each document corresponds to a query. This center\nquery can be regarded as the most representative one for\nthe whole cluster, and thus provides a label for the cluster\nnaturally. All the clusters obtained are related to the input\nquery q from different perspectives, and they represent the\npossible aspects of interests about query q of users.\n4.3 Categorizing Search Results\nIn order to organize the search results according to users\"\ninterests, we use the learned aspects from the related past\nqueries to categorize the search results. Given the top m\nWeb pages returned by a search engine for q: {s1, ..., sm},\nwe group them into different aspects using a categorization\nalgorithm.\nIn principle, any categorization algorithm can be used\nhere. Here we use a simple centroid-based method for \ncategorization. Naturally, more sophisticated methods such as\nSVM [21] may be expected to achieve even better \nperformance.\nBased on the pseudo-documents in each discovered aspect\nCi, we build a centroid prototype pi by taking the average\nof all the vectors of the documents in Ci:\npi =\n1\n|Ci|\n\u00c2\u00a0\nl\u00e2\u02c6\u02c6Ci\nvl.\nAll these pi\"s are used to categorize the search results. \nSpecifically, for any search result sj, we build a TF-IDF vector.\nThe centroid-based method computes the cosine similarity\nbetween the vector representation of sj and each centroid\nprototype pi. We then assign sj to the aspect with which it\nhas the highest cosine similarity score.\nAll the aspects are finally ranked according to the number\nof search results they have. Within each aspect, the search\nresults are ranked according to their original search engine\nranking.\n5. DATA COLLECTION\nWe construct our data set based on the MSN search log\ndata set released by the Microsoft Live Labs in 2006 [14].\nIn total, this log data spans 31 days from 05/01/2006 to\n05/31/2006. There are 8,144,000 queries, 3,441,000 distinct\nqueries, and 4,649,000 distinct URLs in the raw data.\nTo test our algorithm, we separate the whole data set into\ntwo parts according to the time: the first 2/3 data is used\nto simulate the historical data that a search engine \naccumulated, and we use the last 1/3 to simulate future queries.\nIn the history collection, we clean the data by only \nkeeping those frequent, well-formatted, English queries (queries\nwhich only contain characters \u00e2\u20ac\u02dca\", \u00e2\u20ac\u02dcb\", ..., \u00e2\u20ac\u02dcz\", and space, and\nappear more than 5 times). After cleaning, we get 169,057\nunique queries in our history data collection totally. On\naverage, each query has 3.5 distinct clicks. We build the\npseudo-documents for all these queries as described in\nSection 3. The average length of these pseudo-documents\nis 68 words and the total data size of our history collection\nis 129MB.\nWe construct our test data from the last 1/3 data. \nAccording to the time, we separate this data into two test sets\nequally for cross-validation to set parameters. For each test\nset, we use every session as a test case. Each session \ncontains a single query and several clicks. (Note that we do not\naggregate sessions for test cases. Different test cases may\nhave the same queries but possibly different clicks.) Since it\nis infeasible to ask the original user who submitted a query\nto judge the results for the query, we follow the work [11]\nand opt to use the clicks associated with the query in a\nsession to approximate relevant documents. Using clicks as\njudgments, we can then compare different algorithms for \norganizing search results to see how well these algorithms can\nhelp users reach the clicked URLs.\nOrganizing search results into different aspects is expected\nto help informational queries. It thus makes sense to focus\non the informational queries in our evaluation. For each\ntest case, i.e., each session, we count the number of different\nclicks and filter out those test cases with fewer than 4 clicks\nunder the assumption that a query with more clicks is more\nlikely to be an informational query. Since we want to test\nwhether our algorithm can learn from the past queries, we\nalso filter out those test cases whose queries can not retrieve\nat least 100 pseudo-documents from our history collection.\nFinally, we obtain 172 and 177 test cases in the first and\nsecond test sets respectively. On average, we have 6.23 and\n5.89 clicks for each test case in the two test sets respectively.\n6. EXPERIMENTS\nIn the section, we describe our experiments on the search\nresult organization based past search engine logs.\n6.1 Experimental Design\nWe use two baseline methods to evaluate the proposed\nmethod for organizing search results. For each test case,\nthe first method is the default ranked list from a search\nengine (baseline). The second method is to organize the\nsearch results by clustering them (cluster-based). For fair\ncomparison, we use the same clustering algorithm as our \nlogbased method (i.e., star clustering). That is, we treat each\nsearch result as a document, construct the similarity graph,\nand find the star-shaped clusters. We compare our method\n(log-based) with the two baseline methods in the following\nexperiments. For both cluster-based and log-based methods,\nthe search results within each cluster is ranked based on their\noriginal ranking given by the search engine.\nTo compare different result organization methods, we adopt\na similar method as in the paper [9]. That is, we compare the\nquality (e.g., precision) of the best cluster, which is defined\nas the one with the largest number of relevant documents.\nOrganizing search results into clusters is to help users \nnavigate into relevant documents quickly. The above metric is to\nsimulate a scenario when users always choose the right \ncluster and look into it. Specifically, we download and organize\nthe top 100 search results into aspects for each test case. We\nuse Precision at 5 documents (P@5) in the best cluster as\nthe primary measure to compare different methods. P@5 is\na very meaningful measure as it tells us the perceived \nprecision when the user opens a cluster and looks at the first 5\ndocuments. We also use Mean Reciprocal Rank (MRR) as\nanother metric. MRR is calculated as\nMRR =\n1\n|T|\n\u00c2\u00a0\nq\u00e2\u02c6\u02c6T\n1\nrq\nwhere T is a set of test queries, rq is the rank of the first\nrelevant document for q.\nTo give a fair comparison across different organization \nalgorithms, we force both cluster-based and log-based \nmethods to output the same number of aspects and force each\nsearch result to be in one and only one aspect. The \nnumber of aspects is fixed at 10 in all the following experiments.\nThe star clustering algorithm can output different number\nof clusters for different input. To constrain the number of\nclusters to 10, we order all the clusters by their sizes, select\nthe top 10 as aspect candidates. We then re-assign each\nsearch result to one of these selected 10 aspects that has\nthe highest similarity score with the corresponding aspect\ncentroid. In our experiments, we observe that the sizes of\nthe best clusters are all larger than 5, and this ensures that\nP@5 is a meaningful metric.\n6.2 Experimental Results\nOur main hypothesis is that organizing search results based\non the users\" interests learned from a search log data set is\nmore beneficial than to organize results using a simple list\nor cluster search results. In the following, we test our \nhypothesis from two perspectives - organization and labeling.\nMethod Test set 1 Test set 2\nMMR P@5 MMR P@5\nBaseline 0.7347 0.3325 0.7393 0.3288\nCluster-based 0.7735 0.3162 0.7666 0.2994\nLog-based 0.7833 0.3534 0.7697 0.3389\nCluster/Baseline 5.28% -4.87% 3.69% -8.93%\nLog/Baseline 6.62% 6.31% 4.10% 3.09%\nLog/Cluster 1.27% 11.76% 0.40% 13.20%\nTable 2: Comparison of different methods by MMR\nand P@5. We also show the percentage of relative\nimprovement in the lower part.\nComparison Test set 1 Test set 2\nImpr./Decr. Impr./Decr.\nCluster/Baseline 53/55 50/64\nLog/Baseline 55/44 60/45\nLog/Cluster 68/47 69/44\nTable 3: Pairwise comparison w.r.t the number of\ntest cases whose P@5\"s are improved versus \ndecreased w.r.t the baseline.\n6.2.1 Overall performance\nWe compare three methods, basic search engine \nranking (baseline), traditional clustering based method \n(clusterbased), and our log based method (log-based), in Table 2 \nusing MRR and P@5. We optimize the parameter \u00cf\u0192\"s for each\ncollection individually based on P@5 values. This shows the\nbest performance that each method can achieve. In this \ntable, we can see that in both test collections, our method\nis better than both the baseline and the cluster-based\nmethods. For example, in the first test collection, the \nbaseline method of MMR is 0.734, the cluster-based method is\n0.773 and our method is 0.783. We achieve higher \naccuracy than both cluster-based method (1.27% improvement)\nand the baseline method (6.62% improvement). The P@5\nvalues are 0.332 for the baseline, 0.316 for cluster-based\nmethod, but 0.353 for our method. Our method improves\nover the baseline by 6.31%, while the cluster-based method\neven decreases the accuracy. This is because cluster-based\nmethod organizes the search results only based on the \ncontents. Thus it could organize the results differently from\nusers\" preferences. This confirms our hypothesis of the bias\nof the cluster-based method. Comparing our method with\nthe cluster-based method, we achieve significant \nimprovement on both test collections. The p-values of the \nsignificance tests based on P@5 on both collections are 0.01 and\n0.02 respectively. This shows that our log-based method is\neffective to learn users\" preferences from the past query \nhistory, and thus it can organize the search results in a more\nuseful way to users.\nWe showed the optimal results above. To test the \nsensitivity of the parameter \u00cf\u0192 of our log-based method, we use\none of the test sets to tune the parameter to be optimal\nand then use the tuned parameter on the other set. We\ncompare this result (log tuned outside) with the optimal \nresults of both cluster-based (cluster optimized) and log-based\nmethods (log optimized) in Figure 1. We can see that, as\nexpected, the performance using the parameter tuned on a\nseparate set is worse than the optimal performance. \nHowever, our method still performs much better than the optimal\nresults of cluster-based method on both test collections.\n0.27\n0.28\n0.29\n0.3\n0.31\n0.32\n0.33\n0.34\n0.35\n0.36\nTest set 1 Test set 2\nP@5\ncluster optimized log optimized log tuned outside\nFigure 1: Results using parameters tuned from the\nother test collection. We compare it with the \noptimal performance of the cluster-based and our \nlogbased methods.\n0\n10\n20\n30\n40\n50\n60\n1 2 3 4\nBin number\n#Queries\nImproved Decreased\nFigure 2: The correlation between performance\nchange and result diversity.\nIn Table 3, we show pairwise comparisons of the three\nmethods in terms of the numbers of test cases for which\nP@5 is increased versus decreased. We can see that our\nmethod improves more test cases compared with the other\ntwo methods. In the next section, we show more detailed\nanalysis to see what types of test cases can be improved by\nour method.\n6.2.2 Detailed Analysis\nTo better understand the cases where our log-based method\ncan improve the accuracy, we test two properties: result \ndiversity and query difficulty. All the analysis below is based\non test set 1.\nDiversity Analysis: Intuitively, organizing search \nresults into different aspects is more beneficial to those queries\nwhose results are more diverse, as for such queries, the \nresults tend to form two or more big clusters. In order to\ntest the hypothesis that log-based method help more those\nqueries with diverse results, we compute the size ratios of\nthe biggest and second biggest clusters in our log-based \nresults and use this ratio as an indicator of diversity. If the\nratio is small, it means that the first two clusters have a\nsmall difference thus the results are more diverse. In this\ncase, we would expect our method to help more. The \nresults are shown in Figure 2. In this figure, we partition the\nratios into 4 bins. The 4 bins correspond to the ratio ranges\n[1, 2), [2, 3), [3, 4), and [4, +\u00e2\u02c6\u017e) respectively. ([i, j) means\nthat i \u00e2\u2030\u00a4 ratio < j.) In each bin, we count the numbers of\ntest cases whose P@5\"s are improved versus decreased with\nrespect to the ranking baseline, and plot the numbers in this\nfigure. We can observe that when the ratio is smaller, the\nlog-based method can improve more test cases. But when\n0\n5\n10\n15\n20\n25\n30\n1 2 3 4\nBin number\n#Queries\nImproved Decreased\nFigure 3: The correlation between performance\nchange and query difficulty.\nthe ratio is large, the log-based method can not improve\nover the baseline. For example, in bin 1, 48 test cases are\nimproved and 34 are decreased. But in bin 4, all the 4 test\ncases are decreased. This confirms our hypothesis that our\nmethod can help more if the query has more diverse results.\nThis also suggests that we should turn off the option of\nre-organizing search results if the results are not very diverse\n(e.g., as indicated by the cluster size ratio).\nDifficulty Analysis: Difficult queries have been studied\nin recent years [7, 25, 5]. Here we analyze the effectiveness\nof our method in helping difficult queries. We quantify the\nquery difficulty by the Mean Average Precision (MAP) of\nthe original search engine ranking for each test case. We\nthen order the 172 test cases in test set 1 in an increasing\norder of MAP values. We partition the test cases into 4 bins\nwith each having a roughly equal number of test cases. A\nsmall MAP means that the utility of the original ranking is\nlow. Bin 1 contains those test cases with the lowest MAP\"s\nand bin 4 contains those test cases with the highest MAP\"s.\nFor each bin, we compute the numbers of test cases whose\nP@5\"s are improved versus decreased. Figure 3 shows the\nresults. Clearly, in bin 1, most of the test cases are improved\n(24 vs 3), while in bin 4, log-based method may decrease\nthe performance (3 vs 20). This shows that our method\nis more beneficial to difficult queries, which is as expected\nsince clustering search results is intended to help difficult\nqueries. This also shows that our method does not really\nhelp easy queries, thus we should turn off our organization\noption for easy queries.\n6.2.3 Parameter Setting\nWe examine parameter sensitivity in this section. For the\nstar clustering algorithm, we study the similarity threshold\nparameter \u00cf\u0192. For the OKAPI retrieval function, we study\nthe parameters k1 and b. We also study the impact of the\nnumber of past queries retrieved in our log-based method.\nFigure 4 shows the impact of the parameter \u00cf\u0192 for both\ncluster-based and log-based methods on both test sets. We\nvary \u00cf\u0192 from 0.05 to 0.3 with step 0.05. Figure 4 shows that\nthe performance is not very sensitive to the parameter \u00cf\u0192. We\ncan always obtain the best result in range 0.1 \u00e2\u2030\u00a4 \u00cf\u0192 \u00e2\u2030\u00a4 0.25.\nIn Table 4, we show the impact of OKAPI parameters.\nWe vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to\n1 with step 0.2. From this table, it is clear that P@5 is\nalso not very sensitive to the parameter setting. Most of the\nvalues are larger than 0.35. The default values k1 = 1.2 and\nb = 0.8 give approximately optimal results.\nWe further study the impact of the amount of history\n0.2\n0.25\n0.3\n0.35\n0.4\n0.05 0.1 0.15 0.2 0.25 0.3\nP@5\nsimilarity threhold: sigma\ncluster-based 1\nlog-based 1\ncluster-based 2\nlog-based 2\nFigure 4: The impact of similarity threshold \u00cf\u0192 on\nboth cluster-based and log-based methods. We show\nthe result on both test collections.\nb\n0.0 0.2 0.4 0.6 0.8 1.0\n1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453\n1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546\nk1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465\n1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476\n1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476\n2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546\nTable 4: Impact of OKAPI parameters k1 and b.\ninformation to learn from by varying the number of past\nqueries to be retrieved for learning aspects. The results on\nboth test collections are shown in Figure 5. We can see\nthat the performance gradually increases as we enlarge the\nnumber of past queries retrieved. Thus our method could\npotentially learn more as we accumulate more history. More\nimportantly, as time goes, more and more queries will have\nsufficient history, so we can improve more and more queries.\n6.2.4 An Illustrative Example\nWe use the query area codes to show the difference in\nthe results of the log-based method and the cluster-based\nmethod. This query may mean phone codes or zip codes.\nTable 5 shows the representative keywords extracted from\nthe three biggest clusters of both methods. In the \nclusterbased method, the results are partitioned based on locations:\nlocal or international. In the log-based method, the results\nare disambiguated into two senses: phone codes or zip\ncodes. While both are reasonable partitions, our \nevaluation indicates that most users using such a query are often\ninterested in either phone codes or zip codes. since the\nP@5 values of cluster-based and log-based methods are 0.2\nand 0.6, respectively. Therefore our log-based method is\nmore effective in helping users to navigate into their desired\nresults.\nCluster-based method Log-based method\ncity, state telephone, city, international\nlocal, area phone, dialing\ninternational zip, postal\nTable 5: An example showing the difference between\nthe cluster-based method and our log-based method\n0.16\n0.18\n0.2\n0.22\n0.24\n0.26\n0.28\n0.3\n1501201008050403020\nP@5\n#queries retrieved\nTest set 1\nTest set 2\nFigure 5: The impact of the number of past queries\nretrieved.\n6.2.5 Labeling Comparison\nWe now compare the labels between the cluster-based\nmethod and log-based method. The cluster-based method\nhas to rely on the keywords extracted from the snippets to\nconstruct the label for each cluster. Our log-based method\ncan avoid this difficulty by taking advantage of queries. \nSpecifically, for the cluster-based method, we count the frequency\nof a keyword appearing in a cluster and use the most \nfrequent keywords as the cluster label. For log-based method,\nwe use the center of each star cluster as the label for the\ncorresponding cluster.\nIn general, it is not easy to quantify the readability of a\ncluster label automatically. We use examples to show the\ndifference between the cluster-based and the log-based \nmethods. In Table 6, we list the labels of the top 5 clusters for\ntwo examples jaguar and apple. For the cluster-based\nmethod, we separate keywords by commas since they do not\nform a phrase. From this table, we can see that our log-based\nmethod gives more readable labels because it generates \nlabels based on users\" queries. This is another advantage of\nour way of organizing search results over the clustering \napproach.\nLabel comparison for query jaguar\nLog-based method Cluster-based method\n1. jaguar animal 1. jaguar, auto, accessories\n2. jaguar auto accessories 2. jaguar, type, prices\n3. jaguar cats 3. jaguar, panthera, cats\n4. jaguar repair 4. jaguar, services, boston\n5. jaguar animal pictures 5. jaguar, collection, apparel\nLabel comparison for query apple\nLog-based method Cluster-based method\n1. apple computer 1. apple, support, product\n2. apple ipod 2. apple, site, computer\n3. apple crisp recipe 3. apple, world, visit\n4. fresh apple cake 4. apple, ipod, amazon\n5. apple laptop 5. apple, products, news\nTable 6: Cluster label comparison.\n7. CONCLUSIONS AND FUTURE WORK\nIn this paper, we studied the problem of organizing search\nresults in a user-oriented manner. To attain this goal, we\nrely on search engine logs to learn interesting aspects from\nusers\" perspective. Given a query, we retrieve its related\nqueries from past query history, learn the aspects by \nclustering the past queries and the associated clickthrough \ninformation, and categorize the search results into the aspects\nlearned. We compared our log-based method with the \ntraditional cluster-based method and the baseline of search \nengine ranking. The experiments show that our log-based\nmethod can consistently outperform cluster-based method\nand improve over the ranking baseline, especially when the\nqueries are difficult or the search results are diverse. \nFurthermore, our log-based method can generate more \nmeaningful aspect labels than the cluster labels generated based\non search results when we cluster search results.\nThere are several interesting directions for further \nextending our work: First, although our experiment results have\nclearly shown promise of the idea of learning from search\nlogs to organize search results, the methods we have \nexperimented with are relatively simple. It would be interesting\nto explore other potentially more effective methods. In \nparticular, we hope to develop probabilistic models for learning\naspects and organizing results simultaneously. Second, with\nthe proposed way of organizing search results, we can \nexpect to obtain informative feedback information from a user\n(e.g., the aspect chosen by a user to view). It would thus\nbe interesting to study how to further improve the \norganization of the results based on such feedback information.\nFinally, we can combine a general search log with any \npersonal search log to customize and optimize the organization\nof search results for each individual user.\n8. ACKNOWLEDGMENTS\nWe thank the anonymous reviewers for their valuable \ncomments. This work is in part supported by a Microsoft Live\nLabs Research Grant, a Google Research Grant, and an NSF\nCAREER grant IIS-0347933.\n9. REFERENCES\n[1] E. Agichtein, E. Brill, and S. T. Dumais. Improving\nweb search ranking by incorporating user behavior\ninformation. In SIGIR, pages 19-26, 2006.\n[2] J. A. Aslam, E. Pelekov, and D. Rus. The star\nclustering algorithm for static and dynamic\ninformation organization. Journal of Graph\nAlgorithms and Applications, 8(1):95-129, 2004.\n[3] R. A. Baeza-Yates. Applications of web query mining.\nIn ECIR, pages 7-22, 2005.\n[4] D. Beeferman and A. L. Berger. Agglomerative\nclustering of a search engine query log. In KDD, pages\n407-416, 2000.\n[5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.\nWhat makes a query difficult? In SIGIR, pages\n390-397, 2006.\n[6] H. Chen and S. T. Dumais. Bringing order to the web:\nautomatically categorizing search results. In CHI,\npages 145-152, 2000.\n[7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.\nPredicting query performance. In Proceedings of ACM\nSIGIR 2002, pages 299-306, 2002.\n[8] S. T. Dumais, E. Cutrell, and H. Chen. Optimizing\nsearch by showing results in context. In CHI, pages\n277-284, 2001.\n[9] M. A. Hearst and J. O. Pedersen. Reexamining the\ncluster hypothesis: Scatter/gather on retrieval results.\nIn SIGIR, pages 76-84, 1996.\n[10] T. Joachims. Optimizing search engines using\nclickthrough data. In KDD, pages 133-142, 2002.\n[11] T. Joachims. Evaluating Retrieval Performance Using\nClickthrough Data., pages 79-96. Physica/Springer\nVerlag, 2003. in J. Franke and G. Nakhaeizadeh and I.\nRenz, Text Mining.\n[12] R. Jones, B. Rey, O. Madani, and W. Greiner.\nGenerating query substitutions. In WWW, pages\n387-396, 2006.\n[13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and\nR. Krishnapuram. A hierarchical monothetic\ndocument clustering algorithm for summarization and\nbrowsing search results. In WWW, pages 658-665,\n2004.\n[14] Microsoft Live Labs. Accelerating search in academic\nresearch, 2006.\nhttp://research.microsoft.com/ur/us/fundingopps/RFPs/\nSearch 2006 RFP.aspx.\n[15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.\nScatter/gather browsing communicates the topic\nstructure of a very large text collection. In CHI, pages\n213-220, 1996.\n[16] F. Radlinski and T. Joachims. Query chains: learning\nto rank from implicit feedback. In KDD, pages\n239-248, 2005.\n[17] S. E. Robertson and S. Walker. Some simple effective\napproximations to the 2-poisson model for\nprobabilistic weighted retrieval. In SIGIR, pages\n232-241, 1994.\n[18] G. Salton, A. Wong, and C. S. Yang. A vector space\nmodel for automatic indexing. Commun. ACM,\n18(11):613-620, 1975.\n[19] X. Shen, B. Tan, and C. Zhai. Context-sensitive\ninformation retrieval using implicit feedback. In\nSIGIR, pages 43-50, 2005.\n[20] C. J. van Rijsbergen. Information Retrieval, second\nedition. Butterworths, London, 1979.\n[21] V. N. Vapnik. The Nature of Statistical Learning\nTheory. Springer-Verlag, Berlin, 1995.\n[22] Vivisimo. http://vivisimo.com/.\n[23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai. Latent\nsemantic analysis for multiple-type interrelated data\nobjects. In SIGIR, pages 236-243, 2006.\n[24] J.-R. Wen, J.-Y. Nie, and H. Zhang. Clustering user\nqueries of a search engine. In WWW, pages 162-168,\n2001.\n[25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.\nLearning to estimate query difficulty: including\napplications to missing content detection and\ndistributed information retrieval. In SIGIR, pages\n512-519, 2005.\n[26] O. Zamir and O. Etzioni. Web document clustering: A\nfeasibility demonstration. In SIGIR, pages 46-54,\n1998.\n[27] O. Zamir and O. Etzioni. Grouper: A dynamic\nclustering interface to web search results. Computer\nNetworks, 31(11-16):1361-1374, 1999.\n[28] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma.\nLearning to cluster web search results. In SIGIR,\npages 210-217, 2004.\n": ["retrieval model", "ranking function", "ambiguity", "clustering view", "meaningful cluster label", "history collection", "past query", "clickthrough", "star clustering algorithm", "suffix tree clustering algorithm", "search result snippet", "monothetic clustering algorithm", "pseudo-document", "pairwise similarity graph", "similarity threshold parameter", "centroid-based method", "cosine similarity", "centroid prototype", "reciprocal rank", "log-based method", "mean average precision", "search result organization", "search engine log", "interest aspect", ""]}