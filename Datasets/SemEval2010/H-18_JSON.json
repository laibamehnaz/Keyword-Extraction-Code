{"Topic Segmentation with Shared Topic Detection and\nAlignment of Multiple Documents\nBingjun Sun*, Prasenjit Mitra*\u00e2\u20ac\u00a0\n, Hongyuan Zha\u00e2\u20ac\u00a1\n, C. Lee Giles*\u00e2\u20ac\u00a0\n, John Yen*\u00e2\u20ac\u00a0\n*Department of Computer Science and Engineering\n\u00e2\u20ac\u00a0\nCollege of Information Sciences and Technology\nThe Pennsylvania State University\nUniversity Park, PA 16802\n\u00e2\u20ac\u00a1\nCollege of Computing\nThe Georgia Institute of Technology\nAtlanta, GA 30332\n*bsun@cse.psu.edu, \u00e2\u20ac\u00a0\n{pmitra,giles,jyen}@ist.psu.edu, \u00e2\u20ac\u00a1\nzha@cc.gatech.edu\nABSTRACT\nTopic detection and tracking [26] and topic segmentation\n[15] play an important role in capturing the local and \nsequential information of documents. Previous work in this\narea usually focuses on single documents, although similar\nmultiple documents are available in many domains. In this\npaper, we introduce a novel unsupervised method for shared\ntopic detection and topic segmentation of multiple similar\ndocuments based on mutual information (MI) and weighted\nmutual information (WMI) that is a combination of MI and\nterm weights. The basic idea is that the optimal \nsegmentation maximizes MI(or WMI). Our approach can detect\nshared topics among documents. It can find the optimal\nboundaries in a document, and align segments among \ndocuments at the same time. It also can handle single-document\nsegmentation as a special case of the multi-document \nsegmentation and alignment. Our methods can identify and\nstrengthen cue terms that can be used for segmentation and\npartially remove stop words by using term weights based on\nentropy learned from multiple documents. Our experimental\nresults show that our algorithm works well for the tasks of\nsingle-document segmentation, shared topic detection, and\nmulti-document segmentation. Utilizing information from\nmultiple documents can tremendously improve the \nperformance of topic segmentation, and using WMI is even better\nthan using MI for the multi-document segmentation.\nCategories and Subject Descriptors\nH.3.3 [Information Storage and Retrieval]: \nInformation Search and Retrieval-Clustering; H.3.1 [Information\nStorage and Retrieval]: Content Analysis and \nIndexingLinguistic processing; I.2.7 [Artificial Intelligence]: \nNatural Language Processing-Text analysis; I.5.3 [Pattern\nRecognition]: Clustering-Algorithms;Similarity measures\nGeneral Terms\nAlgorithms, Design, Experimentation\n1. INTRODUCTION\nMany researchers have worked on topic detection and \ntracking (TDT) [26] and topic segmentation during the past decade.\nTopic segmentation intends to identify the boundaries in a\ndocument with the goal to capture the latent topical \nstructure. Topic segmentation tasks usually fall into two \ncategories [15]: text stream segmentation where topic transition\nis identified, and coherent document segmentation in which\ndocuments are split into sub-topics. The former category\nhas applications in automatic speech recognition, while the\nlatter one has more applications such as partial-text query\nof long documents in information retrieval, text summary,\nand quality measurement of multiple documents. Previous\nresearch in connection with TDT falls into the former \ncategory, targeted on topic tracking of broadcast speech data\nand newswire text, while the latter category has not been\nstudied very well.\nTraditional approaches perform topic segmentation on \ndocuments one at a time [15, 25, 6]. Most of them perform\nbadly in subtle tasks like coherent document segmentation\n[15]. Often, end-users seek documents that have the \nsimilar content. Search engines, like, Google, provide links to\nobtain similar pages. At a finer granularity, users may \nactually be looking to obtain sections of a document similar\nto a particular section that presumably discusses a topic of\nthe users interest. Thus, the extension of topic \nsegmentation from single documents to identifying similar segments\nfrom multiple similar documents with the same topic is a\nnatural and necessary direction, and multi-document topic\nsegmentation is expected to have a better performance since\nmore information is utilized.\nTraditional approaches using similarity measurement based\non term frequency generally have the same assumption that\nsimilar vocabulary tends to be in a coherent topic segment\n[15, 25, 6]. However, they usually suffer the issue of \nidentifying stop words. For example, additional document-dependent\nstop words are removed together with the generic stop words\nin [15]. There are two reasons that we do not remove stop\nwords directly. First, identifying stop words is another \nissue [12] that requires estimation in each domain. Removing\ncommon stop words may result in the loss of useful \ninformation in a specific domain. Second, even though stop words\ncan be identified, hard classification of stop words and \nnonstop words cannot represent the gradually changing amount\nof information content of each word. We employ a soft \nclassification using term weights.\nIn this paper, we view the problem of topic segmentation\nas an optimization issue using information theoretic \ntechniques to find the optimal boundaries of a document given\nthe number of text segments so as to minimize the loss of\nmutual information (MI) (or a weighted mutual information\n(WMI)) after segmentation and alignment. This is equal to\nmaximizing the MI (or WMI). The MI focuses on \nmeasuring the difference among segments whereas previous research\nfocused on finding the similarity (e.g. cosine distance) of\nsegments [15, 25, 6]. Topic alignment of multiple similar\ndocuments can be achieved by clustering sentences on the\nsame topic into the same cluster. Single-document topic\nsegmentation is just a special case of the multi-document\ntopic segmentation and alignment problem. Terms can be\nco-clustered as in [10] at the same time, given the number of\nclusters, but our experimental results show that this method\nresults in a worse segmentation (see Tables 1, 4, and 6). \nUsually, human readers can identify topic transition based on\ncue words, and can ignore stop words. Inspired by this, we\ngive each term (or term cluster) a weight based on entropy\namong different documents and different segments of \ndocuments. Not only can this approach increase the contribution\nof cue words, but it can also decrease the effect of common\nstop words, noisy word, and document-dependent stop words.\nThese words are common in a document. Many methods\nbased on sentence similarity require that these words are\nremoved before topic segmentation can be performed [15].\nOur results in Figure 3 show that term weights are useful\nfor multi-document topic segmentation and alignment.\nThe major contribution of this paper is that it introduces\na novel method for topic segmentation using MI and shows\nthat this method performs better than previously used \ncriteria. Also, we have addressed the problem of topic \nsegmentation and alignment across multiple documents, whereas\nmost existing research focused on segmentation of single\ndocuments. Multi-document segmentation and alignment\ncan utilize information from similar documents and improves\nthe performance of topic segmentation greatly. Obviously,\nour approach can handle single documents as a special case\nwhen multiple documents are unavailable. It can detect\nshared topics among documents to judge if they are multiple\ndocuments on the same topic. We also introduce the new\ncriterion of WMI based on term weights learned from \nmultiple similar documents, which can improve performance of\ntopic segmentation further. We propose an iterative greedy\nalgorithm based on dynamic programming and show that it\nworks well in practice. Some of our prior work is in [24].\nThe rest of this paper is organized as follows: In Section 2,\nwe review related work. Section 3 contains a formulation of\nthe problem of topic segmentation and alignment of multiple\ndocuments with term co-clustering, a review of the criterion\nof MI for clustering, and finally an introduction to WMI. In\nSection 4, we first propose the iterative greedy algorithm of\ntopic segmentation and alignment with term co-clustering,\nand then describe how the algorithm can be optimized by \nusFigure 1: Illustration of multi-document \nsegmentation and alignment.\ning dynamic programming. In Section 5, experiments about\nsingle-document segmentation, shared topic detection, and\nmulti-document segmentation are described, and results are\npresented and discussed to evaluate the performance of our\nalgorithm. Conclusions and some future directions of the\nresearch work are discussed in Section 6.\n2. PREVIOUS WORK\nGenerally, the existing approaches to text segmentation\nfall into two categories: supervised learning [19, 17, 23]\nand unsupervised learning [3, 27, 5, 6, 15, 25, 21]. \nSupervised learning usually has good performance, since it learns\nfunctions from labelled training sets. However, often \ngetting large training sets with manual labels on document\nsentences is prohibitively expensive, so unsupervised \napproaches are desired. Some models consider dependence \nbetween sentences and sections, such as Hidden Markov Model\n[3, 27], Maximum Entropy Markov Model [19], and \nConditional Random Fields [17], while many other approaches are\nbased on lexical cohesion or similarity of sentences [5, 6, 15,\n25, 21]. Some approaches also focus on cue words as hints\nof topic transitions [11]. While some existing methods only\nconsider information in single documents [6, 15], others \nutilize multiple documents [16, 14]. There are not many works\nin the latter category, even though the performance of \nsegmentation is expected to be better with utilization of \ninformation from multiple documents. Previous research studied\nmethods to find shared topics [16] and topic segmentation\nand summarization between just a pair of documents [14].\nText classification and clustering is a related research area\nwhich categorizes documents into groups using supervised or\nunsupervised methods. Topical classification or clustering is\nan important direction in this area, especially co-clustering\nof documents and terms, such as LSA [9], PLSA [13], and\napproaches based on distances and bipartite graph \npartitioning [28] or maximum MI [2, 10], or maximum entropy\n[1, 18]. Criteria of these approaches can be utilized in the \nissue of topic segmentation. Some of those methods have been\nextended into the area of topic segmentation, such as PLSA\n[5] and maximum entropy [7], but to our best knowledge,\nusing MI for topic segmentation has not been studied.\n3. PROBLEM FORMULATION\nOur goal is to segment documents and align the segments\nacross documents (Figure 1). Let T be the set of terms\n{t1, t2, ..., tl}, which appear in the unlabelled set of \ndocuments D = {d1, d2, ..., dm}. Let Sd be the set of sentences\nfor document d \u00e2\u02c6\u02c6 D, i.e.{s1, s2, ..., snd }. We have a 3D \nmatrix of term frequency, in which the three dimensions are\nrandom variables of D, Sd, and T. Sd actually is a random\nvector including a random variable for each d \u00e2\u02c6\u02c6 D. The\nterm frequency can be used to estimate the joint probability\ndistribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND,\nwhere T(t, d, s) is the number of t in d\"s sentence s and ND\nis the total number of terms in D. \u00cb\u2020S represents the set of\nsegments {\u00cb\u2020s1, \u00cb\u2020s2, ..., \u00cb\u2020sp} after segmentation and alignment\namong multiple documents, where the number of segments\n| \u00cb\u2020S| = p. A segment \u00cb\u2020si of document d is a sequence of \nadjacent sentences in d. Since for different documents si may\ndiscuss different sub-topics, our goal is to cluster adjacent\nsentences in each document into segments, and align similar\nsegments among documents, so that for different documents\n\u00cb\u2020si is about the same sub-topic. The goal is to find the \noptimal topic segmentation and alignment mapping\nSegd(si) : {s1, s2, ..., snd } \u00e2\u2020\u2019 {\u00cb\u2020s1, \u00cb\u2020s2, ..., \u00cb\u2020sp}\nand Alid(\u00cb\u2020si) : {\u00cb\u2020s1, \u00cb\u2020s2, ..., \u00cb\u2020sp} \u00e2\u2020\u2019 {\u00cb\u2020s1, \u00cb\u2020s2, ..., \u00cb\u2020sp}, for all d \u00e2\u02c6\u02c6\nD, where \u00cb\u2020si is ith\nsegment with the constraint that only\nadjacent sentences can be mapped to the same segment,\ni.e. for d, {si, si+1, ..., sj} \u00e2\u2020\u2019 {\u00cb\u2020sq}, where q \u00e2\u02c6\u02c6 {1, ..., p},\nwhere p is the segment number, and if i > j, then for d,\n\u00cb\u2020sq is missing. After segmentation and alignment, random\nvector Sd becomes an aligned random variable \u00cb\u2020S. Thus,\nP(D, Sd, T) becomes P(D, \u00cb\u2020S, T).\nTerm co-clustering is a technique that has been employed\n[10] to improve the accuracy of document clustering. We\nevaluate the effect of it for topic segmentation. A term t\nis mapped to exactly one term cluster. Term co-clustering\ninvolves simultaneously finding the optimal term clustering\nmapping Clu(t) : {t1, t2, ..., tl} \u00e2\u2020\u2019 {\u00cb\u2020t1, \u00cb\u2020t2, ..., \u00cb\u2020tk}, where k \u00e2\u2030\u00a4\nl, l is the total number of words in all the documents, and\nk is the number of clusters.\n4. METHODOLOGY\nWe now describe a novel algorithm which can handle \nsingledocument segmentation, shared topic detection, and \nmultidocument segmentation and alignment based on MI or WMI.\n4.1 Mutual Information\nMI I(X; Y ) is a quantity to measure the amount of \ninformation which is contained in two or more random variables\n[8, 10]. For the case of two random variables, we have\nI(X; Y ) =\nx\u00e2\u02c6\u02c6X y\u00e2\u02c6\u02c6Y\np(x, y)log\np(x, y)\np(x)p(y)\n, (1)\nObviously, when random variables X and Y are \nindependent, I(X; Y ) = 0. Thus, intuitively, the value of MI \ndepends on how random variables are dependent on each other.\nThe optimal co-clustering is the mapping Clux : X \u00e2\u2020\u2019 \u00cb\u2020X and\nCluy : Y \u00e2\u2020\u2019 \u00cb\u2020Y that minimizes the loss: I(X; Y ) \u00e2\u02c6\u2019 I( \u00cb\u2020X; \u00cb\u2020Y ),\nwhich is equal to maximizing I( \u00cb\u2020X; \u00cb\u2020Y ). This is the criterion\nof MI for clustering.\nIn the case of topic segmentation, the two random \nvariables are the term variable T and the segment variable S,\nand each sample is an occurrence of a term T = t in a\nparticular segment S = s. I(T; S) is used to measure how\ndependent T and S are. However, I(T; S) cannot be \ncomputed for documents before segmentation, since we do not\nhave a set of S due to the fact that sentences of Document d,\nsi \u00e2\u02c6\u02c6 Sd, is not aligned with other documents. Thus, instead\nof minimizing the loss of MI, we can maximize MI after topic\nsegmentation, computed as:\nI( \u00cb\u2020T; \u00cb\u2020S) =\n\u00cb\u2020t\u00e2\u02c6\u02c6 \u00cb\u2020T \u00cb\u2020s\u00e2\u02c6\u02c6 \u00cb\u2020S\np(\u00cb\u2020t, \u00cb\u2020s)log\np(\u00cb\u2020t, \u00cb\u2020s)\np(\u00cb\u2020t)p(\u00cb\u2020s)\n, (2)\nwhere p(\u00cb\u2020t, \u00cb\u2020s) are estimated by the term frequency tf of Term\nCluster \u00cb\u2020t and Segment \u00cb\u2020s in the training set D. Note that\nhere a segment \u00cb\u2020s includes sentences about the the same topic\namong all documents. The optimal solution is the mapping\nClut : T \u00e2\u2020\u2019 \u00cb\u2020T, Segd : Sd \u00e2\u2020\u2019 \u00cb\u2020S , and Alid : \u00cb\u2020S \u00e2\u2020\u2019 \u00cb\u2020S, which\nmaximizes I( \u00cb\u2020T; \u00cb\u2020S).\n4.2 Weighted Mutual Information\nIn topic segmentation and alignment of multiple \ndocuments, if P(D, \u00cb\u2020S, T) is known, based on the marginal \ndistributions P(D|T) and P( \u00cb\u2020S|T) for each term t \u00e2\u02c6\u02c6 T, we can\ncategorize terms into four types in the data set:\n\u00e2\u20ac\u00a2 Common stop words are common both along the \ndimensions of documents and segments.\n\u00e2\u20ac\u00a2 Document-dependent stop words that depends on the\npersonal writing style are common only along the \ndimension of segments for some documents.\n\u00e2\u20ac\u00a2 Cue words are the most important elements for \nsegmentation. They are common along the dimension of\ndocuments only for the same segment, and they are\nnot common along the dimensions of segments.\n\u00e2\u20ac\u00a2 Noisy words are other words which are not common\nalong both dimensions.\nEntropy based on P(D|T) and P( \u00cb\u2020S|T) can be used to \nidentify different types of terms. To reinforce the contribution\nof cue words in the MI computation, and simultaneously \nreduce the effect of the other three types of words, similar as\nthe idea of the tf-idf weight [22], we use entropies of each\nterm along the dimensions of document D and segment \u00cb\u2020S,\ni.e. ED(\u00cb\u2020t) and E\u00cb\u2020S(\u00cb\u2020t), to compute the weight. A cue word\nusually has a large value of ED(\u00cb\u2020t) but a small value of E\u00cb\u2020S(\u00cb\u2020t).\nWe introduce term weights (or term cluster weights)\nw\u00cb\u2020t = (\nED(\u00cb\u2020t)\nmax\u00cb\u2020t \u00e2\u02c6\u02c6 \u00cb\u2020T (ED(\u00cb\u2020t ))\n)a\n(1 \u00e2\u02c6\u2019\nE\u00cb\u2020S(\u00cb\u2020t)\nmax\u00cb\u2020t \u00e2\u02c6\u02c6 \u00cb\u2020T (E\u00cb\u2020S(\u00cb\u2020t ))\n)b\n, (3)\nwhere ED(\u00cb\u2020t) = d\u00e2\u02c6\u02c6D p(d|\u00cb\u2020t)log|D|\n1\np(d|\u00cb\u2020t)\n,\nE\u00cb\u2020S(\u00cb\u2020t) = \u00cb\u2020s\u00e2\u02c6\u02c6 \u00cb\u2020S p(\u00cb\u2020s|\u00cb\u2020t)log| \u00cb\u2020S|\n1\np(\u00cb\u2020s|\u00cb\u2020t)\n, and a > 0 and b > 0 are\npowers to adjust term weights. Usually a = 1 and b = 1\nas default, and max\u00cb\u2020t \u00e2\u02c6\u02c6 \u00cb\u2020T (ED(\u00cb\u2020t )) and max\u00cb\u2020t \u00e2\u02c6\u02c6 \u00cb\u2020T (E\u00cb\u2020S(\u00cb\u2020t )) are\nused to normalize the entropy values. Term cluster weights\nare used to adjust p(\u00cb\u2020t, \u00cb\u2020s),\npw(\u00cb\u2020t, \u00cb\u2020s) =\nw\u00cb\u2020tp(\u00cb\u2020t, \u00cb\u2020s)\n\u00cb\u2020t\u00e2\u02c6\u02c6 \u00cb\u2020T ;\u00cb\u2020s\u00e2\u02c6\u02c6 \u00cb\u2020S w\u00cb\u2020tp(\u00cb\u2020t, \u00cb\u2020s)\n, (4)\nand\nIw( \u00cb\u2020T; \u00cb\u2020S) =\n\u00cb\u2020t\u00e2\u02c6\u02c6 \u00cb\u2020T \u00cb\u2020s\u00e2\u02c6\u02c6 \u00cb\u2020S\npw(\u00cb\u2020t, \u00cb\u2020s)log\npw(\u00cb\u2020t, \u00cb\u2020s)\npw(\u00cb\u2020t)pw(\u00cb\u2020s)\n, (5)\nwhere pw(\u00cb\u2020t) and pw(\u00cb\u2020s) are marginal distributions of pw(\u00cb\u2020t, \u00cb\u2020s).\nHowever, since we do not know either the term weights\nor P(D, \u00cb\u2020S, T), we need to estimate them, but w\u00cb\u2020t depends\non p(\u00cb\u2020s|t) and \u00cb\u2020S, while \u00cb\u2020S and p(\u00cb\u2020s|t) also depend on w\u00cb\u2020t that\nis still unknown. Thus, an iterative algorithm is required\nto estimate term weights w\u00cb\u2020t and find the best \nsegmentation and alignment to optimize the objective function Iw\nconcurrently. After a document is segmented into sentences\nInput:\nJoint probability distribution P(D, Sd, T),\nnumber of text segments p \u00e2\u02c6\u02c6 {2, 3, ..., max(sd)},\nnumber of term clusters k \u00e2\u02c6\u02c6 {2, 3, ..., l} (if k = l, no term\nco-clustering required), and\nweight type w \u00e2\u02c6\u02c6 {0, 1}, indicating to use I or Iw, respectively.\nOutput:\nMapping Clu, Seg, Ali, and term weights w\u00cb\u2020t.\nInitialization:\n0. i = 0. Initialize Clu\n(0)\nt , Seg\n(0)\nd , and Ali\n(0)\nd ; Initialize w\n(0)\n\u00cb\u2020t\nusing Equation (6) if w = 1;\nStage 1:\n1. If |D| = 1, k = l, and w = 0, check all sequential \nsegmentations of d into p segments and find the best one\nSegd(s) = argmax\u00cb\u2020sI( \u00cb\u2020T; \u00cb\u2020S),\nand return Segd; otherwise, if w = 1 and k = l, go to 3.1;\nStage 2:\n2.1 If k < l, for each term t, find the best cluster \u00cb\u2020t as\nClu(i+1)(t) = argmax\u00cb\u2020tI( \u00cb\u2020T; \u00cb\u2020S(i))\nbased on Seg(i) and Ali(i);\n2.2 For each d, check all sequential segmentations of d into p\nsegments with mapping s \u00e2\u2020\u2019 \u00cb\u2020s \u00e2\u2020\u2019 \u00cb\u2020s, and find the best one\nAli\n(i+1)\nd (Seg\n(i+1)\nd (s)) = argmax\u00cb\u2020sI( \u00cb\u2020T(i+1); \u00cb\u2020S)\nbased on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l;\n2.3 i + +. If Clu, Seg, or Ali changed, go to 2.1; otherwise,\nif w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1;\nStage 3:\n3.1 Update w\n(i+j+1)\n\u00cb\u2020t\nbased on Seg(i+j), Ali(i+j), and Clu(i)\nusing Equation (3);\n3.2 For each d, check all sequential segmentations of d into p\nsegments with mapping s \u00e2\u2020\u2019 \u00cb\u2020s \u00e2\u2020\u2019 \u00cb\u2020s, and find the best one\nAli\n(i+j+1)\nd (Seg\n(i+j+1)\nd (s)) = argmax\u00cb\u2020sIw( \u00cb\u2020T(i); \u00cb\u2020S)\nbased on Clu(i) and w\n(i+j+1)\n\u00cb\u2020t\n;\n3.3 j + +. If Iw( \u00cb\u2020T; \u00cb\u2020S) changes, go to step 6; otherwise, stop\nand return Clu(i), Seg(i+j), Ali(i+j), and w\n(i+j)\n\u00cb\u2020t\n;\nFigure 2: Algorithm: Topic segmentation and \nalignment based on MI or WMI.\nand each sentence is segmented into words, each word is\nstemmed. Then the joint probability distribution P(D, Sd, T)\ncan be estimated. Finally, this distribution can be used to\ncompute MI in our algorithm.\n4.3 Iterative Greedy Algorithm\nOur goal is to maximize the objective function, I( \u00cb\u2020T; \u00cb\u2020S) or\nIw( \u00cb\u2020T; \u00cb\u2020S), which can measure the dependence of term \noccurrences in different segments. Generally, first we do not know\nthe estimate term weights, which depend on the optimal\ntopic segmentation and alignment, and term clusters. \nMoreover, this problem is NP-hard [10], even though if we know\nthe term weights. Thus, an iterative greedy algorithm is \ndesired to find the best solution, even though probably only\nlocal maxima are reached. We present the iterative greedy\nalgorithm in Figure 2 to find a local maximum of I( \u00cb\u2020T; \u00cb\u2020S) or\nIw( \u00cb\u2020T; \u00cb\u2020S) with simultaneous term weight estimation. This\nalgorithm can is iterative and greedy for multi-document\ncases or single-document cases with term weight estimation\nand/or term co-clustering. Otherwise, since it is just a one\nstep algorithm to solve the task of single-document \nsegmentation [6, 15, 25], the global maximum of MI is guaranteed.\nWe will show later that term co-clustering reduces the \naccuracy of the results and is not necessary, and for \nsingledocument segmentation, term weights are also not required.\n4.3.1 Initialization\nIn Step 0, the initial term clustering Clut and topic \nsegmentation and alignment Segd and Alid are important to\navoid local maxima and reduce the number of iterations.\nFirst, a good guess of term weights can be made by using\nthe distributions of term frequency along sentences for each\ndocument and averaging them to get the initial values of w\u00cb\u2020t:\nwt = (\nED(t)\nmaxt \u00e2\u02c6\u02c6T (ED(t ))\n)(1 \u00e2\u02c6\u2019\nES(t)\nmaxt \u00e2\u02c6\u02c6T (ES(t ))\n), (6)\nwhere\nES(t) =\n1\n|Dt|\nd\u00e2\u02c6\u02c6Dt\n(1 \u00e2\u02c6\u2019\ns\u00e2\u02c6\u02c6Sd\np(s|t)log|Sd|\n1\np(s|t)\n),\nwhere Dt is the set of documents which contain Term t.\nThen, for the initial segmentation Seg(0)\n, we can simply\nsegment documents equally by sentences. Or we can find\nthe optimal segmentation just for each document d which\nmaximizes the WMI, Seg\n(0)\nd = argmax\u00cb\u2020sIw(T; \u00cb\u2020S), where\nw = w\n(0)\n\u00cb\u2020t\n. For the initial alignment Ali(0)\n, we can first\nassume that the order of segments for each d is the same.\nFor the initial term clustering Clu(0)\n, first cluster labels can\nbe set randomly, and after the first time of Step 3, a good\ninitial term clustering is obtained.\n4.3.2 Different Cases\nAfter initialization, there are three stages for different\ncases. Totally there are eight cases, |D| = 1 or |D| > 1,\nk = l or k < l, w = 0 or w = 1. Single document \nsegmentation without term clustering and term weight estimation\n(|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1). If\nterm clustering is required (k < l), Stage 2 (Step 2.1, 2.2,\nand 2.3) is executed iteratively. If term weight estimation\nis required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is \nexecuted iteratively. If both are required (k < l, w = 1), Stage 2\nand 3 run one after the other. For multi-document \nsegmentation without term clustering and term weight estimation\n(|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3\nare required.\nAt Stage 1, the global maximum can be found based on\nI( \u00cb\u2020T; \u00cb\u2020S) using dynamic programming in Section 4.4. \nSimultaneously finding a good term clustering and estimated term\nweights is impossible, since when moving a term to a new\nterm cluster to maximize Iw( \u00cb\u2020T; \u00cb\u2020S), we do not know that the\nweight of this term should be the one of the new cluster or\nthe old cluster. Thus, we first do term clustering at Stage\n2, and then estimate term weights at Stage 3.\nAt Stage 2, Step 2.1 is to find the best term clustering\nand Step 2.2 is to find the best segmentation. This cycle is\nrepeated to find a local maximum based on MI I until it \nconverges. The two steps are: (1) based on current term \nclustering Clu\u00cb\u2020t, for each document d, the algorithm segments\nall the sentences Sd into p segments sequentially (some \nsegments may be empty), and put them into the p segments\n\u00cb\u2020S of the whole training set D (all possible cases of different\nsegmentation Segd and alignment Alid are checked) to find\nthe optimal case, and (2) based on the current segmentation\nand alignment, for each term t, the algorithm finds the best\nterm cluster of t based on the current segmentation Segd\nand alignment Alid. After finding a good term clustering,\nterm weights are estimated if w = 1.\nAt Stage 3, similar as Stage 2, Step 3.1 is term weight\nre-estimation and Step 3.2 is to find a better segmentation.\nThey are repeated to find a local maximum based on WMI\nIw until it converges. However, if the term clustering in\nStage 2 is not accurate, then the term weight estimation at\nStage 3 may have a bad result. Finally, at Step 3.3, this \nalgorithm converges and return the output. This algorithm can\nhandle both single-document and multi-document \nsegmentation. It also can detect shared topics among documents\nby checking the proportion of overlapped sentences on the\nsame topics, as described in Sec 5.2.\n4.4 Algorithm Optimization\nIn many previous works on segmentation, dynamic \nprogramming is a technique used to maximize the objective\nfunction. Similarly, at Step 1, 2.2, and 3.2 of our algorithm,\nwe can use dynamic programming. For Stage 1, using \ndynamic programming can still find the global optimum, but\nfor Stage 2 and Stage 3, we can only find the optimum for\neach step of topic segmentation and alignment of a \ndocument. Here we only show the dynamic programming for\nStep 3.2 using WMI (Step 1 and 2.2 are similar but they can\nuse either I or Iw). There are two cases that are not shown\nin the algorithm in Figure 2: (a) single-document \nsegmentation or multi-document segmentation with the same \nsequential order of segments, where alignment is not required,\nand (b) multi-document segmentation with different \nsequential orders of segments, where alignment is necessary. The\nalignment mapping function of the former case is simply just\nAlid(\u00cb\u2020si) = \u00cb\u2020si, while for the latter one\"s alignment mapping\nfunction Alid(\u00cb\u2020si) = \u00cb\u2020sj, i and j may be different. The \ncomputational steps for the two cases are listed below:\nCase 1 (no alignment):\nFor each document d:\n(1) Compute pw(\u00cb\u2020t), partial pw(\u00cb\u2020t, \u00cb\u2020s) and partial pw(\u00cb\u2020s) \nwithout counting sentences from d. Then put sentences from i\nto j into Part k, and compute partial WMI\nPIw( \u00cb\u2020T; \u00cb\u2020sk(si, si+1, ..., sj))\n\u00cb\u2020t\u00e2\u02c6\u02c6 \u00cb\u2020T\npw(\u00cb\u2020t, \u00cb\u2020sk)log\npw(\u00cb\u2020t, \u00cb\u2020sk)\npw(\u00cb\u2020t)pw(\u00cb\u2020sk)\n,\nwhere Alid(si, si+1, ..., sj) = k, k \u00e2\u02c6\u02c6 {1, 2, ..., p}, 1 \u00e2\u2030\u00a4 i \u00e2\u2030\u00a4 j \u00e2\u2030\u00a4\nnd, and Segd(sq) = \u00cb\u2020sk for all i \u00e2\u2030\u00a4 q \u00e2\u2030\u00a4 j.\n(2) Let M(sm, 1) = PIw( \u00cb\u2020T; \u00cb\u2020s1(s1, s2, ..., sm)). Then\nM(sm, L) = maxi[M(si\u00e2\u02c6\u20191, L \u00e2\u02c6\u2019 1) + PIw( \u00cb\u2020T; \u00cb\u2020sL(si, ..., sm))],\nwhere 0 \u00e2\u2030\u00a4 m \u00e2\u2030\u00a4 nd, 1 < L < p, 1 \u00e2\u2030\u00a4 i \u00e2\u2030\u00a4 m + 1, and when\ni > m, no sentences are put into \u00cb\u2020sk when compute PIw\n(note PIw( \u00cb\u2020T; \u00cb\u2020s(si, ..., sm)) = 0 for single-document \nsegmentation).\n(3) Finally M(snd , p) = maxi[M(si\u00e2\u02c6\u20191, p \u00e2\u02c6\u2019 1)+\nPIw( \u00cb\u2020T; \u00cb\u2020sp(si, ..., snd ))], where 1 \u00e2\u2030\u00a4 i \u00e2\u2030\u00a4 nd+1. The optimal\nIw is found and the corresponding segmentation is the best.\nCase 2 (alignment required):\nFor each document d:\n(1) Compute pw(\u00cb\u2020t), partial pw(\u00cb\u2020t, \u00cb\u2020s), and partial pw(\u00cb\u2020s), and\nPIw( \u00cb\u2020T; \u00cb\u2020sk(si, si+1, ..., sj)) similarly as Case 1.\n(2) Let M(sm, 1, k) = PIw( \u00cb\u2020T; \u00cb\u2020sk(s1, s2, ..., sm)), where\nk \u00e2\u02c6\u02c6 {1, 2, ..., p}. Then M(sm, L, kL) = maxi,j[M(si\u00e2\u02c6\u20191, L \u00e2\u02c6\u2019\n1, kL/j) + PIw( \u00cb\u2020T; \u00cb\u2020sAlid(\u00cb\u2020sL\n)=j(si, si+1, ..., sm))],\nwhere 0 \u00e2\u2030\u00a4 m \u00e2\u2030\u00a4 nd, 1 < L < p, 1 \u00e2\u2030\u00a4 i \u00e2\u2030\u00a4 m + 1, kL \u00e2\u02c6\u02c6\nSet(p, L), which is the set of all p!\nL!(p\u00e2\u02c6\u2019L)!\ncombinations of\nL segments chosen from all p segments, j \u00e2\u02c6\u02c6 kL, the set\nof L segments chosen from all p segments, and kL/j is the\ncombination of L \u00e2\u02c6\u2019 1 segments in kL except Segment j.\n(3) Finally, M(snd , p, kp) = maxi,j[M(si\u00e2\u02c6\u20191, p \u00e2\u02c6\u2019 1, kp/j)\n+PIw( \u00cb\u2020T; \u00cb\u2020sAlid(\u00cb\u2020sL\n)=j(si, si+1, ..., snd ))],\nwhere kp is just the combination of all p segments and 1 \u00e2\u2030\u00a4\ni \u00e2\u2030\u00a4 nd + 1, which is the optimal Iw and the corresponding\nsegmentation is the best.\nThe steps of Case 1 and 2 are similar, except in Case 2,\nalignment is considered in addition to segmentation. First,\nbasic items of probability for computing Iw are computed\nexcluding Doc d, and then partial WMI by putting every\npossible sequential segment (including empty segment) of d\ninto every segment of the set. Second, the optimal sum of\nPIw for L segments and the leftmost m sentences, M(sm, L),\nis found. Finally, the maximal WMI is found among \ndifferent sums of M(sm, p \u00e2\u02c6\u2019 1) and PIw for Segment p.\n5. EXPERIMENTS\nIn this section, single-document segmentation, shared topic\ndetection, and multi-document segmentation will be tested.\nDifferent hyper parameters of our method are studied. For\nconvenience, we refer to the method using I as MIk if w = 0,\nand Iw as WMIk if w = 2 or as WMIk if w = 1, where k\nis the number of term clusters, and if k = l, where l is the\ntotal number of terms, then no term clustering is required,\ni.e. MIl and WMIl.\n5.1 Single-document Segmentation\n5.1.1 Test Data and Evaluation\nThe first data set we tested is a synthetic one used in\nprevious research [6, 15, 25] and many other papers. It has\n700 samples. Each is a concatenation of ten segments. Each\nsegment is the first n sentence selected randomly from the\nBrown corpus, which is supposed to have a different topic\nfrom each other. Currently, the best results on this data\nset is achieved by Ji et.al. [15]. To compare the \nperformance of our methods, the criterion used widely in previous\nresearch is applied, instead of the unbiased criterion \nintroduced in [20]. It chooses a pair of words randomly. If they\nare in different segments (different) for the real \nsegmentation (real), but predicted (pred) as in the same segment,\nit is a miss. If they are in the same segment (same), but\npredicted as in different segments, it is a false alarm. Thus,\nthe error rate is computed using the following equation:\np(err|real, pred) = p(miss|real, pred, diff)p(diff|real)\n+p(false alarm|real, pred, same)p(same|real).\n5.1.2 Experiment Results\nWe tested the case when the number of segments is known.\nTable 1 shows the results of our methods with different hyper\nparameter values and three previous approaches, C99[25],\nU00[6], and ADDP03[15], on this data set when the \nsegment number is known. In WMI for single-document \nsegmentation, the term weights are computed as follows: w\u00cb\u2020t =\n1\u00e2\u02c6\u2019E\u00cb\u2020S(\u00cb\u2020t)/max\u00cb\u2020t \u00e2\u02c6\u02c6 \u00cb\u2020T (E\u00cb\u2020S(\u00cb\u2020t )). For this case, our methods MIl\nand WMIl both outperform all the previous approaches.\nWe compared our methods with ADDP03using one-sample\none-sided t-test and p-values are shown in Table 2. From\nthe p-values, we can see that mostly the differences are very\nTable 1: Average Error Rates of Single-document\nSegmentation Given Segment Numbers Known\nRange of n 3-11 3-5 6-8 9-11\nSample size 400 100 100 100\nC99 12% 11% 10% 9%\nU00 10% 9% 7% 5%\nADDP03 6.0% 6.8% 5.2% 4.3%\nMIl 4.68% 5.57% 2.59% 1.59%\nWMIl 4.94% 6.33% 2.76% 1.62%\nMI100 9.62% 12.92% 8.66% 6.67%\nTable 2: Single-document Segmentation: P-values\nof T-test on Error Rates\nRange of n 3-11 3-5 6-8 9-11\nADDP03, MIl 0.000 0.000 0.000 0.000\nADDP03, WMIl 0.000 0.099 0.000 0.000\nMIl, WMIl 0.061 0.132 0.526 0.898\nsignificant. We also compare the error rates between our\ntwo methods using two-sample two-sided t-test to check the\nhypothesis that they are equal. We cannot reject the \nhypothesis that they are equal, so the difference are not \nsignificant, even though all the error rates for MIl are smaller\nthan WMIl. However, we can conclude that term weights\ncontribute little in single-document segmentation. The \nresults also show that MI using term co-clustering (k = 100)\ndecreases the performance. We tested different number of\nterm clusters, and found that the performance becomes \nbetter when the cluster number increases to reach l. WMIk<l\nhas similar results that we did not show in the table.\nAs mentioned before, using MI may be inconsistent on \noptimal boundaries given different numbers of segments. This\nsituation occurs especially when the similarities among \nsegments are quite different, i.e. some transitions are very\nobvious, while others are not. This is because usually a\ndocument is a hierarchical structure instead of only a \nsequential structure. When the segments are not at the same\nlevel, this situation may occur. Thus, a hierarchical topic\nsegmentation approach is desired, and the structure highly\ndepends on the number of segments for each internal node\nand the stop criteria of splitting. For this data set of \nsingledocument segmentation, since it is just a synthetic set, which\nis just a concatenation of several segments about different\ntopics, it is reasonable that approaches simply based on term\nfrequency have a good performance. Usually for the tasks\nof segmenting coherent documents for sub-topics, the \neffectiveness decreases much.\n5.2 Shared Topic Detection\n5.2.1 Test Data and Evaluation\nThe second data set contains 80 news articles from Google\nNews. There are eight topics and each has 10 articles. We\nrandomly split the set into subsets with different document\nnumbers and each subset has all eight topics. We \ncompare our approach MIl and WMIl with LDA [4]. LDA\ntreats a document in the data set as a bag of words, finds\nits distribution on topics, and its major topic. MIl and\nWMIl views each sentence as a bag of words and tag it\nwith a topic label. Then for each pair of documents, LDA\ndetermines if they are on the same topic, while MIl and\nTable 3: Shared Topic Detection: Average Error\nRates for Different Numbers of Documents in Each\nSubset\n#Doc 10 20 40 80\nLDA 8.89% 16.33% 1.35% 0.60%\nMIl, \u00ce\u00b8 = 0.6 4.17% 1.71% 1.47% 0.0%\nWMIl, \u00ce\u00b8 = 0.8 18.6% 3.16% 1.92% 0.0%\nWMIl check whether the proportion overlapped sentences\non the same topic is larger than the adjustable threshold \u00ce\u00b8.\nThat is, in MIl and WMIl, for a pair of documents d, d ,\nif [ s\u00e2\u02c6\u02c6Sd,s \u00e2\u02c6\u02c6Sd\n1(topics=topics )/min(|Sd|, |Sd|)] > \u00ce\u00b8, where\nSd is the set of sentences of d, and |Sd| is the number of\nsentences of d, then d and d have the shared topic.\nFor a pair of documents selected randomly, the error rate\nis computed using the following equation:\np(err|real, pred) = p(miss|real, pred, same)p(same|real)\n+p(false alarm|real, pred, diff)p(diff|real),\nwhere a miss means if they have the same topic (same)\nfor the real case (real), but predicted (pred) as on the same\ntopic. If they are on different topics (diff), but predicted\nas on the same topic, it is a false alarm.\n5.2.2 Experiment Results\nThe results are shown in Table 3. If most documents have\ndifferent topics, in WMIl, the estimation of term weights in\nEquation (3) is not correct. Thus, WMIl is not expected to\nhave a better performance than MIl, when most documents\nhave different topics. When there are fewer documents in\na subset with the same number of topics, more documents\nhave different topics, so WMIl is more worse than MIl. We\ncan see that for most cases MIl has a better (or at least\nsimilar) performance than LDA. After shared topic \ndetection, multi-document segmentation of documents with the\nshared topics is able to be executed.\n5.3 Multi-document Segmentation\n5.3.1 Test Data and Evaluation\nFor multi-document segmentation and alignment, our goal\nis to identify these segments about the same topic among\nmultiple similar documents with shared topics. Using Iw\nis expected to perform better than I, since without term\nweights the result is affected seriously by document-dependent\nstop words and noisy words which depends on the personal\nwriting style. It is more likely to treat the same segments\nof different documents as different segments under the effect\nof document-dependent stop words and noisy words. Term\nweights can reduce the effect of document-dependent stop\nwords and noisy words by giving cue terms more weights.\nThe data set for multi-document segmentation and \nalignment has 102 samples and 2264 sentences totally. Each is\nthe introduction part of a lab report selected from the course\nof Biol 240W, Pennsylvania State University. Each sample\nhas two segments, introduction of plant hormones and the\ncontent in the lab. The length range of samples is from\ntwo to 56 sentences. Some samples only have one part and\nsome have a reverse order the these two segments. It is not\nhard to identify the boundary between two segments for \nhuman. We labelled each sentence manually for evaluation.\nThe criterion of evaluation is just using the proportion of\nthe number of sentences with wrong predicted segment \nlabels in the total number of sentences in the whole training\nTable 4: Average Error Rates of Multi-document\nSegmentation Given Segment Numbers Known\n#Doc MIl WMIl k MIk WMIk\n102 3.14% 2.78% 300 4.68% 6.58%\n51 4.17% 3.63% 300 17.83% 22.84%\n34 5.06% 4.12% 300 18.75% 20.95%\n20 7.08% 5.42% 250 20.40% 21.83%\n10 10.38% 7.89% 250 21.42% 21.91%\n5 15.77% 11.64% 250 21.89% 22.59%\n2 25.90% 23.18% 50 25.44% 25.49%\n1 23.90% 24.82% 25 25.75% 26.15%\nTable 5: Multi-document Segmentation: P-values of\nT-test on Error Rates for MIl and WMIl\n#Doc 51 34 20 10 5 2\nP-value 0.19 0.101 0.025 0.001 0.000 0.002\nset as the error rate:\np(error|predicted, real)\n= d\u00e2\u02c6\u02c6D s\u00e2\u02c6\u02c6Sd\n1(predicteds=reals)/ d\u00e2\u02c6\u02c6D nd.\nIn order to show the benefits of multi-document \nsegmentation and alignment, we compared our method with different\nparameters on different partitions of the same training set.\nExcept the cases that the number of documents is 102 and\none (they are special cases of using the whole set and the\npure single-document segmentation), we randomly divided\nthe training set into m partitions, and each has 51, 34, 20,\n10, 5, and 2 document samples. Then we applied our \nmethods on each partition and calculated the error rate of the\nwhole training set. Each case was repeated for 10 times for\ncomputing the average error rates. For different partitions\nof the training set, different k values are used, since the \nnumber of terms increases when the document number in each\npartition increases.\n5.3.2 Experiment Results\nFrom the experiment results in Table 4, we can see the\nfollowing observations: (1) When the number of documents\nincreases, all methods have better performances. Only from\none to two documents, MIl has decrease a little. We can\nobserve this from Figure 3 at the point of document \nnumber = 2. Most curves even have the worst results at this\npoint. There are two reasons. First, because samples vote\nfor the best multi-document segmentation and alignment,\nbut if only two documents are compared with each other, the\none with missing segments or a totally different sequence will\naffect the correct segmentation and alignment of the other.\nSecond, as noted at the beginning of this section, if two \ndocuments have more document-dependent stop words or noisy\nwords than cue words, then the algorithm may view them\nas two different segments and the other segment is missing.\nGenerally, we can only expect a better performance when\nthe number of documents is larger than the number of \nsegments. (2) Except single-document segmentation, WMIl is\nalways better than MIl, and when the number of documents\nis reaching one or increases to a very large number, their\nperformances become closer. Table 5 shows p-values of \ntwosample one-sided t-test between MIl and WMIl. We also\ncan see this trend from p-values. When document number =\n5, we reached the smallest p-value and the largest difference\nbetween error rates of MIl and WMIl. For single-document\nTable 6: Multi-document Segmentation: Average\nError Rate for Document Number = 5 in Each \nSubset with Different Number of Term Clusters\n#Cluster 75 100 150 250 l\nMIk 24.67% 24.54% 23.91% 22.59% 15.77%\nsegmentation, WMIl is even a little bit worse than MIl,\nwhich is similar as the results of the single-document \nsegmentation on the first data set. The reason is that for \nsingledocument segmentation, we cannot estimate term weights\naccurately, since multiple documents are unavailable. (3)\nUsing term clustering usually gets worse results than MIl\nand WMIl.(4) Using term clustering in WMIk is even worse\nthan in MIk, since in WMIk term clusters are found first\nusing I before using Iw. If the term clusters are not correct,\nthen the term weights are estimated worse, which may \nmislead the algorithm to reach even worse results. From the\nresults we also found that in multi-document segmentation\nand alignment, most documents with missing segments and\na reverse order are identified correctly.\nTable 6 illustrates the experiment results for the case of 20\npartitions (each has five document samples) of the training\nset and topic segmentation and alignment using MIk with\ndifferent numbers of term clusters k. Notice that when the\nnumber of term clusters increases, the error rate becomes\nsmaller. Without term clustering, we have the best result.\nWe did not show results for WMIk with term clustering,\nbut the results are similar.\nWe also tested WMIl with different hyper parameters\nof a and b to adjust term weights. The results are \npresented in Figure 3. It was shown that the default case\nWMIl : a = 1, b = 1 gave the best results for different \npartitions of the training set. We can see the trend that when\nthe document number is very small or large, the difference\nbetween MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 \nbecomes quite small. When the document number is not large\n(about from 2 to 10), all the cases using term weights have\nbetter performances than MIl : a = 0, b = 0 without term\nweights, but when the document number becomes larger,\nthe cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1\nbecome worse than MIl : a = 0, b = 0. When the document\nnumber becomes very large, they are even worse than cases\nwith small document numbers. This means that a proper\nway to estimate term weights for the criterion of WMI is\nvery important. Figure 4 shows the term weights learned\nfrom the whole training set. Four types of words are \ncategorized roughly even though the transition among them are\nsubtle. Figure 5 illustrates the change in (weighted) mutual\ninformation for MIl and WMIl. As expected, mutual \ninformation for MIl increases monotonically with the number of\nsteps, while WMIl does not. Finally, MIl and WMIl are\nscalable, with computational complexity shown in Figure 6.\nOne advantage for our approach based on MI is that \nremoving stop words is not required. Another important \nadvantage is that there are no necessary hyper parameters to\nadjust. In single-document segmentation, the performance\nbased on MI is even better for that based on WMI, so no\nextra hyper parameter is required. In multi-document \nsegmentation, we show in the experiment, a = 1 and b = 1\nis the best. Our method gives more weights to cue terms.\nHowever, usually cue terms or sentences appear at the \nbeginning of a segment, while the end of the segment may be\n1 2 5 10 20 34 51 102\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nDocument Number\nErrorRate\nMIl\n:a=0,b=0\nWMI\nl\n:a=1,b=1\nWMI\nl\n:a=1,b=0\nWMI\nl\n:a=2,b=1\nFigure 3: Error rates for\ndifferent hyper \nparameters of term weights.\n0 0.2 0.4 0.6 0.8 1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nNormalized Document Entropy\nNormalizedSegmentEntropy\nNoisy words\nCue words\nCommon stop words\nDoc\u00e2\u02c6\u2019dep stop words\nFigure 4: Term weights\nlearned from the whole\ntraining set.\n0 100 200 300 400 500 600\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\nNumber of Steps\n(Weighted)MutualInformation\nMI\nl\nWMI\nl\nFigure 5: Change in\n(weighted) MI for MIl\nand WMIl.\n0 20 40 60 80 100 120\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\nDocument Number\nTimetoConverge(sec)\nMI\nl\nWMI\nl\nFigure 6: Time to\nconverge for MIl and\nWMIl.\nmuch noisy. One possible solution is giving more weights to\nterms at the beginning of each segment. Moreover, when the\nlength of segments are quite different, long segments have\nmuch higher term frequencies, so they may dominate the\nsegmentation boundaries. Normalization of term \nfrequencies versus the segment length may be useful.\n6. CONCLUSIONS AND FUTURE WORK\nWe proposed a novel method for multi-document topic\nsegmentation and alignment based on weighted mutual \ninformation, which can also handle single-document cases. We\nused dynamic programming to optimize our algorithm. Our\napproach outperforms all the previous methods on \nsingledocument cases. Moreover, we also showed that doing \nsegmentation among multiple documents can improve the \nperformance tremendously. Our results also illustrated that \nusing weighted mutual information can utilize the information\nof multiple documents to reach a better performance.\nWe only tested our method on limited data sets. More\ndata sets especially complicated ones should be tested. More\nprevious methods should be compared with. Moreover, \nnatural segmentations like paragraphs are hints that can be\nused to find the optimal boundaries. Supervised learning\nalso can be considered.\n7. ACKNOWLEDGMENTS\nThe authors want to thank Xiang Ji, and Prof. J. Scott\nPayne for their help.\n8. REFERENCES\n[1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and\nD. Modha. A generalized maximum entropy approach to\nbregman co-clustering and matrix approximation. In\nProceedings of SIGKDD, 2004.\n[2] R. Bekkerman, R. El-Yaniv, and A. McCallum. Multi-way\ndistributional clustering via pairwise interactions. In\nProceedings of ICML, 2005.\n[3] D. M. Blei and P. J. Moreno. Topic segmentation with an\naspect hidden markov model. In Proceedings of SIGIR,\n2001.\n[4] D. M. Blei, A. Ng, and M. Jordan. Latent dirichlet\nallocation. Journal of Machine Learning Research,\n3:993-1022, 2003.\n[5] T. Brants, F. Chen, and I. Tsochantaridis. Topic-based\ndocument segmentation with probabilistic latent semantic\nanalysis. In Proceedings of CIKM, 2002.\n[6] F. Choi. Advances in domain indepedent linear text\nsegmentation. In Proceedings of the NAACL, 2000.\n[7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.\nMaximum entropy segmentation of broadcast news. In\nProceedings of ICASSP, 2005.\n[8] T. Cover and J. Thomas. Elements of Information Theory.\nJohn Wiley and Sons, New York, USA, 1991.\n[9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and\nR. Harshman. Indexing by latent semantic analysis. Journal\nof the American Society for Information Systems, 1990.\n[10] I. Dhillon, S. Mallela, and D. Modha. Information-theoretic\nco-clustering. In Proceedings of SIGKDD, 2003.\n[11] M. Hajime, H. Takeo, and O. Manabu. Text segmentation\nwith multiple surface linguistic cues. In Proceedings of\nCOLING-ACL, 1998.\n[12] T. K. Ho. Stop word location and identification for\nadaptive text recognition. International Journal of\nDocument Analysis and Recognition, 3(1), August 2000.\n[13] T. Hofmann. Probabilistic latent semantic analysis. In\nProceedings of the UAI\"99, 1999.\n[14] X. Ji and H. Zha. Correlating summarization of a pair of\nmultilingual documents. In Proceedings of RIDE, 2003.\n[15] X. Ji and H. Zha. Domain-independent text segmentation\nusing anisotropic diffusion and dynamic programming. In\nProceedings of SIGIR, 2003.\n[16] X. Ji and H. Zha. Extracting shared topics of multiple\ndocuments. In Proceedings of the 7th PAKDD, 2003.\n[17] J. Lafferty, A. McCallum, and F. Pereira. Conditional\nrandom fields: Probabilistic models for segmenting and\nlabeling sequence data. In Proceedings of ICML, 2001.\n[18] T. Li, S. Ma, and M. Ogihara. Entropy-based criterion in\ncategorical clustering. In Proceedings of ICML, 2004.\n[19] A. McCallum, D. Freitag, and F. Pereira. Maximum\nentropy markov models for information extraction and\nsegmentation. In Proceedings of ICML, 2000.\n[20] L. Pevzner and M. Hearst. A critique and improvement of\nan evaluation metric for text segmentation. Computational\nLinguistic, 28(1):19-36, 2002.\n[21] J. C. Reynar. Statistical models for topic segmentation. In\nProceedings of ACL, 1999.\n[22] G. Salton and M. McGill. Introduction to Modern\nInformation Retrieval. McGraw Hill, 1983.\n[23] B. Sun, Q. Tan, P. Mitra, and C. L. Giles. Extraction and\nsearch of chemical formulae in text documents on the web.\nIn Proceedings of WWW, 2007.\n[24] B. Sun, D. Zhou, H. Zha, and J. Yen. Multi-task text\nsegmentation and alignment based on weighted mutual\ninformation. In Proceedings of CIKM, 2006.\n[25] M. Utiyama and H. Isahara. A statistical model for\ndomain-independent text segmentation. In Proceedings of\nthe 39th ACL, 1999.\n[26] C. Wayne. Multilingual topic detection and tracking:\nSuccessful research enabled by corpora and evaluation. In\nProceedings of LREC, 2000.\n[27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van\nMulbregt. A hidden markov model approach to text\nsegmentation and event tracking. In Proceedings of\nICASSP, 1998.\n[28] H. Zha and X. Ji. Correlating multilingual documents via\nbipartite graph modeling. In Proceedings of SIGIR, 2002.\n": ["topic detection", "tracking", "topic segmentation", "local and sequential information of document", "document local and sequential information", "single document", "multiple document", "wmus", "shared topic", "optimal boundary", "single-document segmentation", "multi-document segmentation", "cue term", "stop word", "term weight", "performance of topic segmentation", "topic segmentation performance", "multi-document segmentation", "share topic detection", "topic alignment", "mutual information", ""]}