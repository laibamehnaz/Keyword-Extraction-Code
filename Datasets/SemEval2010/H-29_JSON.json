{"Estimation and Use of Uncertainty\nin Pseudo-relevance Feedback\nKevyn Collins-Thompson and Jamie Callan\nLanguage Technologies Institute\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213-8213 U.S.A.\n{kct | callan}@cs.cmu.edu\nABSTRACT\nExisting pseudo-relevance feedback methods typically \nperform averaging over the top-retrieved documents, but \nignore an important statistical dimension: the risk or variance\nassociated with either the individual document models, or\ntheir combination. Treating the baseline feedback method\nas a black box, and the output feedback model as a random\nvariable, we estimate a posterior distribution for the \nfeedback model by resampling a given query\"s top-retrieved \ndocuments, using the posterior mean or mode as the enhanced\nfeedback model. We then perform model combination over\nseveral enhanced models, each based on a slightly modified\nquery sampled from the original query. We find that \nresampling documents helps increase individual feedback model\nprecision by removing noise terms, while sampling from the\nquery improves robustness (worst-case performance) by \nemphasizing terms related to multiple query aspects. The \nresult is a meta-feedback algorithm that is both more robust\nand more precise than the original strong baseline method.\nCategories and Subject Descriptors:\nH.3.3 [Information Retrieval]: Retrieval Models\nGeneral Terms: Algorithms, Experimentation\n1. INTRODUCTION\nUncertainty is an inherent feature of information retrieval.\nNot only do we not know the queries that will be presented\nto our retrieval algorithm ahead of time, but the user\"s \ninformation need may be vague or incompletely specified by\nthese queries. Even if the query were perfectly specified,\nlanguage in the collection documents is inherently complex\nand ambiguous and matching such language effectively is a\nformidable problem by itself. With this in mind, we wish\nto treat many important quantities calculated by the \nretrieval system, whether a relevance score for a document,\nor a weight for a query expansion term, as random \nvariables whose true value is uncertain but where the \nuncertainty about the true value may be quantified by replacing\nthe fixed value with a probability distribution over possible\nvalues. In this way, retrieval algorithms may attempt to\nquantify the risk or uncertainty associated with their \noutput rankings, or improve the stability or precision of their\ninternal calculations.\nCurrent algorithms for pseudo-relevance feedback (PRF)\ntend to follow the same basic method whether we use \nvector space-based algorithms such as Rocchio\"s formula [16],\nor more recent language modeling approaches such as \nRelevance Models [10]. First, a set of top-retrieved documents is\nobtained from an initial query and assumed to approximate\na set of relevant documents. Next, a single feedback model\nvector is computed according to some sort of average, \ncentroid, or expectation over the set of possibly-relevant \ndocument models. For example, the document vectors may be\ncombined with equal weighting, as in Rocchio, or by query\nlikelihood, as may be done using the Relevance Model1\n. The\nuse of an expectation is reasonable for practical and \ntheoretical reasons, but by itself ignores potentially valuable\ninformation about the risk of the feedback model.\nOur main hypothesis in this paper is that estimating the\nuncertainty in feedback is useful and leads to better \nindividual feedback models and more robust combined models.\nTherefore, we propose a method for estimating uncertainty\nassociated with an individual feedback model in terms of\na posterior distribution over language models. To do this,\nwe systematically vary the inputs to the baseline feedback\nmethod and fit a Dirichlet distribution to the output. We\nuse the posterior mean or mode as the improved feedback\nmodel estimate. This process is shown in Figure 1. As we\nshow later, the mean and mode may vary significantly from\nthe single feedback model proposed by the baseline method.\nWe also perform model combination using several improved\nfeedback language models obtained by a small number of\nnew queries sampled from the original query. A model\"s\nweight combines two complementary factors: the model\"s\nprobability of generating the query, and the variance of the\nmodel, with high-variance models getting lower weight.\n1\nFor example, an expected parameter vector conditioned on\nthe query observation is formed from top-retrieved \ndocuments, which are treated as training strings (see [10], p. 62).\nFigure 1: Estimating the uncertainty of the feedback model\nfor a single query.\n2. SAMPLING-BASED FEEDBACK\nIn Sections 2.1-2.5 we describe a general method for \nestimating a probability distribution over the set of possible\nlanguage models. In Sections 2.6 and 2.7 we summarize how\ndifferent query samples are used to generate multiple \nfeedback models, which are then combined.\n2.1 Modeling Feedback Uncertainty\nGiven a query Q and a collection C, we assume a \nprobabilistic retrieval system that assigns a real-valued document\nscore f(D, Q) to each document D in C, such that the score\nis proportional to the estimated probability of relevance. We\nmake no other assumptions about f(D, Q). The nature of\nf(D, Q) may be complex: for example, if the retrieval \nsystem supports structured query languages [12], then f(D, Q)\nmay represent the output of an arbitrarily complex \ninference network defined by the structured query operators. In\ntheory, the scoring function can vary from query to query,\nalthough in this study for simplicity we keep the scoring\nfunction the same for all queries. Our specific query method\nis given in Section 3.\nWe treat the feedback algorithm as a black box and \nassume that the inputs to the feedback algorithm are the \noriginal query and the corresponding top-retrieved documents,\nwith a score being given to each document. We assume that\nthe output of the feedback algorithm is a vector of term\nweights to be used to add or reweight the terms in the \nrepresentation of the original query, with the vector normalized\nto form a probability distribution. We view the the inputs\nto the feedback black box as random variables, and analyze\nthe feedback model as a random variable that changes in \nresponse to changes in the inputs. Like the document scoring\nfunction f(D, Q), the feedback algorithm may implement\na complex, non-linear scoring formula, and so as its inputs\nvary, the resulting feedback models may have a complex\ndistribution over the space of feedback models (the sample\nspace). Because of this potential complexity, we do not \nattempt to derive a posterior distribution in closed form, but\ninstead use simulation. We call this distribution over \npossible feedback models the feedback model distribution. Our\ngoal in this section is to estimate a useful approximation to\nthe feedback model distribution.\nFor a specific framework for experiments, we use the \nlanguage modeling (LM) approach for information retrieval [15].\nThe score of a document D with respect to a query Q and\ncollection C is given by p(Q|D) with respect to language\nmodels \u00cb\u2020\u00ce\u00b8Q and \u00cb\u2020\u00ce\u00b8D estimated for the query and document\nrespectively. We denote the set of k top-retrieved \ndocuments from collection C in response to Q by DQ(k, C). For\nsimplicity, we assume that queries and documents are \ngenerated by multinomial distributions whose parameters are\nrepresented by unigram language models.\nTo incorporate feedback in the LM approach, we assume a\nmodel-based scheme in which our goal is take the query and\nresulting ranked documents DQ(k, C) as input, and output\nan expansion language model \u00cb\u2020\u00ce\u00b8E, which is then interpolated\nwith the original query model \u00cb\u2020\u00ce\u00b8Q:\n\u00cb\u2020\u00ce\u00b8New = (1 \u00e2\u02c6\u2019 \u00ce\u00b1) \u00c2\u00b7 \u00cb\u2020\u00ce\u00b8Q + \u00ce\u00b1 \u00c2\u00b7 \u00cb\u2020\u00ce\u00b8E (1)\nThis includes the possibility of \u00ce\u00b1 = 1 where the original\nquery mode is completely replaced by the feedback model.\nOur sample space is the set of all possible language \nmodels LF that may be output as feedback models. Our \napproach is to take samples from this space and then fit a\ndistribution to the samples using maximum likelihood. For\nsimplicity, we start by assuming the latent feedback \ndistribution has the form of a Dirichlet distribution. Although the\nDirichlet is a unimodal distribution, and in general quite\nlimited in its expressiveness in the sample space, it is a \nnatural match for the multinomial language model, can be \nestimated quickly, and can capture the most salient features of\nconfident and uncertain feedback models, such as the overall\nspread of the distibution.\n2.2 Resampling document models\nWe would like an approximation to the posterior \ndistribution of the feedback model LF . To accomplish this, we\napply a widely-used simulation technique called bootstrap\nsampling ([7], p. 474) on the input parameters, namely, the\nset of top-retrieved documents.\nBootstrap sampling allows us to simulate the approximate\neffect of perturbing the parameters within the black box\nfeedback algorithm by perturbing the inputs to that \nalgorithm in a systematic way, while making no assumptions\nabout the nature of the feedback algorithm.\nSpecifically, we sample k documents with replacement from\nDQ(k, C), and calculate an expansion language model \u00ce\u00b8b \nusing the black box feedback method. We repeat this process\nB times to obtain a set of B feedback language models, to\nwhich we then fit a Dirichlet distribution. Typically B is\nin the range of 20 to 50 samples, with performance being\nrelatively stable in this range. Note that instead of treating\neach top document as equally likely, we sample according to\nthe estimated probabilities of relevance of each document in\nDQ(k, C). Thus, a document is more likely to be chosen the\nhigher it is in the ranking.\n2.3 Justification for a sampling approach\nThe rationale for our sampling approach has two parts.\nFirst, we want to improve the quality of individual \nfeedback models by smoothing out variation when the baseline\nfeedback model is unstable. In this respect, our approach\nresembles bagging [4], an ensemble approach which \ngenerates multiple versions of a predictor by making bootstrap\ncopies of the training set, and then averages the (numerical)\npredictors. In our application, top-retrieved documents can\nbe seen as a kind of noisy training set for relevance.\nSecond, sampling is an effective way to estimate basic\nproperties of the feedback posterior distribution, which can\nthen be used for improved model combination. For \nexample, a model may be weighted by its prediction confidence,\nestimated as a function of the variability of the posterior\naround the model.\nfoo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood\n(a) Topic 401\nForeign\nminorities,\nGermany\nfoo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood\n(b) Topic 402\nBehavioral\ngenetics\nfoo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood\n(c) Topic 459\nWhen can a\nlender foreclose\non property\nFigure 2: Visualization of expansion language model \nvariance using self-organizing maps, showing the distribution of\nlanguage models that results from resampling the inputs to\nthe baseline expansion method. The language model that\nwould have been chosen by the baseline expansion is at\nthe center of each map. The similarity function is \nJensenShannon divergence.\n2.4 Visualizing feedback distributions\nBefore describing how we fit and use the Dirichlet \ndistribution over feedback models, it is instructive to view some\nexamples of actual feedback model distributions that result\nfrom bootstrap sampling the top-retrieved documents from\ndifferent TREC topics.\nEach point in our sample space is a language model, which\ntypically has several thousand dimensions. To help analyze\nthe behavior of our method we used a Self-Organizing Map\n(via the SOM-PAK package [9]), to \u00e2\u20ac\u02dcflatten\" and visualize\nthe high-dimensional density function2\n.\nThe density maps for three TREC topics are shown in\nFigure 2 above. The dark areas represent regions of high\nsimilarity between language models. The light areas \nrepresent regions of low similarity - the \u00e2\u20ac\u02dcvalleys\" between \nclusters. Each diagram is centered on the language model that\nwould have been chosen by the baseline expansion. A single\npeak (mode) is evident in some examples, but more complex\nstructure appears in others. Also, while the distribution is\nusually close to the baseline feedback model, for some topics\nthey are a significant distance apart (as measured by \nJensenShannon divergence), as in Subfigure 2c. In such cases, the\nmode or mean of the feedback distribution often performs\nsignificantly better than the baseline (and in a smaller \nproportion of cases, significantly worse).\n2.5 Fitting a posterior feedback distribution\nAfter obtaining feedback model samples by resampling\nthe feedback model inputs, we estimate the feedback \ndistribution. We assume that the multinomial feedback \nmodels {\u00cb\u2020\u00ce\u00b81, . . . , \u00cb\u2020\u00ce\u00b8B} were generated by a latent Dirichlet \ndistribution with parameters {\u00ce\u00b11, . . . , \u00ce\u00b1N }. To estimate the\n{\u00ce\u00b11, . . . , \u00ce\u00b1N }, we fit the Dirichlet parameters to the B \nlanguage model samples according to maximum likelihood \nusing a generalized Newton procedure, details of which are\ngiven in Minka [13]. We assume a simple Dirichlet prior over\nthe {\u00ce\u00b11, . . . , \u00ce\u00b1N }, setting each to \u00ce\u00b1i = \u00ce\u00bc \u00c2\u00b7 p(wi | C), where \u00ce\u00bc\nis a parameter and p(\u00c2\u00b7 | C) is the collection language model\nestimated from a set of documents from collection C. The\nparameter fitting converges very quickly - typically just 2 or\n2\nBecause our points are language models in the \nmultinomial simplex, we extended SOM-PAK to support \nJensenShannon divergence, a widely-used similarity measure \nbetween probability distributions.\n3 iterations are enough - so that it is practical to apply at\nquery-time when computational overhead must be small. In\npractice, we can restrict the calculation to the vocabulary of\nthe top-retrieved documents, instead of the entire collection.\nNote that for this step we are re-using the existing retrieved\ndocuments and not performing additional queries.\nGiven the parameters of an N-dimensional Dirichlet \ndistribution Dir(\u00ce\u00b1) the mean \u00ce\u00bc and mode x vectors are easy\nto calculate and are given respectively by\n\u00ce\u00bci = \u00ce\u00b1iP\n\u00ce\u00b1i\n(2) and xi = \u00ce\u00b1i\u00e2\u02c6\u20191P\n\u00ce\u00b1i\u00e2\u02c6\u2019N\n. (3)\nWe can then choose the language model at the mean or the\nmode of the posterior as the final enhanced feedback model.\n(We found the mode to give slightly better performance.)\nFor information retrieval, the number of samples we will\nhave available is likely to be quite small for performance \nreasons - usually less than ten. Moreover, while random \nsampling is useful in certain cases, it is perfectly acceptable to\nallow deterministic sampling distributions, but these must\nbe designed carefully in order to approximate an accurate\noutput variance. We leave this for future study.\n2.6 Query variants\nWe use the following methods for generating variants of\nthe original query. Each variant corresponds to a different\nassumption about which aspects of the original query may\nbe important. This is a form of deterministic sampling.\nWe selected three simple methods that cover complimentary\nassumptions about the query.\nNo-expansion Use only the original query. The \nassumption is that the given terms are a complete description\nof the information need.\nLeave-one-out A single term is left out of the original\nquery. The assumption is that one of the query terms\nis a noise term.\nSingle-term A single term is chosen from the original query.\nThis assumes that only one aspect of the query, namely,\nthat represented by the term, is most important.\nAfter generating a variant of the original query, we combine\nit with the original query using a weight \u00ce\u00b1SUB so that we\ndo not stray too \u00e2\u20ac\u02dcfar\". In this study, we set \u00ce\u00b1SUB = 0.5. For\nexample, using the Indri [12] query language, a \nleave-oneout variant of the initial query that omits the term \u00e2\u20ac\u02dcireland\"\nfor TREC topic 404 is:\n#weight(0.5 #combine(ireland peace talks)\n0.5 #combine(peace talks))\n2.7 Combining enhanced feedback models\nfrom multiple query variants\nWhen using multiple query variants, the resulting \nenhanced feedback models are combined using Bayesian model\ncombination. To do this, we treat each word as an item to\nbe classified as belonging to a relevant or non-relevant class,\nand derive a class probability for each word by combining\nthe scores from each query variant. Each score is given by\nthat term\"s probability in the Dirichlet distribution. The\nterm scores are weighted by the inverse of the variance of\nthe term in the enhanced feedback model\"s Dirichlet \ndistribution. The prior probability of a word\"s membership in\nthe relevant class is given by the probability of the original\nquery in the entire enhanced expansion model.\n3. EVALUATION\nIn this section we present results confirming the usefulness\nof estimating a feedback model distribution from weighted\nresampling of top-ranked documents, and of combining the\nfeedback models obtained from different small changes in\nthe original query.\n3.1 General method\nWe evaluated performance on a total of 350 queries \nderived from four sets of TREC topics: 51-200 (TREC-1&2),\n351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g,\nTREC-9&10). We chose these for their varied content and\ndocument properties. For example, wt10g documents are\nWeb pages with a wide variety of subjects and styles while\nTREC-1&2 documents are more homogeneous news articles.\nIndexing and retrieval was performed using the Indri system\nin the Lemur toolkit [12] [1]. Our queries were derived from\nthe words in the title field of the TREC topics. Phrases\nwere not used. To generate the baseline queries passed to\nIndri, we wrapped the query terms with Indri\"s #combine\noperator. For example, the initial query for topic 404 is:\n#combine(ireland peace talks)\nWe performed Krovetz stemming for all experiments. \nBecause we found that the baseline (Indri) expansion method\nperformed better using a stopword list with the feedback\nmodel, all experiments used a stoplist of 419 common \nEnglish words. However, an interesting side-effect of our \nresampling approach is that it tends to remove many stopwords\nfrom the feedback model, making a stoplist less critical. This\nis discussed further in Section 3.6.\n3.2 Baseline feedback method\nFor our baseline expansion method, we use an algorithm\nincluded in Indri 1.0 as the default expansion method. This\nmethod first selects terms using a log-odds calculation \ndescribed by Ponte [14], but assigns final term weights using\nLavrenko\"s relevance model[10].\nWe chose the Indri method because it gives a consistently\nstrong baseline, is based on a language modeling approach,\nand is simple to experiment with. In a TREC evaluation\nusing the GOV2 corpus [6], the method was one of the \ntopperforming runs, achieving a 19.8% gain in MAP compared\nto using unexpanded queries. In this study, it achieves an\naverage gain in MAP of 17.25% over the four collections.\nIndri\"s expansion method first calculates a log-odds ratio\no(v) for each potential expansion term v given by\no(v) =\nX\nD\nlog\np(v|D)\np(v|C)\n(4)\nover all documents D containing v, in collection C. Then,\nthe expansion term candidates are sorted by descending\no(v), and the top m are chosen. Finally, the term weights\nr(v) used in the expanded query are calculated based on the\nrelevance model\nr(v) =\nX\nD\np(q|D)p(v|D)\np(v)\np(D)\n(5)\nThe quantity p(q|D) is the probability score assigned to the\ndocument in the initial retrieval. We use Dirichlet \nsmoothing of p(v|D) with \u00ce\u00bc = 1000.\nThis relevance model is then combined with the original\nquery using linear interpolation, weighted by a parameter \u00ce\u00b1.\nBy default we used the top 50 documents for feedback and\nthe top 20 expansion terms, with the feedback interpolation\nparameter \u00ce\u00b1 = 0.5 unless otherwise stated. For example,\nthe baseline expanded query for topic 404 is:\n#weight(0.5 #combine(ireland peace talks) 0.5\n#weight(0.10 ireland 0.08 peace 0.08 northern ...)\n3.3 Expansion performance\nWe measure our feedback algorithm\"s effectiveness by two\nmain criteria: precision, and robustness. Robustness, and\nthe tradeoff between precision and robustness, is analyzed\nin Section 3.4. In this section, we examine average \nprecision and precision in the top 10 documents (P10). We also\ninclude recall at 1,000 documents.\nFor each query, we obtained a set of B feedback models\nusing the Indri baseline. Each feedback model was obtained\nfrom a random sample of the top k documents taken with\nreplacement. For these experiments, B = 30 and k = 50.\nEach feedback model contained 20 terms. On the query side,\nwe used leave-one-out (LOO) sampling to create the query\nvariants. Single-term query sampling had consistently worse\nperformance across all collections and so our results here \nfocus on LOO sampling. We used the methods described in\nSection 2 to estimate an enhanced feedback model from the\nDirichlet posterior distribution for each query variant, and\nto combine the feedback models from all the query variants.\nWe call our method \u00e2\u20ac\u02dcresampling expansion\" and denote it as\nRS-FB here. We denote the Indri baseline feedback method\nas Base-FB. Results from applying both the baseline \nexpansion method (Base-FB) and resampling expansion (RS-FB)\nare shown in Table 1.\nWe observe several trends in this table. First, the average\nprecision of RS-FB was comparable to Base-FB, achieving\nan average gain of 17.6% compared to using no expansion\nacross the four collections. The Indri baseline expansion\ngain was 17.25%. Also, the RS-FB method achieved \nconsistent improvements in P10 over Base-FB for every topic set,\nwith an average improvement of 6.89% over Base-FB for all\n350 topics. The lowest P10 gain over Base-FB was +3.82%\nfor TREC-7 and the highest was +11.95% for wt10g. \nFinally, both Base-FB and RS-FB also consistently improved\nrecall over using no expansion, with Base-FB achieving \nbetter recall than RS-FB for all topic sets.\n3.4 Retrieval robustness\nWe use the term robustness to mean the worst-case \naverage precision performance of a feedback algorithm. Ideally,\na robust feedback method would never perform worse than\nusing the original query, while often performing better using\nthe expansion.\nTo evaluate robustness in this study, we use a very \nsimple measure called the robustness index (RI)3\n. For a set of\nqueries Q, the RI measure is defined as:\nRI(Q) =\nn+ \u00e2\u02c6\u2019 n\u00e2\u02c6\u2019\n|Q|\n(6)\nwhere n+ is the number of queries helped by the feedback\nmethod and n\u00e2\u02c6\u2019 is the number of queries hurt. Here, by\n\u00e2\u20ac\u02dchelped\" we mean obtaining a higher average precision as a\nresult of feedback. The value of RI ranges from a minimum\n3\nThis is sometimes also called the reliability of improvement\nindex and was used in Sakai et al. [17].\nCollection NoExp Base-FB RS-FB\nTREC\n1&2\nAvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%)\nP10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%)\nRecall 15084/37393 19172/37393 15396/37393\nTREC 7\nAvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%)\nP10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%)\nRecall 2179/4674 2608/4674 2487/4674\nTREC 8\nAvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%)\nP10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%)\nRecall 2144/4728 2642/4728 2485/4728\nwt10g\nAvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%)\nP10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%)\nRecall 3361/5980 3725/5980 3664/5980\nTable 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB). Improvement shown for \nBaseFB and RS-FB is relative to using no expansion.\n(a) TREC 1&2 (upper curve); TREC 8\n(lower curve)\n(b) TREC 7 (upper curve); wt10g (lower\ncurve)\nFigure 3: The trade-off between robustness and average \nprecision for different corpora. The x-axis gives the change in\nMAP over using baseline expansion with \u00ce\u00b1 = 0.5. The \nyaxis gives the Robustness Index (RI). Each curve through\nuncircled points shows the RI/MAP tradeoff using the \nsimple small-\u00ce\u00b1 strategy (see text) as \u00ce\u00b1 decreases from 0.5 to\nzero in the direction of the arrow. Circled points represent\nthe tradeoffs obtained by resampling feedback for \u00ce\u00b1 = 0.5.\nCollection N Base-FB RS-FB\nn\u00e2\u02c6\u2019 RI n\u00e2\u02c6\u2019 RI\nTREC 1&2 103 26 +0.495 15 +0.709\nTREC 7 46 14 +0.391 10 +0.565\nTREC 8 44 12 +0.455 12 +0.455\nwt10g 91 48 -0.055 39 +0.143\nCombined 284 100 +0.296 76 +0.465\nTable 2: Comparison of robustness index (RI) for baseline\nfeedback (Base-FB) vs. resampling feedback (RS-FB). Also\nshown are the actual number of queries hurt by feedback\n(n\u00e2\u02c6\u2019) for each method and collection. Queries for which \ninitial average precision was negligible (\u00e2\u2030\u00a4 0.01) were ignored,\ngiving the remaining query count in column N.\nof \u00e2\u02c6\u20191.0, when all queries are hurt by the feedback method,\nto +1.0 when all queries are helped. The RI measure does\nnot take into account the magnitude or distribution of the\namount of change across the set Q. However, it is easy to\nunderstand as a general indication of robustness.\nOne obvious way to improve the worst-case performance\nof feedback is simply to use a smaller fixed \u00ce\u00b1 interpolation\nparameter, such as \u00ce\u00b1 = 0.3, placing less weight on the \n(possibly risky) feedback model and more on the original query.\nWe call this the \u00e2\u20ac\u02dcsmall-\u00ce\u00b1\" strategy. Since we are also \nreducing the potential gains when the feedback model is \u00e2\u20ac\u02dcright\",\nhowever, we would expect some trade-off between average\nprecision and robustness. We therefore compared the \nprecision/robustness trade-off between our resampling feedback\nalgorithm, and the simple small-\u00ce\u00b1 method. The results are\nsummarized in Figure 3. In the figure, the curve for each\ntopic set interpolates between trade-off points, beginning\nat x=0, where \u00ce\u00b1 = 0.5, and continuing in the direction of\nthe arrow as \u00ce\u00b1 decreases and the original query is given\nmore and more weight. As expected, robustness \ncontinuously increases as we move along the curve, but mean \naverage precision generally drops as the gains from feedback are\neliminated. For comparison, the performance of resampling\nfeedback at \u00ce\u00b1 = 0.5 is shown for each collection as the circled\npoint. Higher and to the right is better. This figure shows\nthat resampling feedback gives a somewhat better trade-off\nthan the small-\u00ce\u00b1 approach for 3 of the 4 collections.\nFigure 4: Histogram showing improved robustness of \nresampling feedback (RS-FB) over baseline feedback (Base-FB)\nfor all datasets combined. Queries are binned by % change\nin AP compared to the unexpanded query.\nCollection DS + QV DS + No QV\nTREC\n1&2\nAvgP 0.2406 0.2547 (+5.86%)\nP10 0.5263 0.5362 (+1.88%)\nRI 0.7087 0.6515 (-0.0572)\nTREC 7\nAvgP 0.2169 0.2200 (+1.43%)\nP10 0.4480 0.4300 (-4.02%)\nRI 0.5652 0.2609 (-0.3043)\nTREC 8\nAvgP 0.2268 0.2257 (-0.49%)\nP10 0.4340 0.4200 (-3.23%)\nRI 0.4545 0.4091 (-0.0454)\nwt10g\nAvgP 0.1946 0.1865 (-4.16%)\nP10 0.2960 0.2680 (-9.46%)\nRI 0.1429 0.0220 (-0.1209)\nTable 3: Comparison of resampling feedback using \ndocument sampling (DS) with (QV) and without (No QV) \ncombining feedback models from multiple query variants.\nTable 2 gives the Robustness Index scores for Base-FB\nand RS-FB. The RS-FB feedback method obtained higher\nrobustness than Base-FB on three of the four topic sets, with\nonly slightly worse performance on TREC-8.\nA more detailed view showing the distribution over \nrelative changes in AP is given by the histogram in Figure 4.\nCompared to Base-FB, the RS-FB method achieves a \nnoticable reduction in the number of queries significantly hurt\nby expansion (i.e. where AP is hurt by 25% or more), while\npreserving positive gains in AP.\n3.5 Effect of query and document\nsampling methods\nGiven our algorithm\"s improved robustness seen in \nSection 3.4, an important question is what component of our\nsystem is responsible. Is it the use of document re-sampling,\nthe use of multiple query variants, or some other factor? The\nresults in Table 3 suggest that the model combination based\non query variants may be largely account for the improved\nrobustness. When query variants are turned off and the \noriginal query is used by itself with document sampling, there\nis little net change in average precision, a small decrease in\nP10 for 3 out of the 4 topic sets, but a significant drop in\nrobustness for all topic sets. In two cases, the RI measure\ndrops by more than 50%.\nWe also examined the effect of the document sampling\nmethod on retrieval effectiveness, using two different \nstrategies. The \u00e2\u20ac\u02dcuniform weighting\" strategy ignored the relevance\nscores from the initial retrieval and gave each document in\nthe top k the same probability of selection. In contrast, the\n\u00e2\u20ac\u02dcrelevance-score weighting\" strategy chose documents with\nprobability proportional to their relevance scores. In this\nway, documents that were more highly ranked were more\nlikely to be selected. Results are shown in Table 4.\nThe relevance-score weighting strategy performs better\noverall, with significantly higher RI and P10 scores on 3 of\nthe 4 topic sets. The difference in average precision between\nthe methods, however, is less marked. This suggests that\nuniform weighting acts to increase variance in retrieval \nresults: when initial average precision is high, there are many\nrelevant documents in the top k and uniform sampling may\ngive a more representative relevance model than focusing on\nthe highly-ranked items. On the other hand, when initial\nprecision is low, there are few relevant documents in the\nbottom ranks and uniform sampling mixes in more of the\nnon-relevant documents.\nFor space reasons we only summarize our findings on \nsample size here. The number of samples has some effect on\nprecision when less than 10, but performance stabilizes at\naround 15 to 20 samples. We used 30 samples for our \nexperiments. Much beyond this level, the additional benefits\nof more samples decrease as the initial score distribution is\nmore closely fit and the processing time increases.\n3.6 The effect of resampling on expansion\nterm quality\nIdeally, a retrieval model should not require a stopword\nlist when estimating a model of relevance: a robust \nstatistical model should down-weight stopwords automatically\ndepending on context. Stopwords can harm feedback if \nselected as feedback terms, because they are typically poor\ndiscriminators and waste valuable term slots. In practice,\nhowever, because most term selection methods resemble a\ntf \u00c2\u00b7 idf type of weighting, terms with low idf but very high\ntf can sometimes be selected as expansion term candidates.\nThis happens, for example, even with the Relevance Model\napproach that is part of our baseline feedback. To ensure\nas strong a baseline as possible, we use a stoplist for all \nexperiments reported here. If we turn off the stopword list,\nhowever, we obtain results such as those shown in Table 5\nwhere four of the top ten baseline feedback terms for TREC\ntopic 60 (said, but, their, not) are stopwords using the \nBaseFB method. (The top 100 expansion terms were selected to\ngenerate this example.)\nIndri\"s method attempts to address the stopword \nproblem by applying an initial step based on Ponte [14] to \nselect less-common terms that have high log-odds of being\nin the top-ranked documents compared to the whole \ncollection. Nevertheless, this does not overcome the stopword\nproblem completely, especially as the number of feedback\nterms grows.\nUsing resampling feedback, however, appears to mitigate\nCollection QV + Uniform QV + Relevance-score\nweighting weighting\nTREC\n1&2\nAvgP 0.2545 0.2406 (-5.46%)\nP10 0.5369 0.5263 (-1.97%)\nRI 0.6212 0.7087 (+14.09%)\nTREC 7\nAvgP 0.2174 0.2169 (-0.23%)\nP10 0.4320 0.4480 (+3.70%)\nRI 0.4783 0.5652 (+18.17%)\nTREC 8\nAvgP 0.2267 0.2268 (+0.04%)\nP10 0.4120 0.4340 (+5.34%)\nRI 0.4545 0.4545 (+0.00%)\nwt10g\nAvgP 0.1808 0.1946 (+7.63%)\nP10 0.2680 0.2960 (+10.45%)\nRI 0.0220 0.1099 (+399.5%)\nTable 4: Comparison of uniform and relevance-weighted document sampling. The percentage change compared to uniform\nsampling is shown in parentheses. QV indicates that query variants were used in both runs.\nBaseline FB p(wi|R) Resampling FB p(wi|R)\nsaid 0.055 court 0.026\ncourt 0.055 pay 0.018\npay 0.034 federal 0.012\nbut 0.026 education 0.011\nemployees 0.024 teachers 0.010\ntheir 0.024 employees 0.010\nnot 0.023 case 0.010\nfederal 0.021 their 0.009\nworkers 0.020 appeals 0.008\neducation 0.020 union 0.007\nTable 5: Feedback term quality when a stoplist is not used.\nFeedback terms for TREC topic 60: merit pay vs seniority.\nthe effect of stopwords automatically. In the example of \nTable 5, resampling feedback leaves only one stopword (their)\nin the top ten. We observed similar feedback term behavior\nacross many other topics. The reason for this effect appears\nto be the interaction of the term selection score with the\ntop-m term cutoff. While the presence and even \nproportion of particular stopwords is fairly stable across different\ndocument samples, their relative position in the top-m list\nis not, as sets of documents with varying numbers of \nbetter, lower-frequency term candidates are examined for each\nsample. As a result, while some number of stopwords may\nappear in each sampled document set, any given stopword\ntends to fall below the cutoff for multiple samples, leading\nto its classification as a high-variance, low-weight feature.\n4. RELATED WORK\nOur approach is related to previous work from several \nareas of information retrieval and machine learning. Our use\nof query variation was inspired by the work of YomTov et\nal. [20], Carpineto et al. [5], and Amati et al. [2], among\nothers. These studies use the idea of creating multiple \nsubqueries and then examining the nature of the overlap in the\ndocuments and/or expansion terms that result from each\nsubquery. Model combination is performed using heuristics.\nIn particular, the studies of Amati et al. and Carpineto et al.\ninvestigated combining terms from individual distributional\nmethods using a term-reranking combination heuristic. In\na set of TREC topics they found wide average variation in\nthe rank-distance of terms from different expansion \nmethods. Their combination method gave modest positive \nimprovements in average precision.\nThe idea of examining the overlap between lists of \nsuggested terms has also been used in early query expansion\napproaches. Xu and Croft\"s method of Local Context \nAnalysis (LCA) [19] includes a factor in the empirically-derived\nweighting formula that causes expansion terms to be \npreferred that have connections to multiple query terms.\nOn the document side, recent work by Zhou & Croft [21]\nexplored the idea of adding noise to documents, re-scoring\nthem, and using the stability of the resulting rankings as\nan estimate of query difficulty. This is related to our use\nof document sampling to estimate the risk of the feedback\nmodel built from the different sets of top-retrieved \ndocuments. Sakai et al. [17] proposed an approach to improving\nthe robustness of pseudo-relevance feedback using a method\nthey call selective sampling. The essence of their method\nis that they allow skipping of some top-ranked documents,\nbased on a clustering criterion, in order to select a more \nvaried and novel set of documents later in the ranking for use\nby a traditional pseudo-feedback method. Their study did\nnot find significant improvements in either robustness (RI)\nor MAP on their corpora.\nGreiff, Morgan and Ponte [8] explored the role of variance\nin term weighting. In a series of simulations that simplified\nthe problem to 2-feature documents, they found that average\nprecision degrades as term frequency variance - high \nnoiseincreases. Downweighting terms with high variance resulted\nin improved average precision. This seems in accord with\nour own findings for individual feedback models.\nEstimates of output variance have recently been used for\nimproved text classification. Lee et al. [11] used \nqueryspecific variance estimates of classifier outputs to perform\nimproved model combination. Instead of using sampling,\nthey were able to derive closed-form expressions for classifier\nvariance by assuming base classifiers using simple types of\ninference networks.\nAndo and Zhang proposed a method that they call \nstructural feedback [3] and showed how to apply it to query \nexpansion for the TREC Genomics Track. They used r query\nvariations to obtain R different sets Sr of top-ranked \ndocuments that have been intersected with the top-ranked \ndocuments obtained from the original query qorig. For each Si,\nthe normalized centroid vector \u00cb\u2020wi of the documents is \ncalculated. Principal component analysis (PCA) is then applied\nto the \u00cb\u2020wi to obtain the matrix \u00ce\u00a6 of H left singular vectors\n\u00cf\u2020h that are used to obtain the new, expanded query\nqexp = qorig + \u00ce\u00a6T\n\u00ce\u00a6qorig. (7)\nIn the case H = 1, we have a single left singular vector \u00cf\u2020:\nqexp = qorig + (\u00cf\u2020T\nqorig)\u00cf\u2020\nso that the dot product \u00cf\u2020T\nqorig is a type of dynamic weight\non the expanded query that is based on the similarity of the\noriginal query to the expanded query. The use of variance as\na feedback model quality measure occurs indirectly through\nthe application of PCA. It would be interesting to study\nthe connections between this approach and our own \nmodelfitting method.\nFinally, in language modeling approaches to feedback, Tao\nand Zhai [18] describe a method for more robust feedback\nthat allows each document to have a different feedback \u00ce\u00b1.\nThe feedback weights are derived automatically using \nregularized EM. A roughly equal balance of query and expansion\nmodel is implied by their EM stopping condition. They \npropose tailoring the stopping parameter \u00ce\u00b7 based on a function\nof some quality measure of feedback documents.\n5. CONCLUSIONS\nWe have presented a new approach to pseudo-relevance\nfeedback based on document and query sampling. The use\nof sampling is a very flexible and powerful device and is \nmotivated by our general desire to extend current models of \nretrieval by estimating the risk or variance associated with the\nparameters or output of retrieval processes. Such variance\nestimates, for example, may be naturally used in a Bayesian\nframework for improved model estimation and combination.\nApplications such as selective expansion may then be \nimplemented in a principled way.\nWhile our study uses the language modeling approach as a\nframework for experiments, we make few assumptions about\nthe actual workings of the feedback algorithm. We believe\nit is likely that any reasonably effective baseline feedback\nalgorithm would benefit from our approach. Our results on\nstandard TREC collections show that our framework \nimproves the robustness of a strong baseline feedback method\nacross a variety of collections, without sacrificing average\nprecision. It also gives small but consistent gains in \ntop10 precision. In future work, we envision an investigation\ninto how varying the set of sampling methods used and the\nnumber of samples controls the trade-off between \nrobustness, accuracy, and efficiency.\nAcknowledgements\nWe thank Paul Bennett for valuable discussions related to\nthis work, which was supported by NSF grants #IIS-0534345\nand #CNS-0454018, and U.S. Dept. of Education grant\n#R305G03123. Any opinions, findings, and conclusions or\nrecommendations expressed in this material are the authors.\nand do not necessarily reflect those of the sponsors.\n6. REFERENCES\n[1] The Lemur toolkit for language modeling and retrieval.\nhttp://www.lemurproject.org.\n[2] G. Amati, C. Carpineto, and G. Romano. Query difficulty,\nrobustness, and selective application of query expansion. In\nProc. of the 25th European Conf. on Information Retrieval\n(ECIR 2004), pages 127-137.\n[3] R. K. Ando and T. Zhang. A high-performance semi-supervised\nlearning method for text chunking. In Proc. of the 43rd\nAnnual Meeting of the ACL, pages 1-9, June 2005.\n[4] L. Breiman. Bagging predictors. Machine Learning,\n24(2):123-140, 1996.\n[5] C. Carpineto, G. Romano, and V. Giannini. Improving retrieval\nfeedback with multiple term-ranking function combination.\nACM Trans. Info. Systems, 20(3):259 - 290.\n[6] K. Collins-Thompson, P. Ogilvie, and J. Callan. Initial results\nwith structured queries and language models on half a terabyte\nof text. In Proc. of 2005 Text REtrieval Conference. NIST\nSpecial Publication.\n[7] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern\nClassification. Wiley and Sons, 2nd edition, 2001.\n[8] W. R. Greiff, W. T. Morgan, and J. M. Ponte. The role of\nvariance in term weighting for probabilistic information\nretrieval. In Proc. of the 11th Intl. Conf. on Info. and\nKnowledge Mgmt. (CIKM 2002), pages 252-259.\n[9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.\nSOMPAK: The self-organizing map program package. Technical\nReport A31, Helsinki University of Technology, 1996.\nhttp://www.cis.hut.fi/research/papers/som tr96.ps.Z.\n[10] V. Lavrenko. A Generative Theory of Relevance. PhD thesis,\nUniversity of Massachusetts, Amherst, 2004.\n[11] C.-H. Lee, R. Greiner, and S. Wang. Using query-specific\nvariance estimates to combine Bayesian classifiers. In Proc. of\nthe 23rd Intl. Conf. on Machine Learning (ICML 2006),\npages 529-536.\n[12] D. Metzler and W. B. Croft. Combining the language model\nand inference network approaches to retrieval. Info. Processing\nand Mgmt., 40(5):735-750, 2004.\n[13] T. Minka. Estimating a Dirichlet distribution. Technical report,\n2000. http://research.microsoft.com/ minka/papers/dirichlet.\n[14] J. Ponte. Advances in Information Retrieval, chapter\nLanguage models for relevance feedback, pages 73-96. 2000.\nW.B. Croft, ed.\n[15] J. M. Ponte and W. B. Croft. A language modeling approach to\ninformation retrieval. In Proc. of the 1998 ACM SIGIR\nConference on Research and Development in Information\nRetrieval, pages 275-281.\n[16] J. Rocchio. The SMART Retrieval System, chapter Relevance\nFeedback in Information Retrieval, pages 313-323.\nPrentice-Hall, 1971. G. Salton, ed.\n[17] T. Sakai, T. Manabe, and M. Koyama. Flexible\npseudo-relevance feedback via selective sampling. ACM\nTransactions on Asian Language Information Processing\n(TALIP), 4(2):111-135, 2005.\n[18] T. Tao and C. Zhai. Regularized estimation of mixture models\nfor robust pseudo-relevance feedback. In Proc. of the 2006\nACM SIGIR Conference on Research and Development in\nInformation Retrieval, pages 162-169.\n[19] J. Xu and W. B. Croft. Improving the effectiveness of\ninformation retrieval with local context analysis. ACM Trans.\nInf. Syst., 18(1):79-112, 2000.\n[20] E. YomTov, S. Fine, D. Carmel, and A. Darlow. Learning to\nestimate query difficulty. In Proc. of the 2005 ACM SIGIR\nConf. on Research and Development in Information\nRetrieval, pages 512-519.\n[21] Y. Zhou and W. B. Croft. Ranking robustness: a novel\nframework to predict query performance. In Proc. of the 15th\nACM Intl. Conf. on Information and Knowledge Mgmt.\n(CIKM 2006), pages 567-574.\n": ["feedback method", "posterior distribution", "enhanced feedback model", "information retrieval", "query expansion", "probability distribution", "pseudo-relevance feedback", "vector space-based algorithm", "risk", "feedback model", "estimating uncertainty", "language modeling", "feedback distribution", ""]}