{"The Impact of Caching on Search Engines\nRicardo Baeza-Yates1\nrbaeza@acm.org\nAristides Gionis1\ngionis@yahoo-inc.com\nFlavio Junqueira1\nfpj@yahoo-inc.com\nVanessa Murdock1\nvmurdock@yahoo-inc.com\nVassilis Plachouras1\nvassilis@yahoo-inc.com\nFabrizio Silvestri2\nf.silvestri@isti.cnr.it\n1\nYahoo! Research Barcelona 2\nISTI - CNR\nBarcelona, SPAIN Pisa, ITALY\nABSTRACT\nIn this paper we study the trade-offs in designing efficient\ncaching systems for Web search engines. We explore the\nimpact of different approaches, such as static vs. dynamic\ncaching, and caching query results vs. caching posting lists.\nUsing a query log spanning a whole year we explore the \nlimitations of caching and we demonstrate that caching posting\nlists can achieve higher hit rates than caching query \nanswers. We propose a new algorithm for static caching of\nposting lists, which outperforms previous methods. We also\nstudy the problem of finding the optimal way to split the\nstatic cache between answers and posting lists. Finally, we\nmeasure how the changes in the query log affect the \neffectiveness of static caching, given our observation that the\ndistribution of the queries changes slowly over time. Our\nresults and observations are applicable to different levels of\nthe data-access hierarchy, for instance, for a memory/disk\nlayer or a broker/remote server layer.\nCategories and Subject Descriptors\nH.3.3 [Information Storage and Retrieval]: Information\nSearch and Retrieval - Search process; H.3.4 [Information\nStorage and Retrieval]: Systems and Software - \nDistributed systems, Performance evaluation (efficiency and \neffectiveness)\nGeneral Terms\nAlgorithms, Experimentation\n1. INTRODUCTION\nMillions of queries are submitted daily to Web search \nengines, and users have high expectations of the quality and\nspeed of the answers. As the searchable Web becomes larger\nand larger, with more than 20 billion pages to index, \nevaluating a single query requires processing large amounts of\ndata. In such a setting, to achieve a fast response time and\nto increase the query throughput, using a cache is crucial.\nThe primary use of a cache memory is to speedup \ncomputation by exploiting frequently or recently used data, \nalthough reducing the workload to back-end servers is also a\nmajor goal. Caching can be applied at different levels with\nincreasing response latencies or processing requirements. For\nexample, the different levels may correspond to the main\nmemory, the disk, or resources in a local or a wide area\nnetwork.\nThe decision of what to cache is either off-line (static)\nor online (dynamic). A static cache is based on historical\ninformation and is periodically updated. A dynamic cache\nreplaces entries according to the sequence of requests. When\na new request arrives, the cache system decides whether to\nevict some entry from the cache in the case of a cache miss.\nSuch online decisions are based on a cache policy, and several\ndifferent policies have been studied in the past.\nFor a search engine, there are two possible ways to use a\ncache memory:\nCaching answers: As the engine returns answers to a \nparticular query, it may decide to store these answers to\nresolve future queries.\nCaching terms: As the engine evaluates a particular query,\nit may decide to store in memory the posting lists of\nthe involved query terms. Often the whole set of \nposting lists does not fit in memory, and consequently, the\nengine has to select a small set to keep in memory and\nspeed up query processing.\nReturning an answer to a query that already exists in\nthe cache is more efficient than computing the answer using\ncached posting lists. On the other hand, previously unseen\nqueries occur more often than previously unseen terms, \nimplying a higher miss rate for cached answers.\nCaching of posting lists has additional challenges. As\nposting lists have variable size, caching them dynamically\nis not very efficient, due to the complexity in terms of \nefficiency and space, and the skewed distribution of the query\nstream, as shown later. Static caching of posting lists poses\neven more challenges: when deciding which terms to cache\none faces the trade-off between frequently queried terms and\nterms with small posting lists that are space efficient. \nFinally, before deciding to adopt a static caching policy the\nquery stream should be analyzed to verify that its \ncharacteristics do not change rapidly over time.\nBroker\nStatic caching\nposting lists\nDynamic/Static\ncached answers\nLocal query processor\nDisk\nNext caching level\nLocal network access\nRemote network access\nFigure 1: One caching level in a distributed search\narchitecture.\nIn this paper we explore the trade-offs in the design of each\ncache level, showing that the problem is the same and only\na few parameters change. In general, we assume that each\nlevel of caching in a distributed search architecture is similar\nto that shown in Figure 1. We use a query log spanning a\nwhole year to explore the limitations of dynamically caching\nquery answers or posting lists for query terms.\nMore concretely, our main conclusions are that:\n\u00e2\u20ac\u00a2 Caching query answers results in lower hit ratios \ncompared to caching of posting lists for query terms, but\nit is faster because there is no need for query \nevaluation. We provide a framework for the analysis of the\ntrade-off between static caching of query answers and\nposting lists;\n\u00e2\u20ac\u00a2 Static caching of terms can be more effective than \ndynamic caching with, for example, LRU. We provide\nalgorithms based on the Knapsack problem for \nselecting the posting lists to put in a static cache, and\nwe show improvements over previous work, achieving\na hit ratio over 90%;\n\u00e2\u20ac\u00a2 Changes of the query distribution over time have little\nimpact on static caching.\nThe remainder of this paper is organized as follows. \nSections 2 and 3 summarize related work and characterize the\ndata sets we use. Section 4 discusses the limitations of \ndynamic caching. Sections 5 and 6 introduce algorithms for\ncaching posting lists, and a theoretical framework for the\nanalysis of static caching, respectively. Section 7 discusses\nthe impact of changes in the query distribution on static\ncaching, and Section 8 provides concluding remarks.\n2. RELATED WORK\nThere is a large body of work devoted to query \noptimization. Buckley and Lewit [3], in one of the earliest works,\ntake a term-at-a-time approach to deciding when inverted\nlists need not be further examined. More recent examples\ndemonstrate that the top k documents for a query can be\nreturned without the need for evaluating the complete set\nof posting lists [1, 4, 15]. Although these approaches seek to\nimprove query processing efficiency, they differ from our \ncurrent work in that they do not consider caching. They may\nbe considered separate and complementary to a cache-based\napproach.\nRaghavan and Sever [12], in one of the first papers on \nexploiting user query history, propose using a query base, built\nupon a set of persistent optimal queries submitted in the\npast, to improve the retrieval effectiveness for similar future\nqueries. Markatos [10] shows the existence of temporal \nlocality in queries, and compares the performance of different\ncaching policies. Based on the observations of Markatos,\nLempel and Moran propose a new caching policy, called\nProbabilistic Driven Caching, by attempting to estimate the\nprobability distribution of all possible queries submitted to\na search engine [8]. Fagni et al. follow Markatos\" work by\nshowing that combining static and dynamic caching policies\ntogether with an adaptive prefetching policy achieves a high\nhit ratio [7]. Different from our work, they consider caching\nand prefetching of pages of results.\nAs systems are often hierarchical, there has also been some\neffort on multi-level architectures. Saraiva et al. propose a\nnew architecture for Web search engines using a two-level\ndynamic caching system [13]. Their goal for such systems\nhas been to improve response time for hierarchical engines.\nIn their architecture, both levels use an LRU eviction \npolicy. They find that the second-level cache can effectively\nreduce disk traffic, thus increasing the overall throughput.\nBaeza-Yates and Saint-Jean propose a three-level index \norganization [2]. Long and Suel propose a caching system\nstructured according to three different levels [9]. The \nintermediate level contains frequently occurring pairs of terms\nand stores the intersections of the corresponding inverted\nlists. These last two papers are related to ours in that they\nexploit different caching strategies at different levels of the\nmemory hierarchy.\nFinally, our static caching algorithm for posting lists in\nSection 5 uses the ratio frequency/size in order to evaluate\nthe goodness of an item to cache. Similar ideas have been\nused in the context of file caching [17], Web caching [5], and\neven caching of posting lists [9], but in all cases in a dynamic\nsetting. To the best of our knowledge we are the first to use\nthis approach for static caching of posting lists.\n3. DATA CHARACTERIZATION\nOur data consists of a crawl of documents from the UK\ndomain, and query logs of one year of queries submitted to\nhttp://www.yahoo.co.uk from November 2005 to November\n2006. In our logs, 50% of the total volume of queries are\nunique. The average query length is 2.5 terms, with the\nlongest query having 731 terms.\n1e-07\n1e-06\n1e-05\n1e-04\n0.001\n0.01\n0.1\n1\n1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1\nFrequency(normalized)\nFrequency rank (normalized)\nFigure 2: The distribution of queries (bottom curve)\nand query terms (middle curve) in the query log.\nThe distribution of document frequencies of terms\nin the UK-2006 dataset (upper curve).\nFigure 2 shows the distributions of queries (lower curve),\nand query terms (middle curve). The x-axis represents the\nnormalized frequency rank of the query or term. (The most\nfrequent query appears closest to the y-axis.) The y-axis is\nTable 1: Statistics of the UK-2006 sample.\nUK-2006 sample statistics\n# of documents 2,786,391\n# of terms 6,491,374\n# of tokens 2,109,512,558\nthe normalized frequency for a given query (or term). As \nexpected, the distribution of query frequencies and query term\nfrequencies follow power law distributions, with slope of 1.84\nand 2.26, respectively. In this figure, the query frequencies\nwere computed as they appear in the logs with no \nnormalization for case or white space. The query terms (middle\ncurve) have been normalized for case, as have the terms in\nthe document collection.\nThe document collection that we use for our experiments\nis a summary of the UK domain crawled in May 2006.1\nThis\nsummary corresponds to a maximum of 400 crawled \ndocuments per host, using a breadth first crawling strategy, \ncomprising 15GB. The distribution of document frequencies of\nterms in the collection follows a power law distribution with\nslope 2.38 (upper curve in Figure 2). The statistics of the\ncollection are shown in Table 1. We measured the correlation\nbetween the document frequency of terms in the collection\nand the number of queries that contain a particular term in\nthe query log to be 0.424. A scatter plot for a random \nsample of terms is shown in Figure 3. In this experiment, terms\nhave been converted to lower case in both the queries and\nthe documents so that the frequencies will be comparable.\n1e-07\n1e-06\n1e-05\n1e-04\n0.001\n0.01\n0.1\n1\n1e-06 1e-05 1e-04 0.001 0.01 0.1 1\nQueryfrequency\nDocument frequency\nFigure 3: Normalized scatter plot of document-term\nfrequencies vs. query-term frequencies.\n4. CACHING OF QUERIES AND TERMS\nCaching relies upon the assumption that there is locality\nin the stream of requests. That is, there must be sufficient\nrepetition in the stream of requests and within intervals of\ntime that enable a cache memory of reasonable size to be\neffective. In the query log we used, 88% of the unique queries\nare singleton queries, and 44% are singleton queries out of\nthe whole volume. Thus, out of all queries in the stream\ncomposing the query log, the upper threshold on hit ratio is\n56%. This is because only 56% of all the queries comprise\nqueries that have multiple occurrences. It is important to\nobserve, however, that not all queries in this 56% can be\ncache hits because of compulsory misses. A compulsory miss\n1\nThe collection is available from the University of Milan:\nhttp://law.dsi.unimi.it/. URL retrieved 05/2007.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n240 260 280 300 320 340 360\nNumberofelements\nBin number\nTotal terms\nTerms diff\nTotal queries\nUnique queries\nUnique terms\nQuery diff\nFigure 4: Arrival rate for both terms and queries.\nhappens when the cache receives a query for the first time.\nThis is different from capacity misses, which happen due to\nspace constraints on the amount of memory the cache uses.\nIf we consider a cache with infinite memory, then the hit\nratio is 50%. Note that for an infinite cache there are no\ncapacity misses.\nAs we mentioned before, another possibility is to cache the\nposting lists of terms. Intuitively, this gives more freedom\nin the utilization of the cache content to respond to queries\nbecause cached terms might form a new query. On the other\nhand, they need more space.\nAs opposed to queries, the fraction of singleton terms in\nthe total volume of terms is smaller. In our query log, only\n4% of the terms appear once, but this accounts for 73% of\nthe vocabulary of query terms. We show in Section 5 that\ncaching a small fraction of terms, while accounting for terms\nappearing in many documents, is potentially very effective.\nFigure 4 shows several graphs corresponding to the \nnormalized arrival rate for different cases using days as bins.\nThat is, we plot the normalized number of elements that\nappear in a day. This graph shows only a period of 122\ndays, and we normalize the values by the maximum value\nobserved throughout the whole period of the query log. \nTotal queries and Total terms correspond to the total \nvolume of queries and terms, respectively. Unique queries\nand Unique terms correspond to the arrival rate of unique\nqueries and terms. Finally, Query diff and Terms diff\ncorrespond to the difference between the curves for total and\nunique.\nIn Figure 4, as expected, the volume of terms is much\nhigher than the volume of queries. The difference between\nthe total number of terms and the number of unique terms is\nmuch larger than the difference between the total number of\nqueries and the number of unique queries. This observation\nimplies that terms repeat significantly more than queries. If\nwe use smaller bins, say of one hour, then the ratio of unique\nto volume is higher for both terms and queries because it\nleaves less room for repetition.\nWe also estimated the workload using the document \nfrequency of terms as a measure of how much work a query\nimposes on a search engine. We found that it follows closely\nthe arrival rate for terms shown in Figure 4.\nTo demonstrate the effect of a dynamic cache on the query\nfrequency distribution of Figure 2, we plot the same \nfrequency graph, but now considering the frequency of queries\nFigure 5: Frequency graph after LRU cache.\nafter going through an LRU cache. On a cache miss, an\nLRU cache decides upon an entry to evict using the \ninformation on the recency of queries. In this graph, the most\nfrequent queries are not the same queries that were most\nfrequent before the cache. It is possible that queries that\nare most frequent after the cache have different \ncharacteristics, and tuning the search engine to queries frequent before\nthe cache may degrade performance for non-cached queries.\nThe maximum frequency after caching is less than 1% of\nthe maximum frequency before the cache, thus showing that\nthe cache is very effective in reducing the load of frequent\nqueries. If we re-rank the queries according to after-cache\nfrequency, the distribution is still a power law, but with a\nmuch smaller value for the highest frequency.\nWhen discussing the effectiveness of dynamically caching,\nan important metric is cache miss rate. To analyze the cache\nmiss rate for different memory constraints, we use the \nworking set model [6, 14]. A working set, informally, is the set\nof references that an application or an operating system is\ncurrently working with. The model uses such sets in a \nstrategy that tries to capture the temporal locality of references.\nThe working set strategy then consists in keeping in memory\nonly the elements that are referenced in the previous \u00ce\u00b8 steps\nof the input sequence, where \u00ce\u00b8 is a configurable parameter\ncorresponding to the window size.\nOriginally, working sets have been used for page \nreplacement algorithms of operating systems, and considering such\na strategy in the context of search engines is interesting for\nthree reasons. First, it captures the amount of locality of\nqueries and terms in a sequence of queries. Locality in this\ncase refers to the frequency of queries and terms in a window\nof time. If many queries appear multiple times in a window,\nthen locality is high. Second, it enables an o\u00ef\u00ac\u201eine analysis of\nthe expected miss rate given different memory constraints.\nThird, working sets capture aspects of efficient caching \nalgorithms such as LRU. LRU assumes that references farther\nin the past are less likely to be referenced in the present,\nwhich is implicit in the concept of working sets [14].\nFigure 6 plots the miss rate for different working set sizes,\nand we consider working sets of both queries and terms. The\nworking set sizes are normalized against the total number\nof queries in the query log. In the graph for queries, there\nis a sharp decay until approximately 0.01, and the rate at\nwhich the miss rate drops decreases as we increase the size\nof the working set over 0.01. Finally, the minimum value it\nreaches is 50% miss rate, not shown in the figure as we have\ncut the tail of the curve for presentation purposes.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 0.05 0.1 0.15 0.2\nMissrate\nNormalized working set size\nQueries\nTerms\nFigure 6: Miss rate as a function of the working set\nsize.\n1 10 100 1000 10000 100000 1e+06\nFrequency\nDistance\nFigure 7: Distribution of distances expressed in\nterms of distinct queries.\nCompared to the query curve, we observe that the \nminimum miss rate for terms is substantially smaller. The miss\nrate also drops sharply on values up to 0.01, and it decreases\nminimally for higher values. The minimum value, however,\nis slightly over 10%, which is much smaller than the \nminimum value for the sequence of queries. This implies that\nwith such a policy it is possible to achieve over 80% hit rate,\nif we consider caching dynamically posting lists for terms as\nopposed to caching answers for queries. This result does\nnot consider the space required for each unit stored in the\ncache memory, or the amount of time it takes to put \ntogether a response to a user query. We analyze these issues\nmore carefully later in this paper.\nIt is interesting also to observe the histogram of Figure 7,\nwhich is an intermediate step in the computation of the miss\nrate graph. It reports the distribution of distances between\nrepetitions of the same frequent query. The distance in the\nplot is measured in the number of distinct queries \nseparating a query and its repetition, and it considers only queries\nappearing at least 10 times. From Figures 6 and 7, we \nconclude that even if we set the size of the query answers cache\nto a relatively large number of entries, the miss rate is high.\nThus, caching the posting lists of terms has the potential to\nimprove the hit ratio. This is what we explore next.\n5. CACHING POSTING LISTS\nThe previous section shows that caching posting lists can\nobtain a higher hit rate compared to caching query answers.\nIn this section we study the problem of how to select \nposting lists to place on a certain amount of available memory,\nassuming that the whole index is larger than the amount of\nmemory available. The posting lists have variable size (in\nfact, their size distribution follows a power law), so it is \nbeneficial for a caching policy to consider the sizes of the posting\nlists. We consider both dynamic and static caching. For \ndynamic caching, we use two well-known policies, LRU and\nLFU, as well as a modified algorithm that takes posting-list\nsize into account.\nBefore discussing the static caching strategies, we \nintroduce some notation. We use fq(t) to denote the query-term\nfrequency of a term t, that is, the number of queries \ncontaining t in the query log, and fd(t) to denote the document\nfrequency of t, that is, the number of documents in the \ncollection in which the term t appears.\nThe first strategy we consider is the algorithm proposed by\nBaeza-Yates and Saint-Jean [2], which consists in selecting\nthe posting lists of the terms with the highest query-term\nfrequencies fq(t). We call this algorithm Qtf.\nWe observe that there is a trade-off between fq(t) and\nfd(t). Terms with high fq(t) are useful to keep in the cache\nbecause they are queried often. On the other hand, terms\nwith high fd(t) are not good candidates because they \ncorrespond to long posting lists and consume a substantial\namount of space. In fact, the problem of selecting the best\nposting lists for the static cache corresponds to the \nstandard Knapsack problem: given a knapsack of fixed \ncapacity, and a set of n items, such as the i-th item has value ci\nand size si, select the set of items that fit in the knapsack\nand maximize the overall value. In our case, value \ncorresponds to fq(t) and size corresponds to fd(t). Thus, we\nemploy a simple algorithm for the knapsack problem, which\nis selecting the posting lists of the terms with the highest\nvalues of the ratio\nfq(t)\nfd(t)\n. We call this algorithm QtfDf. We\ntried other variations considering query frequencies instead\nof term frequencies, but the gain was minimal compared to\nthe complexity added.\nIn addition to the above two static algorithms we consider\nthe following algorithms for dynamic caching:\n\u00e2\u20ac\u00a2 LRU: A standard LRU algorithm, but many posting\nlists might need to be evicted (in order of least-recent\nusage) until there is enough space in the memory to\nplace the currently accessed posting list;\n\u00e2\u20ac\u00a2 LFU: A standard LFU algorithm (eviction of the \nleastfrequently used), with the same modification as the\nLRU;\n\u00e2\u20ac\u00a2 Dyn-QtfDf: A dynamic version of the QtfDf \nalgorithm; evict from the cache the term(s) with the lowest\nfq(t)\nfd(t)\nratio.\nThe performance of all the above algorithms for 15 weeks\nof the query log and the UK dataset are shown in Figure 8.\nPerformance is measured with hit rate. The cache size is\nmeasured as a fraction of the total space required to store\nthe posting lists of all terms.\nFor the dynamic algorithms, we load the cache with terms\nin order of fq(t) and we let the cache warm up for 1 \nmillion queries. For the static algorithms, we assume complete\nknowledge of the frequencies fq(t), that is, we estimate fq(t)\nfrom the whole query stream. As we show in Section 7 the\nresults do not change much if we compute the query-term\nfrequencies using the first 3 or 4 weeks of the query log and\nmeasure the hit rate on the rest.\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1 0.2 0.3 0.4 0.5 0.6 0.7\nHitrate\nCache size\nCaching posting lists\nstatic QTF/DF\nLRU\nLFU\nDyn-QTF/DF\nQTF\nFigure 8: Hit rate of different strategies for caching\nposting lists.\nThe most important observation from our experiments is\nthat the static QtfDf algorithm has a better hit rate than\nall the dynamic algorithms. An important benefit a static\ncache is that it requires no eviction and it is hence more\nefficient when evaluating queries. However, if the \ncharacteristics of the query traffic change frequently over time, then\nit requires re-populating the cache often or there will be a\nsignificant impact on hit rate.\n6. ANALYSIS OF STATIC CACHING\nIn this section we provide a detailed analysis for the \nproblem of deciding whether it is preferable to cache query \nanswers or cache posting lists. Our analysis takes into account\nthe impact of caching between two levels of the data-access\nhierarchy. It can either be applied at the memory/disk layer\nor at a server/remote server layer as in the architecture we\ndiscussed in the introduction.\nUsing a particular system model, we obtain estimates for\nthe parameters required by our analysis, which we \nsubsequently use to decide the optimal trade-off between caching\nquery answers and caching posting lists.\n6.1 Analytical Model\nLet M be the size of the cache measured in answer units\n(the cache can store M query answers). Assume that all\nposting lists are of the same length L, measured in answer\nunits. We consider the following two cases: (A) a cache\nthat stores only precomputed answers, and (B) a cache that\nstores only posting lists. In the first case, Nc = M answers\nfit in the cache, while in the second case Np = M/L posting\nlists fit in the cache. Thus, Np = Nc/L. Note that although\nposting lists require more space, we can combine terms to\nevaluate more queries (or partial queries).\nFor case (A), suppose that a query answer in the cache\ncan be evaluated in 1 time unit. For case (B), assume that\nif the posting lists of the terms of a query are in the cache\nthen the results can be computed in TR1 time units, while\nif the posting lists are not in the cache then the results can\nbe computed in TR2 time units. Of course TR2 > TR1.\nNow we want to compare the time to answer a stream of\nQ queries in both cases. Let Vc(Nc) be the volume of the\nmost frequent Nc queries. Then, for case (A), we have an\noverall time\nTCA = Vc(Nc) + TR2(Q \u00e2\u02c6\u2019 Vc(Nc)).\nSimilarly, for case (B), let Vp(Np) be the number of \ncomputable queries. Then we have overall time\nTP L = TR1Vp(Np) + TR2(Q \u00e2\u02c6\u2019 Vp(Np)).\nWe want to check under which conditions we have TP L <\nTCA. We have\nTP L \u00e2\u02c6\u2019 TCA = (TR2 \u00e2\u02c6\u2019 1)Vc(Nc) \u00e2\u02c6\u2019 (TR2 \u00e2\u02c6\u2019 TR1)Vp(Np) > 0.\nFigure 9 shows the values of Vp and Vc for our data. We can\nsee that caching answers saturates faster and for this \nparticular data there is no additional benefit from using more\nthan 10% of the index space for caching answers.\nAs the query distribution is a power law with parameter\n\u00ce\u00b1 > 1, the i-th most frequent query appears with probability\nproportional to 1\ni\u00ce\u00b1 . Therefore, the volume Vc(n), which is\nthe total number of the n most frequent queries, is\nVc(n) = V0\nn\ni=1\nQ\ni\u00ce\u00b1\n= \u00ce\u00b3nQ (0 < \u00ce\u00b3n < 1).\nWe know that Vp(n) grows faster than Vc(n) and assume,\nbased on experimental results, that the relation is of the\nform Vp(n) = k Vc(n)\u00ce\u00b2\n.\nIn the worst case, for a large cache, \u00ce\u00b2 \u00e2\u2020\u2019 1. That is, both\ntechniques will cache a constant fraction of the overall query\nvolume. Then caching posting lists makes sense only if\nL(TR2 \u00e2\u02c6\u2019 1)\nk(TR2 \u00e2\u02c6\u2019 TR1)\n> 1.\nIf we use compression, we have L < L and TR1 > TR1. \nAccording to the experiments that we show later, compression\nis always better.\nFor a small cache, we are interested in the transient \nbehavior and then \u00ce\u00b2 > 1, as computed from our data. In this\ncase there will always be a point where TP L > TCA for a\nlarge number of queries.\nIn reality, instead of filling the cache only with answers or\nonly with posting lists, a better strategy will be to divide\nthe total cache space into cache for answers and cache for\nposting lists. In such a case, there will be some queries that\ncould be answered by both parts of the cache. As the answer\ncache is faster, it will be the first choice for answering those\nqueries. Let QNc and QNp be the set of queries that can\nbe answered by the cached answers and the cached posting\nlists, respectively. Then, the overall time is\nT = Vc(Nc)+TR1V (QNp \u00e2\u02c6\u2019QNc )+TR2(Q\u00e2\u02c6\u2019V (QNp \u00e2\u02c6\u00aaQNc )),\nwhere Np = (M \u00e2\u02c6\u2019 Nc)/L. Finding the optimal division of\nthe cache in order to minimize the overall retrieval time is a\ndifficult problem to solve analytically. In Section 6.3 we use\nsimulations to derive optimal cache trade-offs for particular\nimplementation examples.\n6.2 Parameter Estimation\nWe now use a particular implementation of a centralized\nsystem and the model of a distributed system as examples\nfrom which we estimate the parameters of the analysis from\nthe previous section. We perform the experiments using\nan optimized version of Terrier [11] for both indexing \ndocuments and processing queries, on a single machine with a\nPentium 4 at 2GHz and 1GB of RAM.\nWe indexed the documents from the UK-2006 dataset,\nwithout removing stop words or applying stemming. The\nposting lists in the inverted file consist of pairs of \ndocument identifier and term frequency. We compress the \ndocument identifier gaps using Elias gamma encoding, and the\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nQueryvolume\nSpace\nprecomputed answers\nposting lists\nFigure 9: Cache saturation as a function of size.\nTable 2: Ratios between the average time to \nevaluate a query and the average time to return cached\nanswers (centralized and distributed case).\nCentralized system TR1 TR2 TR1 TR2\nFull evaluation 233 1760 707 1140\nPartial evaluation 99 1626 493 798\nLAN system TRL\n1 TRL\n2 TR L\n1 TR L\n2\nFull evaluation 242 1769 716 1149\nPartial evaluation 108 1635 502 807\nWAN system TRW\n1 TRW\n2 TR W\n1 TR W\n2\nFull evaluation 5001 6528 5475 5908\nPartial evaluation 4867 6394 5270 5575\nterm frequencies in documents using unary encoding [16].\nThe size of the inverted file is 1,189Mb. A stored answer\nrequires 1264 bytes, and an uncompressed posting takes 8\nbytes. From Table 1, we obtain L = (8\u00c2\u00b7# of postings)\n1264\u00c2\u00b7# of terms\n= 0.75\nand L = Inverted file size\n1264\u00c2\u00b7# of terms\n= 0.26.\nWe estimate the ratio TR = T/Tc between the average\ntime T it takes to evaluate a query and the average time\nTc it takes to return a stored answer for the same query, in\nthe following way. Tc is measured by loading the answers\nfor 100,000 queries in memory, and answering the queries\nfrom memory. The average time is Tc = 0.069ms. T is\nmeasured by processing the same 100,000 queries (the first\n10,000 queries are used to warm-up the system). For each\nquery, we remove stop words, if there are at least three \nremaining terms. The stop words correspond to the terms\nwith a frequency higher than the number of documents in\nthe index. We use a document-at-a-time approach to \nretrieve documents containing all query terms. The only disk\naccess required during query processing is for reading \ncompressed posting lists from the inverted file. We perform both\nfull and partial evaluation of answers, because some queries\nare likely to retrieve a large number of documents, and only\na fraction of the retrieved documents will be seen by users.\nIn the partial evaluation of queries, we terminate the \nprocessing after matching 10,000 documents. The estimated\nratios TR are presented in Table 2.\nFigure 10 shows for a sample of queries the workload of\nthe system with partial query evaluation and compressed\nposting lists. The x-axis corresponds to the total time the\nsystem spends processing a particular query, and the \nvertical axis corresponds to the sum t\u00e2\u02c6\u02c6q fq \u00c2\u00b7 fd(t). Notice\nthat the total number of postings of the query-terms does\nnot necessarily provide an accurate estimate of the workload\nimposed on the system by a query (which is the case for full\nevaluation and uncompressed lists).\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.2 0.4 0.6 0.8 1\nTotalpostingstoprocessquery(normalized)\nTotal time to process query (normalized)\nPartial processing of compressed postings\nquery len = 1\nquery len in [2,3]\nquery len in [4,8]\nquery len > 8\nFigure 10: Workload for partial query evaluation\nwith compressed posting lists.\nThe analysis of the previous section also applies to a \ndistributed retrieval system in one or multiple sites. Suppose\nthat a document partitioned distributed system is running\non a cluster of machines interconnected with a local area\nnetwork (LAN) in one site. The broker receives queries and\nbroadcasts them to the query processors, which answer the\nqueries and return the results to the broker. Finally, the\nbroker merges the received answers and generates the final\nset of answers (we assume that the time spent on \nmerging results is negligible). The difference between the \ncentralized architecture and the document partition \narchitecture is the extra communication between the broker and\nthe query processors. Using ICMP pings on a 100Mbps\nLAN, we have measured that sending the query from the\nbroker to the query processors which send an answer of 4,000\nbytes back to the broker takes on average 0.615ms. Hence,\nTRL\n= TR + 0.615ms/0.069ms = TR + 9.\nIn the case when the broker and the query processors\nare in different sites connected with a wide area network\n(WAN), we estimated that broadcasting the query from the\nbroker to the query processors and getting back an answer\nof 4,000 bytes takes on average 329ms. Hence, TRW\n=\nTR + 329ms/0.069ms = TR + 4768.\n6.3 Simulation Results\nWe now address the problem of finding the optimal \ntradeoff between caching query answers and caching posting lists.\nTo make the problem concrete we assume a fixed budget M\non the available memory, out of which x units are used for\ncaching query answers and M \u00e2\u02c6\u2019 x for caching posting lists.\nWe perform simulations and compute the average response\ntime as a function of x. Using a part of the query log as\ntraining data, we first allocate in the cache the answers to\nthe most frequent queries that fit in space x, and then we\nuse the rest of the memory to cache posting lists. For \nselecting posting lists we use the QtfDf algorithm, applied to\nthe training query log but excluding the queries that have\nalready been cached.\nIn Figure 11, we plot the simulated response time for a\ncentralized system as a function of x. For the uncompressed\nindex we use M = 1GB, and for the compressed index we\nuse M = 0.5GB. In the case of the configuration that uses\npartial query evaluation with compressed posting lists, the\nlowest response time is achieved when 0.15GB out of the\n0.5GB is allocated for storing answers for queries. We \nobtained similar trends in the results for the LAN setting.\nFigure 12 shows the simulated workload for a distributed\nsystem across a WAN. In this case, the total amount of\nmemory is split between the broker, which holds the cached\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n0 0.2 0.4 0.6 0.8 1\nAverageresponsetime\nSpace (GB)\nSimulated workload -- single machine\nfull / uncompr / 1 G\npartial / uncompr / 1 G\nfull / compr / 0.5 G\npartial / compr / 0.5 G\nFigure 11: Optimal division of the cache in a server.\n3000\n3500\n4000\n4500\n5000\n5500\n6000\n0 0.2 0.4 0.6 0.8 1\nAverageresponsetime\nSpace (GB)\nSimulated workload -- WAN\nfull / uncompr / 1 G\npartial / uncompr / 1 G\nfull / compr / 0.5 G\npartial / compr / 0.5 G\nFigure 12: Optimal division of the cache when the\nnext level requires WAN access.\nanswers of queries, and the query processors, which hold\nthe cache of posting lists. According to the figure, the \ndifference between the configurations of the query processors\nis less important because the network communication \noverhead increases the response time substantially. When using\nuncompressed posting lists, the optimal allocation of \nmemory corresponds to using approximately 70% of the memory\nfor caching query answers. This is explained by the fact that\nthere is no need for network communication when the query\ncan be answered by the cache at the broker.\n7. EFFECT OF THE QUERY DYNAMICS\nFor our query log, the query distribution and query-term\ndistribution change slowly over time. To support this claim,\nwe first assess how topics change comparing the distribution\nof queries from the first week in June, 2006, to the \ndistribution of queries for the remainder of 2006 that did not appear\nin the first week in June. We found that a very small \npercentage of queries are new queries. The majority of queries\nthat appear in a given week repeat in the following weeks\nfor the next six months.\nWe then compute the hit rate of a static cache of 128, 000\nanswers trained over a period of two weeks (Figure 13). We\nreport hit rate hourly for 7 days, starting from 5pm. We\nobserve that the hit rate reaches its highest value during the\nnight (around midnight), whereas around 2-3pm it reaches\nits minimum. After a small decay in hit rate values, the hit\nrate stabilizes between 0.28, and 0.34 for the entire week,\nsuggesting that the static cache is effective for a whole week\nafter the training period.\n0.26\n0.27\n0.28\n0.29\n0.3\n0.31\n0.32\n0.33\n0.34\n0.35\n0.36\n0.37\n0 20 40 60 80 100 120 140 160\nHit-rate\nTime\nHits on the frequent queries of distances\nFigure 13: Hourly hit rate for a static cache holding\n128,000 answers during the period of a week.\nThe static cache of posting lists can be periodically \nrecomputed. To estimate the time interval in which we need\nto recompute the posting lists on the static cache we need\nto consider an efficiency/quality trade-off: using too short\na time interval might be prohibitively expensive, while \nrecomputing the cache too infrequently might lead to having\nan obsolete cache not corresponding to the statistical \ncharacteristics of the current query stream.\nWe measured the effect on the QtfDf algorithm of the\nchanges in a 15-week query stream (Figure 14). We compute\nthe query term frequencies over the whole stream, select\nwhich terms to cache, and then compute the hit rate on the\nwhole query stream. This hit rate is as an upper bound, and\nit assumes perfect knowledge of the query term frequencies.\nTo simulate a realistic scenario, we use the first 6 (3) weeks\nof the query stream for computing query term frequencies\nand the following 9 (12) weeks to estimate the hit rate. As\nFigure 14 shows, the hit rate decreases by less than 2%. The\nhigh correlation among the query term frequencies during\ndifferent time periods explains the graceful adaptation of\nthe static caching algorithms to the future query stream.\nIndeed, the pairwise correlation among all possible 3-week\nperiods of the 15-week query stream is over 99.5%.\n8. CONCLUSIONS\nCaching is an effective technique in search engines for\nimproving response time, reducing the load on query \nprocessors, and improving network bandwidth utilization. We\npresent results on both dynamic and static caching. \nDynamic caching of queries has limited effectiveness due to the\nhigh number of compulsory misses caused by the number\nof unique or infrequent queries. Our results show that in\nour UK log, the minimum miss rate is 50% using a working\nset strategy. Caching terms is more effective with respect to\nmiss rate, achieving values as low as 12%. We also propose a\nnew algorithm for static caching of posting lists that \noutperforms previous static caching algorithms as well as dynamic\nalgorithms such as LRU and LFU, obtaining hit rate values\nthat are over 10% higher compared these strategies.\nWe present a framework for the analysis of the trade-off\nbetween caching query results and caching posting lists, and\nwe simulate different types of architectures. Our results\nshow that for centralized and LAN environments, there is\nan optimal allocation of caching query results and caching\nof posting lists, while for WAN scenarios in which network\ntime prevails it is more important to cache query results.\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n0.1 0.2 0.3 0.4 0.5 0.6 0.7\nHitrate\nCache size\nDynamics of static QTF/DF caching policy\nperfect knowledge\n6-week training\n3-week training\nFigure 14: Impact of distribution changes on the\nstatic caching of posting lists.\n9. REFERENCES\n[1] V. N. Anh and A. Moffat. Pruned query evaluation using\npre-computed impacts. In ACM CIKM, 2006.\n[2] R. A. Baeza-Yates and F. Saint-Jean. A three level search\nengine index based in query log distribution. In SPIRE,\n2003.\n[3] C. Buckley and A. F. Lewit. Optimization of inverted\nvector searches. In ACM SIGIR, 1985.\n[4] S. B\u00c2\u00a8uttcher and C. L. A. Clarke. A document-centric\napproach to static index pruning in text retrieval systems.\nIn ACM CIKM, 2006.\n[5] P. Cao and S. Irani. Cost-aware WWW proxy caching\nalgorithms. In USITS, 1997.\n[6] P. Denning. Working sets past and present. IEEE Trans.\non Software Engineering, SE-6(1):64-84, 1980.\n[7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando. Boosting\nthe performance of web search engines: Caching and\nprefetching query results by exploiting historical usage\ndata. ACM Trans. Inf. Syst., 24(1):51-78, 2006.\n[8] R. Lempel and S. Moran. Predictive caching and\nprefetching of query results in search engines. In WWW,\n2003.\n[9] X. Long and T. Suel. Three-level caching for efficient query\nprocessing in large web search engines. In WWW, 2005.\n[10] E. P. Markatos. On caching search engine query results.\nComputer Communications, 24(2):137-143, 2001.\n[11] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald,\nand C. Lioma. Terrier: A High Performance and Scalable\nInformation Retrieval Platform. In SIGIR Workshop on\nOpen Source Information Retrieval, 2006.\n[12] V. V. Raghavan and H. Sever. On the reuse of past optimal\nqueries. In ACM SIGIR, 1995.\n[13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira,\nR. Fonseca, and B. Riberio-Neto. Rank-preserving two-level\ncaching for scalable search engines. In ACM SIGIR, 2001.\n[14] D. R. Slutz and I. L. Traiger. A note on the calculation of\naverage working set size. Communications of the ACM,\n17(10):563-565, 1974.\n[15] T. Strohman, H. Turtle, and W. B. Croft. Optimization\nstrategies for complex queries. In ACM SIGIR, 2005.\n[16] I. H. Witten, T. C. Bell, and A. Moffat. Managing\nGigabytes: Compressing and Indexing Documents and\nImages. John Wiley & Sons, Inc., NY, 1994.\n[17] N. E. Young. On-line file caching. Algorithmica,\n33(3):371-383, 2002.\n": ["efficient caching system", "web search engine", "static caching", "dynamic caching", "caching query result", "caching posting list", "static cache", "answer and posting list", "query log", "effectiveness of static caching", "static caching effectiveness", "distribution of the query", "the query distribution", "data-access hierarchy", "disk layer", "remote server layer", "cache", "web search", "information retrieval system", ""]}