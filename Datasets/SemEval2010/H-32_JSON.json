{"Interesting Nuggets and Their Impact on Definitional\nQuestion Answering\nKian-Wei Kor\nDepartment of Computer Science\nSchool of Computing\nNational University of Singapore\ndkor@comp.nus.edu.sg\nTat-Seng Chua\nDepartment of Computer Science\nSchool of Computing\nNational University of Singapore\nchuats@comp.nus.edu.sg\nABSTRACT\nCurrent approaches to identifying definitional sentences in the \ncontext of Question Answering mainly involve the use of linguistic\nor syntactic patterns to identify informative nuggets. This is \ninsufficient as they do not address the novelty factor that a \ndefinitional nugget must also possess. This paper proposes to address\nthe deficiency by building a Human Interest Model from external\nknowledge. It is hoped that such a model will allow the \ncomputation of human interest in the sentence with respect to the topic. We\ncompare and contrast our model with current definitional question\nanswering models to show that interestingness plays an important\nfactor in definitional question answering.\nCategories and Subject Descriptors\nH.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2\n[User/Machine Systems]: Human Factors\nGeneral Terms\nAlgorithms, Human Factors, Experimentation\n1. DEFINITIONAL QUESTION \nANSWERING\nDefinitional Question Answering was first introduced to the TExt\nRetrieval Conference Question Answering Track main task in 2003.\nThe Definition questions, also called Other questions in recent years,\nare defined as follows. Given a question topic X, the task of a \ndefinitional QA system is akin to answering the question What is X?\nor Who is X?. The definitional QA system is to search through\na news corpus and return return a set of answers that best describes\nthe question topic. Each answer should be a unique topic-specific\nnugget that makes up one facet in the definition of the question\ntopic.\n1.1 The Two Aspects of Topic Nuggets\nOfficially, topic-specific answer nuggets or simply topic nuggets\nare described as informative nuggets. Each informative nugget is\na sentence fragment that describe some factual information about\nthe topic. Depending on the topic type and domain, this can include\ntopic properties, relationships the topic has with some closely \nrelated entity, or events that happened to the topic.\nFrom observation of the answer set for definitional question \nanswering from TREC 2003 to 2005, it seems that a significant \nnumber of topic nuggets cannot simply be described as informative\nnuggets. Rather, these topic nuggets have a trivia-like quality \nassociated with them. Typically, these are out of the ordinary pieces\nof information about a topic that can pique a human reader\"s \ninterest. For this reason, we decided to define answer nuggets that\ncan evoke human interest as interesting nuggets. In essence, \ninteresting nuggets answer the questions What is X famous for?,\nWhat defines X? or What is extraordinary about X?.\nWe now have two very different perspective as to what \nconstitutes an answer to Definition questions. An answer can be some\nimportant factual information about the topic or some novel and\ninteresting aspect about the topic. This duality of informativeness\nand interestingness can be clearly observed in the five vital answer\nnuggets for a TREC 2005 topic of George Foreman. Certain \nanswer nuggets are more informative while other nuggets are more\ninteresting in nature.\nInformative Nuggets\n- Was graduate of Job Corps.\n- Became oldest world champion in boxing history.\nInteresting Nuggets\n- Has lent his name to line of food preparation products.\n- Waved American flag after winning 1968 Olympics championship.\n- Returned to boxing after 10 yr hiatus.\nAs an African-American professional heavyweight boxer, an \naverage human reader would find the last three nuggets about George\nForeman interesting because boxers do not usually lend their names\nto food preparation products, nor do boxers retire for 10 years \nbefore returning to the ring and become the world\"s oldest boxing\nchampion. Foreman\"s waving of the American flag at the Olympics\nis interesting because the innocent action caused some \nAfricanAmericans to accuse Foreman of being an Uncle Tom. As seen\nhere, interesting nuggets has some surprise factor or unique quality\nthat makes them interesting to human readers.\n1.2 Identifying Interesting Nuggets\nSince the original official description for definitions comprise of\nidentifying informative nuggets, most research has focused entirely\non identifying informative nuggets. In this paper, we focus on \nexploring the properties of interesting nuggets and develop ways of\nidentify such interesting nuggets. A Human Interest Model \ndefinitional question answering system is developed with emphasis\non identifying interesting nuggets in order to evaluate the impact\nof interesting nuggets on the performance of a definitional \nquestion answering system. We further experimented with combining\nthe Human Interest Model with a lexical pattern based definitional\nquestion answering system in order to capture both informative and\ninteresting nuggets.\n2. RELATED WORK\nThere are currently two general methods for Definitional \nQuestion Answering. The more common method uses a lexical \npatternbased approach was first proposed by Blair-Goldensohn et al. [1]\nand Xu et al. [14]. Both groups predominantly used patterns such\nas copulas and appositives, as well as manually crafted \nlexicosyntactic patterns to identify sentences that contain informative nuggets.\nFor example, Xu et al. used 40 manually defined structured \npatterns in their 2003 definitional question answering system. Since\nthen, in an attempt to capture a wider class of informational nuggets,\nmany such systems of increasing complexity has been created. A\nrecent system by Harabagiu et al. [6] created a definitional \nquestion answering system that combines the use of 150 manually \ndefined positive and negative patterns, named entity relations and\nspecially crafted information extraction templates for 33 target \ndomains. Here, a musician template may contain lexical patterns that\nidentify information such as the musician\"s musical style, songs\nsung by the musician and the band, if any, that the musician belongs\nto. As one can imagine, this is a knowledge intensive approach that\nrequires an expert linguist to manually define all possible lexical or\nsyntactic patterns required to identify specific types of information.\nThis process requires a lot of manual labor, expertise and is not\nscalable. This lead to the development of the soft-pattern approach\nby Cui et al. [4, 11]. Instead of manually encoding patterns, \nanswers to previous definitional question answering evaluations were\nconverted into generic patterns and a probabilistic model is trained\nto identify such patterns in sentences. Given a potential answer\nsentence, the probabilistic model outputs a probability that \nindicates how likely the sentence matches one or more patterns that the\nmodel has seen in training.\nSuch lexicalosyntactic patterns approach have been shown to be\nadept at identifying factual informative nuggets such as a person\"s\nbirthdate, or the name of a company\"s CEO. However, these \npatterns are either globally applicable to all topics or to a specific set\nof entities such as musicians or organizations. This is in direct\ncontrast to interesting nuggets that are highly specific to \nindividual topics and not to a set of entities. For example, the interesting\nnuggets for George Foreman are specific only George Foreman and\nno other boxer or human being. Topic specificity or topic relevance\nis thus an important criteria that helps identify interesting nuggets.\nThis leads to the exploration of the second relevance-based \napproach that has been used in definitional question answering. \nPredominantly, this approach has been used as a backup method for\nidentifying definitional sentences when the primary method of \nlexicalosyntactic patterns failed to find a sufficient number of \ninformative nuggets [1]. A similar approach has also been used as a\nbaseline system for TREC 2003 [14]. More recently, Chen et al.\n[3] adapted a bi-gram or bi-term language model for definitional\nQuestion Answering.\nGenerally, the relevance-based approach requires a definitional\ncorpus that contain documents highly relevant to the topic. The\nbaseline system in TREC 2003 simply uses the topic words as its\ndefinitional corpus. Blair-Goldensohn et al. [1] uses a machine\nlearner to include in the definitonal corpus sentences that are likely\nto be definitional. Chen et al. [3] collect snippets from Google to\nbuild its definitional corpus.\nFrom the definitional corpus, a definitional centroid vector is\nbuilt or a set of centroid words are selected. This centroid \nvector or set of centroid words is taken to be highly indicative of the\ntopic. Systems can then use this centroid to identify definitional \nanswers by using a variety of distance metrics to compare against \nsentences found in the set of retrieved documents for the topic. \nBlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by\ncentrality. Chen et al. [3] builds a bigram language model using\nthe 350 most frequently occurring google snippet terms, described\nin their paper as an ordered centroid, to estimate the probability that\na sentence is similar to the ordered centroid.\nAs described here, the relevance-based approach is highly \nspecific to individual topics due to its dependence on a topic specific\ndefinitional corpus. However if individual sentences are viewed as\na document, then relevance-based approaches essentially use the\ncollected topic specific centroid words as a form of document \nretrieval with automated query expansion to identify strongly \nrelevant sentences. Thus such methods identify relevant sentences and\nnot sentences containing definitional nuggets. Yet, the TREC 2003\nbaseline system [14] outperformed all but one other system. The\nbi-term language model [3] is able to report results that are highly\ncompetitive to state-of-the-art results using this retrieval-based \napproach. At TREC 2006, a simple weighted sum of all terms model\nwith terms weighted using solely Google snippets outperformed all\nother systems by a significant margin [7].\nWe believe that interesting nuggets often come in the form of\ntrivia, novel or rare facts about the topic that tend to strongly \ncooccur with direct mention of topic keywords. This may explain\nwhy relevance-based method can perform competitively in \ndefinitional question answering. However, simply comparing against a\nsingle centroid vector or set of centroid words may have over \nemphasized topic relevance and has only identified interesting \ndefinitional nuggets in an indirect manner. Still, relevance based retrieval\nmethods can be used as a starting point in identifying interesting\nnuggets. We will describe how we expand upon such methods to\nidentify interesting nuggets in the next section.\n3. HUMAN INTEREST MODEL\nGetting a computer system to identify sentences that a human\nreader would find interesting is a tall order. However, there are\nmany documents on the world wide web that are contain concise,\nhuman written summaries on just about any topic. What\"s more,\nthese documents are written explicitly for human beings and will\ncontain information about the topic that most human readers would\nbe interested in. Assuming we can identify such relevant \ndocuments on the web, we can leverage them to assist in identifying\ndefinitional answers to such topics. We can take the assumption\nthat most sentences found within these web documents will \ncontain interesting facets about the topic at hand.\nThis greatly simplifies the problem to that of finding within the\nAQUAINT corpus sentences similar to those found in web \ndocuments. This approach has been successfully used in several factoid\nand list Question Answering systems [11] and we feel the use of\nsuch an approach for definitional or Other question answering is\njustified. Identifying interesting nuggets requires computing \nmachinery to understand world knowledge and human insight. This\nis still a very challenging task and the use of human written \ndocuments dramatically simplifies the complexity of the task.\nIn this paper, we report on such an approach by experimenting\nwith a simple word-level edit distance based weighted term \ncomparison algorithm. We use the edit distance algorithm to score the\nsimilarity of a pair of sentences, with one sentence coming from\nweb resources and the other sentence selected from the AQUAINT\ncorpus. Through a series of experiments, we will show that even\nsuch a simple approach can be very effective at definitional \nquestion answering.\n3.1 Web Resources\nThere exists on the internet articles on just about any topic a \nhuman can think of. What\"s more, many such articles are centrally\nlocated on several prominent websites, making them an easily \naccessible source of world knowledge. For our work on identifying\ninteresting nuggets, we focused on finding short one or two page\narticles on the internet that are highly relevant to our desired topic.\nSuch articles are useful as they contain concise information about\nthe topic. More importantly, the articles are written by humans, for\nhuman readers and thus contain the critical human world \nknowledge that a computer system currently is unable to capture.\nWe leverage this world knowledge by collecting articles for each\ntopic from the following external resources to build our Interest\nCorpus for each topic.\nWikipedia is a Web-based, free-content encyclopedia written \ncollaboratively by volunteers. This resource has been used by\nmany Question Answering system as a source of knowledge\nabout each topic. We use a snapshot of Wikipedia taken in\nMarch 2006 and include the most relevant article in the \nInterest Corpus.\nNewsLibrary is a searchable archive of news articles from over\n100 different newspaper agencies. For each topic, we \ndownload the 50 most relevant articles and include the title and\nfirst paragraph of each article in the Interest Corpus.\nGoogle Snippets are retrieved by issuing the topic as a query to\nthe Google search engine. From the search results, we \nextracted the top 100 snippets. While Google snippets are not\narticles, we find that they provide a wide coverage of \nauthorative information about most topics.\nDue to their comprehensive coverage of a wide variety of \ntopics, the above resources form the bulk of our Interest Corpus. We\nalso extracted documents from other resources. However, as these\nresources are more specific in nature, we do not always get any\nsingle relevant document. These resources are listed below.\nBiography.com is the website for the Biography television cable\nchannel. The channel\"s website contains searchable \nbiographies on over 25,000 notable people. If the topic is a person\nand we can find a relevant biography on the person, we \ninclude it it in our Interest Corpus.\nBartleby.com contains a searchable copy of several resources \nincluding the Columbia Encyclopedia, the World Factbook,\nand several English dictionaries.\ns9.com is a biography dictionary on over 33,000 notable people.\nLike Biography.com, we include the most relevant biography\nwe can find in the Interest Corpus.\nGoogle Definitions Google search engine offers a feature called\nDefinitions that provides the definition for a query, if it\nhas one. We use this feature and extract whatever definitions\nthe Google search engine has found for each topic into the\nInterest Corpus.\nFigure 1: Human Interest Model Architecture.\nWordNet WordNet is an well-known electronic semantic lexicon\nfor the English language. Besides grouping English words\ninto sets of synonyms called synsets, it also provide a short\ndefinition on the meaning of words found in each synset. We\nadd this short definition, if there is one, into our Interest \nCorpus.\nWe have two major uses for this topic specific Interest Corpus,\nas a source of sentences containing interesting nuggets and as a\nunigram language model of topic terms, I.\n3.2 Multiple Interesting Centroids\nWe have seen that interesting nuggets are highly specific to a\ntopic. Relevance-based approaches such as the bigram language\nmodel used by Chen et al. [3] are focused on identifying highly\nrelevant sentences and pick up definitional answer nuggets as an\nindirect consequence. We believe that the use of only a single \ncollection of centroid words has over-emphasized topic relevance and\nchoose instead to use multiple centroids.\nSince sentences in the Interest Corpus of articles we collected\nfrom the internet are likely to contain nuggets that are of interest to\nhuman readers, we can essentially use each sentence as \npseudocentroids. Each sentence in the Interest Corpus essentially raises\na different aspect of the topic for consideration as a sentence of\ninterest to human readers. By performing a pairwise sentence \ncomparison between sentences in the Interest Corpus and candidate \nsentences retrieved from the AQUAINT corpus, we increase the \nnumber of sentence comparisons from O(n) to O(nm). Here, n is\nthe number of potential candidate sentences and m is the number\nof sentences in the Interest Corpus. In return, we obtain a diverse\nranked list of answers that are individually similar to various \nsentences found in the topic\"s Interest Corpus. An answer can only be\nhighly ranked if it is strongly similar to a sentence in the Interest\nCorpus, and is also strongly relevant to the topic.\n3.3 Implementation\nFigure 1 shows the system architecture for the proposed Human\nInterest-based definitional QA system.\nThe AQUAINT Retrieval module shown in Figure 1 reuses a\ndocument retrieval module of a current Factoid and List Question\nAnswering system we have implemented. Given a set of words\ndescribing the topic, the AQUAINT Retrieval module does query\nexpansion using Google and searches an index of AQUAINT \ndocuments to retrieve the 800 most relevant documents for \nconsideration.\nThe Web Retrieval module on the other hand, searches the online\nresources described in Section 3.1 for interesting documents in\norder to populate the Interest Corpus.\nThe HIM Ranker, or Human Interest Model Ranking module, is\nthe implementation of what is described in this paper. The module\nfirst builds the unigram language model, I, from the collected web\ndocuments. This language model will be used to weight the \nimportance of terms within sentences. Next, a sentence chunker is used\nto segment all 800 retrieved documents into individual sentences.\nEach of these sentences can be a potential answer sentence that will\nbe independently ranked by interestingness. We rank sentences by\ninterestingness using sentences from both the Interest Corpus of \nexternal documents as well as the unigram language model we built\nearlier which we use to weight terms.\nA candidate sentence in our top 800 relevant AQUAINT \ndocuments is considered interesting if it is highly similar in content to\na sentence found in our collection of external web-documents. To\nachieve this, we perform a pairwise similarity comparison between\na candidate sentence and sentences in our external documents \nusing a weighted-term edit distance algorithm. Term weights are used\nto adjust the relative importance of each unique term found in the\nInterest Corpus. When both sentences share the same term, the\nsimilarity score is incremented by the two times the term\"s weight\nand every dissimilar term decrements the similarity score by the\ndissimilar term\"s weight.\nWe choose the highest achieved similarity score for a candidate\nsentence as the Human Interest Model score for the candidate \nsentence. In this manner, every candidate sentence is ranked by \ninterestingness. Finally, to obtain the answer set, we select the top 12\nhighest ranked and non redundant sentences as definitional answers\nfor the topic.\n4. INITIAL EXPERIMENTS\nThe Human Interest-based system described in the previous \nsection is designed to identify only interesting nuggets and not \ninformative nuggets. Thus, it can be described as a handicapped \nsystem that only deals with half the problem in definitional question\nanswering. This is done in order to explore how interestingness\nplays a factor in definitional answers. In order to compare and \ncontrast the differences between informative and interesting nuggets,\nwe also implemented the soft-pattern bigram model proposed by\nCui et al. [4, 11]. In order to ensure comparable results, both \nsystems are provided identical input data. Since both system require\nthe use of external resources, they are both provided the same web\narticles retrieved by our Web Retrieval module. Both systems also\nrank the same same set of candidate sentences in the form of 800\nmost relevant documents as retrieved by our AQUAINT Retrieval\nmodule.\nFor the experiments, we used the TREC 2004 question set to\ntune any system parameters and use the TREC 2005 question sets\nto test the both systems. Both systems are evaluated the results \nusing the standard scoring methodology for TREC definitions. TREC\nprovides a list of vital and okay nuggets for each question topic.\nEvery question is scored on nugget recall (NR) and nugget \nprecision (NP) and a single final score is computed using F-Measure\n(see equation 1) with \u00ce\u00b2 = 3 to emphasize nugget recall. Here, NR\nis the number of vital nuggets returned divided by total number\nof vital nuggets while NP is computed using a minimum allowed\ncharacter length function defined in [12]. The evaluation is \nautomatically conducted using Pourpre v1.0c [10].\nFScore =\n\u00ce\u00b22\n\u00e2\u02c6\u2014 NP \u00e2\u02c6\u2014 NR\n(\u00ce\u00b22 + 1)NP + NR\n(1)\nSystem F3-Score\nBest TREC 2005 System 0.2480\nSoft-Pattern (SP) 0.2872\nHuman Interest Model (HIM) 0.3031\nTable 1: Performance on TREC 2005 Question Set\nFigure 2: Performance by entity types.\n4.1 Informativeness vs Interestingness\nOur first experiment compares the performance of solely \nidentifying interesting nuggets against solely identifying informative\nnuggets. We compare the results attained by the Human Interest\nModel that only identify interesting nuggets with the results of the\nsyntactic pattern finding Soft-Pattern model as well as the result of\nthe top performing definitional system in TREC 2005 [13]. Table 1\nshows the F3 score the three systems for the TREC 2005 question\nset.\nThe Human Interest Model clearly outperform both soft pattern\nand the best TREC 2005 system with a F3 score of 0.303. The\nresult is also comparable with the result of a human manual run,\nwhich attained a F3 score of 0.299 on the same question set [9].\nThis result is confirmation that interesting nuggets does indeed play\na significant role in picking up definitional answers, and may be\nmore vital than using information finding lexical patterns.\nIn order to get a better perspective of how well the Human \nInterest Model performs for different types of topics, we manually\ndivided the TREC 2005 topics into four broad categories of \nPERSON, ORGANIZATION, THING and EVENT as listed in Table\n3. These categories conform to TREC\"s general division of \nquestion topics into 4 main entity types [13]. The performance of \nHuman Interest Model and Soft Pattern Bigram Model for each entity\ntype can be seen in Figure 2. Both systems exhibit consistent \nbehavior across entity types, with the best performance coming from\nPERSON and ORGANIZATION topics and the worst performance\nfrom THING and EVENT topics. This can mainly be attributed\nto our selection of web-based resources for the definitional corpus\nused by both system. In general, it is harder to locate a single web\narticle that describes an event or a general object. However given\nthe same set of web-based information, the Human Interest Model\nconsistently outperforms the soft-pattern model for all four entity\ntypes. This suggests that the Human Interest Model is better able\nto leverage the information found in web resources to identify \ndefinitional answers.\n5. REFINEMENTS\nEncouraged by the initial experimental results, we explored two\nfurther optimization of the basic algorithm.\n5.1 Weighting Interesting Terms\nThe word trivia refer to tidbits of unimportant or uncommon \ninformation. As we have noted, interesting nuggets often has a \ntrivialike quality that makes them of interest to human beings. From this\ndescription of interesting nuggets and trivia, we hypothesize that\ninteresting nuggets are likely to occur rarely in a text corpora.\nThere is a possibility that some low-frequency terms may \nactually be important in identifying interesting nuggets. A standard \nunigram language model would not capture these low-frequency terms\nas important terms. To explore this possibility, we experimented\nwith three different term weighting schemes that can provide more\nweight to certain low-frequency terms. The weighting schemes we\nconsidered include commonly used TFIDF, as well as information\ntheoretic Kullback-Leiber divergence and Jensen-Shannon \ndivergence [8].\nTFIDF, or Term Frequency \u00c3\u2014 Inverse Document Frequency, is\na standard Information Retrieval weighting scheme that balances\nthe importance of a term in a document and in a corpus. For our\nexperiments, we compute the weight of each term as tf \u00c3\u2014 log( N\nnt\n),\nwhere tf is the term frequency, nt is the number of sentences in\nthe Interest Corpus having the term and N is the total number of\nsentences in the Interest Corpus.\nKullback-Leibler Divergence (Equation 2) is also called KL \nDivergence or relative entropy, can be viewed as measuring the \ndissimilarity between two probability distributions. Here, we treat the\nAQUAINT corpus as a unigram language model of general English\n[15], A, and the Interest Corpus as a unigram language model \nconsisting of topic specific terms and general English terms, I. \nGeneral English words are likely to have similar distributions in both\nlanguage models I and A. Thus using KL Divergence as a term\nweighting scheme will cause strong weights to be given to \ntopicspecific terms because their distribution in the Interest Corpus they\noccur significantly more often or less often than in general English.\nIn this way, high frequency centroid terms as well as low frequency\nrare but topic-specific terms are both identified and highly weighted\nusing KL Divergence.\nDKL(I A) =\nt\nI(t)log\nI(t)\nA(t)\n(2)\nDue to the power law distribution of terms in natural language,\nthere are only a small number of very frequent terms and a large\nnumber of rare terms in both I and A. While the common terms\nin English consist of stop words, the common terms in the topic\nspecific corpus, I, consist of both stop words and relevant topic\nwords. These high frequency topic specific words occur very much\nmore frequently in I than in A. As a result, we found that KL\nDivergence has a bias towards highly frequent topic terms as we are\nmeasuring direct dissimilarity against a model of general English\nwhere such topic terms are very rare. For this reason, we explored\nanother divergence measure as a possible term weighting scheme.\nJensen-Shannon Divergence or JS Divergence extends upon KL\nDivergence as seen in Equation 3. As with KL Divergence, we also\nuse JS divergence to measure the dissimilarity between our two\nlanguage models, I and A.\nDJS(I A) = 1\n2\n\u00c2\u00a2DKL\n\u00c2\u00a0I I+A\n2\n\u00c2\u00a1+ DKL\n\u00c2\u00a0A I+A\n2\n\u00c2\u00a1\u00c2\u00a3 (3)\nFigure 3: Performance by various term weighting schemes on\nthe Human Interest Model.\nHowever, JS Divergence has additional properties1\nof being \nsymmetric and non-negative as seen in Equation 4. The symmetric\nproperty gives a more balanced measure of dissimilarity and avoids\nthe bias that KL divergence has.\nDJS(I A) = DJS(A I) =\n0 I = A\n> 0 I <> A\n(4)\nWe conducted another experiment, substituting the unigram \nlanguge model weighting scheme we used in the initial experiments\nwith the three term weighting schemes described above. As lower\nbound reference, we included a term weighting scheme consisting\nof a constant 1 for all terms. Figure 3 show the result of applying\nthe five different term weighting schemes on the Human Interest\nModel. TFIDF performed the worst as we had anticipated. The\nreason is that most terms only appear once within each sentence,\nresulting in a term frequency of 1 for most terms. This causes\nthe IDF component to be the main factor in scoring sentences.\nAs we are computing the Inverse Document Frequency for terms\nin the Interest Corpus collected from web resources, IDF \nheavily down-weights highly frequency topic terms and relevant terms.\nThis results in TFIDF favoring all low frequency terms over high\nfrequency terms in the Interest Corpus. Despite this, the TFIDF\nweighting scheme only scored a slight 0.0085 lower than our lower\nbound reference of constant weights. We view this as a positive\nindication that low frequency terms can indeed be useful in finding\ninteresting nuggets.\nBoth KL and JS divergence performed marginally better than the\nuniform language model probabilistic scheme that we used in our\ninitial experiments. From inspection of the weighted list of terms,\nwe observed that while low frequency relevant terms were boosted\nin strength, high frequency relevant terms still dominate the top of\nthe weighted term list. Only a handful of low frequency terms were\nweighted as strongly as topic keywords and combined with their\nlow frequency, may have limited the impact of re-weighting such\nterms. However we feel that despite this, Jensen-Shannon \ndivergence does provide a small but measurable increase in the \nperformance of our Human Interest Model.\n1\nJS divergence also has the property of being bounded, allowing\nthe results to be treated as a probability if required. However, the\nbounded property is not required here as we are only treating the\ndivergence computed by JS divergence as term weights\n5.2 Selecting Web Resources\nIn one of our initial experiments, we observed that the quality\nof web resources included in the Interest Corpus may have a direct\nimpact on the results we obtain. We wanted to determine what \nimpact the choice of web resources have on the performance of our\nHuman Interest Model. For this reason, we split our collection of\nweb resources into four major groups listed here:\nN - News: Title and first paragraph of the top 50 most relevant\narticles found in NewsLibrary.\nW - Wikipedia: Text from the most relevant article found in\nWikipedia.\nS - Snippets: Snippets extracted from the top 100 most relevant\nlinks after querying Google.\nM - Miscellaneous sources: Combination of content (when \navailable) from secondary sources including biography.com, s9.com,\nbartleby.com articles, Google definitions and WordNet definitions.\nWe conducted a gamut of runs on the TREC 2005 question set\nusing all possible combinations of the above four groups of web\nresources to identify the best possible combination. All runs were\nconducted on Human Interest Model using JS divergence as term\nweighting scheme. The runs were sorted in descending F3-Score\nand the top 3 best performing runs for each entity class are listed\nin Table 2 together with earlier reported F3-scores from Figure 2 as\na baseline reference. A consistent trend can be observed for each\nentity class.\nFor PERSON and EVENT topics, NewsLibrary articles are the\nmain source of interesting nuggets with Google snippets and \nmiscellaneous articles offering additional supporting evidence. This\nseem intuitive for events as newspapers predominantly focus on \nreporting breaking newsworthy events and are thus excellent sources\nof interesting nuggets. We had expected Wikipedia rather than\nnews articles to be a better source of interesting facts about \npeople and were surprised to discover that news articles outperformed\nWikipedia. We believe that the reason is because the people \nselected as topics thus far have been celebrities or well known public\nfigures. Human readers are likely to be interested in news events\nthat spotlight these personalities.\nConversely for ORGANIZATION and THING topics, the best\nsource of interesting nuggets come from Wikipedia\"s most relevant\narticle on the topic with Google snippets again providing additional\ninformation for organizations.\nWith an oracle that can classify topics by entity class with 100%\naccuracy and by using the best web resources for each entity class\nas shown in Table 2, we can attain a F3-Score of 0.3158.\n6. UNIFYING INFORMATIVENESS WITH\nINTERESTINGNESS\nWe have thus far been comparing the Human Interest Model\nagainst the Soft-Pattern model in order to understand the \ndifferences between interesting and informative nuggets. However from\nthe perspective of a human reader, both informative and interesting\nnuggets are useful and definitional. Informative nuggets present a\ngeneral overview of the topic while interesting nuggets give \nreaders added depth and insight by providing novel and unique aspects\nabout the topic. We believe that a good definitional question \nanswering system should provide the reader with a combined mixture\nof both nugget types as a definitional answer set.\nRank PERSON ORG THING EVENT\nBaseline\nUnigram Weighting Scheme, N+W+S+M\n0.3279 0.3630 0.2551 0.2644\n1\nN+S+M W+S W+M N+M\n0.3584 0.3709 0.2688 0.2905\n2\nN+S N+W+S W+S+M N+S+M\n0.3469 0.3702 0.2665 0.2745\n3\nN+M N+W+S+M W+S N+S\n0.3431 0.3680 0.2616 0.2690\nTable 2: Top 3 runs using different web resources for each \nentity class\nWe now have two very different experts at identifying \ndefinitions. The Soft Pattern Bigram Model proposed by Cui et al. is\nan expert in identifying informative nuggets. The Human \nInterest Model we have described in this paper on the other hand is an\nexpert in finding interesting nuggets. We had initially hoped to\nunify the two separate definitional question answering systems by\napplying an ensemble learning method [5] such as voting or \nboosting in order to attain a good mixture of informative and interesting\nnuggets in our answer set. However, none of the ensemble \nlearning methods we attempted could outperform our Human Interest\nModel.\nThe reason is that both systems are picking up very different\nsentences as definitional answers. In essence, our two experts are\ndisagreeing on which sentences are definitional. In the top 10 \nsentences from both systems, only 4.4% of these sentences appeared in\nboth answer sets. The remaining answers were completely \ndifferent. Even when we examined the top 500 sentences generated by\nboth systems, the agreement rate was still an extremely low 5.3%.\nYet, despite the low agreement rate between both systems, each\nindividual system is still able to attain a relatively high F3 score.\nThere is a distinct possibility that each system may be selecting\ndifferent sentences with different syntactic structures but actually\nhave the same or similar semantic content. This could result in both\nsystems having the same nuggets marked as correct even though the\nsource answer sentences are structurally different. Unfortunately,\nwe are unable to automatically verify this as the evaluation software\nwe are using does not report correctly identified answer nuggets.\nTo verify if both systems are selecting the same answer nuggets,\nwe randomly selected a subset of 10 topics from the TREC 2005\nquestion set and manually identified correct answer nuggets (as \ndefined by TREC accessors) from both systems. When we compared\nthe answer nuggets found by both system for this subset of topics,\nwe found that the nugget agreement rate between both systems was\n16.6%. While the nugget agreement rate is higher than the \nsentence agreement rate, both systems are generally still picking up\ndifferent answer nuggets. We view this as further indication that\ndefinitions are indeed made up of a mixture of informative and \ninteresting nuggets. It is also indication that in general, interesting\nand informative nuggets are quite different in nature.\nThere are thus rational reasons and practical motivation in \nunifying answers from both the pattern based and corpus based \napproaches. However, the differences between the two systems also\ncause issues when we attempt to combine both answer sets. \nCurrently, the best approach we found for combining both answer sets\nis to merge and re-rank both answer sets with boosting agreements.\nWe first normalize the top 1,000 ranked sentences from each\nsystem, to obtain the Normalized Human Interest Model score,\nhim(s), and the Normalized Soft Pattern Bigram Model score,\nsp(s), for every unique sentence, s. For each sentence, the two \nseparate scores for are then unified into a single score using Equation 5.\nWhen only one system believes that the sentence is definitional, we\nsimply retain that system\"s normalized score as the unified score.\nWhen both systems agree agree that the sentence is definitional,\nthe sentence\"s score is boosted by the degree of agreement between\nbetween both systems.\nScore(s) = max(shim, ssp)1\u00e2\u02c6\u2019min(shim,ssp)\n(5)\nIn order to maintain a diverse set of answers as well as to \nensure that similar sentences are not given similar ranking, we \nfurther re-rank our combined list of answers using Maximal Marginal\nRelevance or MMR [2]. Using the approach described here, we\nachieve a F3 score of 0.3081. This score is equivalent to the initial\nHuman Interest Model score of 0.3031 but fails to outperform the\noptimized Human Interest Model model.\n7. CONCLUSION\nThis paper has presented a novel perspective for answering \ndefinitional questions through the identification of interesting nuggets.\nInteresting nuggets are uncommon pieces of information about the\ntopic that can evoke a human reader\"s curiosity. The notion of an\naverage human reader is an important consideration in our \napproach. This is very different from the lexico-syntactic pattern \napproach where the context of a human reader is not even considered\nwhen finding answers for definitional question answering.\nUsing this perspective, we have shown that using a combination\nof a carefully selected external corpus, matching against multiple\ncentroids and taking into consideration rare but highly topic \nspecific terms, we can build a definitional question answering \nmodule that is more focused on identifying nuggets that are of interest\nto human beings. Experimental results has shown this approach\ncan significantly outperform state-of-the-art definitional question\nanswering systems.\nWe further showed that at least two different types of answer\nnuggets are required to form a more thorough set of definitional\nanswers. What seems to be a good set of definition answers is some\ngeneral information that provides a quick informative overview mixed\ntogether with some novel or interesting aspects about the topic.\nThus we feel that a good definitional question answering system\nwould need to pick up both informative and interesting nugget types\nin order to provide a complete definitional coverage on all \nimportant aspects of the topic. While we have attempted to build such a\nsystem by combining our proposed Human Interest Model with Cui\net al.\"s Soft Pattern Bigram Model, the inherent differences between\nboth types of nuggets seemingly caused by the low agreement rates\nbetween both models have made this a difficult task. Indeed, this is\nnatural as the two models have been designed to identify two very\ndifferent types of definition answers using very different types of\nfeatures. As a result, we are currently only able to achieve a \nhybrid system that has the same level of performance as our proposed\nHuman Interest Model.\nWe approached the problem of definitional question answering\nfrom a novel perspective, with the notion that interest factor plays\na role in identifying definitional answers. Although the methods\nwe used are simple, they have been shown experimentally to be \neffective. Our approach may also provide some insight into a few\nanomalies in past definitional question answering\"s trials. For \ninstance, the top definitional system at the recent TREC 2006 \nevaluation was able to significantly outperform all other systems using\nrelatively simple unigram probabilities extracted from Google \nsnippets. We suspect the main contributor to the system\"s performance\nEntity Type Topics\nORGANIZATION DePauw University, Merck & Co., \nNorwegian Cruise Lines (NCL), United \nParcel Service (UPS), Little League \nBaseball, Cliffs Notes, American Legion,\nSony Pictures Entertainment (SPE), \nTelefonica of Spain, Lions Club \nInternational, AMWAY, McDonald\"s Corporation,\nHarley-Davidson, U.S. Naval Academy,\nOPEC, NATO, International Bureau of \nUniversal Postal Union (UPU), Organization of\nIslamic Conference (OIC), PBGC\nPERSON Bing Crosby, George Foreman, Akira \nKurosawa, Sani Abacha, Enrico Fermi, Arnold\nPalmer, Woody Guthrie, Sammy Sosa,\nMichael Weiss, Paul Newman, Jesse \nVentura, Rose Crumb, Rachel Carson, Paul \nRevere, Vicente Fox, Rocky Marciano, Enrico\nCaruso, Pope Pius XII, Kim Jong Il\nTHING F16, Bollywood, Viagra, Howdy Doody\nShow, Louvre Museum, meteorites, \nVirginia wine, Counting Crows, Boston Big\nDig, Chunnel, Longwood Gardens, Camp\nDavid, kudzu, U.S. Medal of Honor,\ntsunami, genome, Food-for-Oil Agreement,\nShiite, Kinmen Island\nEVENT Russian submarine Kursk sinks, Miss \nUniverse 2000 crowned, Port Arthur \nMassacre, France wins World Cup in \nsoccer, Plane clips cable wires in Italian \nresort, Kip Kinkel school shooting, Crash\nof EgyptAir Flight 990, Preakness 1998,\nfirst 2000 Bush-Gore presidential debate ,\n1998 indictment and trial of Susan \nMcDougal, return of Hong Kong to Chinese\nsovereignty, 1998 Nagano Olympic Games,\nSuper Bowl XXXIV, 1999 North American\nInternational Auto Show, 1980 Mount St.\nHelens eruption, 1998 Baseball World \nSeries, Hindenburg disaster, Hurricane Mitch\nTable 3: TREC 2005 Topics Grouped by Entity Type\nis Google\"s PageRank algorithm, which mainly consider the \nnumber of linkages, has an indirect effect of ranking web documents by\nthe degree of human interest.\nIn our future work, we seek to further improve on the combined\nsystem by incorporating more evidence in support of correct \ndefinitional answers or to filter away obviously wrong answers.\n8. REFERENCES\n[1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.\nA hybrid approach for qa track definitional questions. In\nTREC \"03: Proceedings of the 12th Text REtrieval\nConference, Gaithersburg, Maryland, 2003.\n[2] J. G. Carbonell and J. Goldstein. The use of MMR,\ndiversity-based reranking for reordering documents and\nproducing summaries. In Research and Development in\nInformation Retrieval, pages 335-336, 1998.\n[3] Y. Chen, M. Zhou, and S. Wang. Reranking answers for\ndefinitional qa using language modeling. In Proceedings of\nthe 21st International Conference on Computational\nLinguistics and 44th Annual Meeting of the Association for\nComputational Linguistics, pages 1081-1088, Sydney,\nAustralia, July 2006. Association for Computational\nLinguistics.\n[4] H. Cui, M.-Y. Kan, and T.-S. Chua. Generic soft pattern\nmodels for definitional question answering. In SIGIR \"05:\nProceedings of the 28th annual international ACM SIGIR\nconference on Research and development in information\nretrieval, pages 384-391, New York, NY, USA, 2005. ACM\nPress.\n[5] T. G. Dietterich. Ensemble methods in machine learning.\nLecture Notes in Computer Science, 1857:1-15, 2000.\n[6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl,\nand P. Wang. Employing two question answering systems at\ntrec 2005. In TREC \"05: Proceedings of the 14th Text\nREtrieval Conference, Gaithersburg, Maryland, 2005.\n[7] M. Kaisser, S. Scheible, and B. Webber. Experiments at the\nuniversity of edinburgh for the trec 2006 qa track. In TREC\n\"06 Notebook: Proceedings of the 14th Text REtrieval\nConference, Gaithersburg, Maryland, 2006. National\nInstitute of Standards and Technology.\n[8] J. Lin. Divergence measures based on the shannon entropy.\nIEEE Transactions on Information Theory, 37(1):145 - 151,\nJan 1991.\n[9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu,\nand Y. Wu. A menagerie of tracks at maryland: Hard,\nenterprise, qa, and genomics, oh my! In TREC \"05:\nProceedings of the 14th Text REtrieval Conference,\nGaithersburg, Maryland, 2005.\n[10] J. Lin and D. Demner-Fushman. Automatically evaluating\nanswers to definition questions. In Proceedings of Human\nLanguage Technology Conference and Conference on\nEmpirical Methods in Natural Language Processing, pages\n931-938, Vancouver, British Columbia, Canada, October\n2005. Association for Computational Linguistics.\n[11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.\nKan. Using syntactic and semantic relation analysis in\nquestion answering. In TREC \"05: Proceedings of the 14th\nText REtrieval Conference, Gaithersburg, Maryland, 2005.\n[12] E. M. Voorhees. Overview of the trec 2003 question\nanswering track. In Text REtrieval Conference 2003,\nGaithersburg, Maryland, 2003. National Institute of\nStandards and Technology.\n[13] E. M. Voorhees. Overview of the trec 2005 question\nanswering track. In TREC \"05: Proceedings of the 14th Text\nREtrieval Conference, Gaithersburg, Maryland, 2005.\nNational Institute of Standards and Technology.\n[14] J. Xu, A. Licuanan, and R. Weischedel. TREC 2003 QA at\nBBN: Answering definitional questions. In TREC \"03:\nProceedings of the 12th Text REtrieval Conference,\nGaithersburg, Maryland, 2003.\n[15] D. Zhang and W. S. Lee. A language modeling approach to\npassage question answering. In TREC \"03: Proceedings of\nthe 12th Text REtrieval Conference, Gaithersburg, Maryland,\n2003.\n": ["use of linguistic", "linguistic use", "external knowledge", "computation of human interest", "human interest computation", "news corpus", "question topic", "informative nugget", "sentence fragment", "human reader", "interest", "interesting nugget", "unique quality", "surprise factor", "lexical pattern", "manual labor", "baseline system", "definitional question answer", "human interest", ""]}