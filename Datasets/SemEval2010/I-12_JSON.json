{"Sharing Experiences to Learn User Characteristics in\nDynamic Environments with Sparse Data\nDavid Sarne, Barbara J. Grosz\nSchool of Engineering and Applied Sciences\nHarvard University, Cambridge MA 02138 USA\n{sarned,grosz}@eecs.harvard.edu\nABSTRACT\nThis paper investigates the problem of estimating the value of \nprobabilistic parameters needed for decision making in environments\nin which an agent, operating within a multi-agent system, has no\na priori information about the structure of the distribution of \nparameter values. The agent must be able to produce estimations\neven when it may have made only a small number of direct \nobservations, and thus it must be able to operate with sparse data.\nThe paper describes a mechanism that enables the agent to \nsignificantly improve its estimation by augmenting its direct observations\nwith those obtained by other agents with which it is coordinating.\nTo avoid undesirable bias in relatively heterogeneous environments\nwhile effectively using relevant data to improve its estimations, the\nmechanism weighs the contributions of other agents\" observations\nbased on a real-time estimation of the level of similarity between\neach of these agents and itself. The coordination autonomy \nmodule of a coordination-manager system provided an empirical setting\nfor evaluation. Simulation-based evaluations demonstrated that the\nproposed mechanism outperforms estimations based exclusively on\nan agent\"s own observations as well as estimations based on an \nunweighted aggregate of all other agents\" observations.\nCategories and Subject Descriptors\nI.2.6 [Artificial Intelligence]: Learning-Parameter learning; I.2.11\n[Artificial Intelligence]: Distributed Artificial \nIntelligence-Intelligent agents, Multiagent systems; G.3 [Mathematics of \nComputing]: Probability and Statistics-Distribution functions\nGeneral Terms\nAlgorithms, Experimentation\n1. INTRODUCTION\nFor many real-world scenarios, autonomous agents need to \noperate in dynamic, uncertain environments in which they have only\nincomplete information about the results of their actions and \ncharacteristics of other agents or people with whom they need to \ncooperate or collaborate. In such environments, agents can benefit\nfrom sharing information they gather, pooling their individual \nexperiences to improve their estimations of unknown parameters \nrequired for reasoning about actions under uncertainty.\nThis paper addresses the problem of learning the distribution of\nthe values of a probabilistic parameter that represents a \ncharacteristic of a person who is interacting with a computer agent. The \ncharacteristic to be learned is (or is clearly related to) an important \nfactor in the agent\"s decision making.1\nThe basic setting we consider\nis one in which an agent accumulates observations about a \nspecific user characteristic and uses them to produce a timely estimate\nof some measure that depends on that characteristic\"s distribution.\nThe mechanisms we develop are designed to be useful in a range of\napplication domains, such as disaster rescue, that are characterized\nby environments in which conditions may be rapidly changing, \nactions (whether of autonomous agents or of people) and the overall\noperations occur at a fast pace, and decisions must be made within\ntightly constrained time frames. Typically, agents must make \ndecisions in real time, concurrent with task execution, and in the midst\nof great uncertainty. In the remainder of this paper, we use the term\nfast-paced to refer to such environments. In fast-paced \nenvironments, information gathering may be limited, and it is not possible\nto learn offline or to wait until large amounts of data are collected\nbefore making decisions.\nFast-paced environments impose three constraints on any \nmechanism for learning a distribution function (including the large range\nof Bayesian update techniques [23]): (a) the no structure constraint:\nno a priori information about the structure of the estimated \nparameter\"s distribution nor any initial data from which such structure\ncan be inferred is available; (b) the limited use constraint: agents\ntypically need to produce only a small number of estimations in\ntotal for this parameter; (c) the early use constraint: high \naccuracy is a critical requirement even in the initial stages of learning.\nThus, the goal of the estimation methods presented in this paper is\nto minimize the average error over time, rather than to determine\nan accurate value at the end of a long period of interaction. That\nis, the agent is expected to work with the user for a limited time,\nand it attempts to minimize the overall error in its estimations. In\nsuch environments, an agent\"s individually acquired data (its own\nobservations) are too sparse for it to obtain good estimations in the\nrequisite time frame. Given the no-structure-constraint of the \nenvironment, approaches that depend on structured distributions may\nresult in a significantly high estimation bias.\nWe consider this problem in the context of a multi-agent \ndistributed system in which computer agents support people who are\ncarrying out complex tasks in a dynamic environment. The fact that\nagents are part of a multi-agent setting, in which other agents may\n1\nLearning the distribution rather than just determining some value in the\ndistribution is important whenever the overall shape of the distribution and\nnot just such individual features as mean are important.\nalso be gathering data to estimate a similar characteristic of their\nusers, offers the possibility for an agent to augment its own \nobservations with those of other agents, thus improving the accuracy of\nits learning process. Furthermore, in the environments we consider,\nagents are usually accumulating data at a relatively similar rate.\nNonetheless, the extent to which the observations of other agents\nwill be useful to a given agent depends on the extent to which their\nusers\" characteristics\" distributions are correlated with that of this\nagent\"s user. There is no guarantee that the distribution for two\ndifferent agents is highly, positively correlated, let alone that they\nare the same. Therefore, to use a data-sharing approach, a \nlearning mechanism must be capable of effectively identifying the level\nof correlation between the data collected by different agents and to\nweigh shared data depending on the level of correlation.\nThe design of a coordination autonomy (CA) module within a\ncoordination-manager system (as part of the DARPA Coordinators\nproject [18]), in which agents support a distributed scheduling task,\nprovided the initial motivation and a conceptual setting for this\nwork. However, the mechanisms themselves are general and can\nbe applied not only to other fast-paced domains, but also in other\nmulti-agent settings in which agents are collecting data that \noverlaps to some extent, at approximately similar rates, and in which\nthe environment imposes the no-structure, limited- and early-use\nconstraints defined above (e.g., exploration of remote planets). In\nparticular, our techniques would be useful in any setting in which a\ngroup of agents undertakes a task in a new environment, with each\nagent obtaining observations at a similar rate of individual \nparameters they need for their decision-making.\nIn this paper, we present a mechanism that was used for \nlearning key user characteristics in fast-paced environments. The \nmechanism provides relatively accurate estimations within short time\nframes by augmenting an individual agent\"s direct observations with\nobservations obtained by other agents with which it is \ncoordinating. In particular, we focus on the related problems of estimating\nthe cost of interrupting a person and estimating the probability that\nthat person will have the information required by the system. Our\nadaptive approach, which we will refer to throughout the paper as\nselective-sharing, allows our CA to improve the accuracy of its\ndistribution-based estimations in comparison to relying only on the\ninteractions with a specific user (subsequently, self-learning) or\npooling all data unconditionally (average all), in particular when\nthe number of available observations is relatively small.\nThe mechanism was successfully tested using a system that \nsimulates a Coordinators environment. The next section of the paper\ndescribes the problem of estimating user-related parameters in \nfastpaced domains. Section 3 provides an overview of the methods we\ndeveloped. The implementation, empirical setting, and results are\ngiven in Sections 4 and 5. A comparison with related methods is\ngiven in Section 6 and conclusions in section 7.\n2. PARAMETER ESTIMATION IN \nFASTPACED DOMAINS\nThe CA module and algorithms we describe in this paper were\ndeveloped and tested in the Coordinators domain [21]. In this \ndomain, autonomous agents, called Coordinators, are intended to\nhelp maximize an overall team objective by handling changes in\nthe task schedule as conditions of operation change. Each agent\noperates on behalf of its owner (e.g., the team leader of a \nfirstresponse team or a unit commander) whose schedule it manages.\nThus, the actual tasks being scheduled are executed either by \nowners or by units they oversee, and the agent\"s responsibility is limited\nto maintaining the scheduling of these tasks and coordinating with\nthe agents of other human team members (i.e., other owners). In\nthis domain, scheduling information and constraints are distributed.\nEach agent receives a different view of the tasks and structures that\nconstitute the full multi-agent problem-typically only a partial,\nlocal one. Schedule revisions that affect more than one agent must\nbe coordinated, so agents thus must share certain kinds of \ninformation. (In a team context they may be designed to share other types\nas well.) However, the fast-paced nature of the domain constrains\nthe amount of information they can share, precluding a centralized\nsolution; scheduling problems must be solved distributively.\nThe agent-owner relationship is a collaborative one, with the\nagent needing to interact with its owner to obtain task and \nenvironment information relevant to scheduling. The CA module is \nresponsible for deciding intelligently when and how to interact with\nthe owner for improving the agent\"s scheduling. As a result, the CA\nmust estimate the expected benefit of any such interaction and the\ncost associated with it [19]. In general, the net benefit of a potential\ninteraction is PV \u00e2\u02c6\u2019 C, where V is the value of the information the\nuser may have, P is the probability that the user has this \ninformation, and C is the cost associated with an interaction. The values of\nP, V , and C are time-varying, and the CA estimates their value at\nthe intended time of initiating the interaction with its owner. This\npaper focuses on the twin problems of estimating the parameters P\nand C, both of which are user-centric in the sense of being \ndetermined by characteristics of the owner and the environment in which\nthe owner is operating); it presumes a mechanism for determining\nV [18].\n2.1 Estimating Interruption Costs\nThe cost of interrupting owners derives from the potential \ndegradation in performance of tasks they are doing caused by the \ndisruption [1; 9, inter alia]. Research on interaction management has \ndeployed sensor-based statistical models of human interruptibility to\ninfer the degree of distraction likely to be caused by an interruption.\nThis work aims to reduce interruption costs by delaying \ninterruptions to times that are convenient. It typically uses Bayesian models\nto learn a user\"s current or likely future focus of attention from an\nongoing stream of actions. By using sensors to provide continuous\nincoming indications of the user\"s attentional state, these models\nattempt to provide a means for computing probability distributions\nover a user\"s attention and intentions [9]. Work which examines\nsuch interruptibility-cost factors as user frustration and \ndistractability [10] includes work on the cost of repeatedly bothering the user\nwhich takes into account the fact that recent interruptions and \ndifficult questions should carry more weight than interruptions in the\ndistant past or straightforward questions [5].\nAlthough this prior work uses interruptibility estimates to \nbalance the interaction\"s estimated importance against the degree of\ndistraction likely to be caused, it differs from the fast-paced \nenvironments problem we address in three ways that fundamentally\nchange the nature of the problem and hence alter the possible \nsolutions. First, it considers settings in which the computer system\nhas information that may be relevant to its user rather than the\nuser (owner) having information needed by the system, which is\nthe complement of the information exchange situation we consider.\nSecond, the interruptibility-estimation models are task-based. Lastly,\nit relies on continuous monitoring of a user\"s activities.\nIn fast-paced environments, there usually is no single task \nstructure, and some of the activities themselves may have little internal\nstructure. As a result, it is difficult to determine the actual \nattentional state of agent-owners [15]. In such settings, owners must\nmake complex decisions that typically involve a number of other\nmembers of their units, while remaining reactive to events that \ndiverge from expectations [24]. For instance, during disaster \nrescue, a first-response unit may begin rescuing survivors trapped in a\nburning house, when a wall collapses suddenly, forcing the unit to\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 203\nretract and re-plan their actions.\nPrior work has tracked users\" focus of attention using a range of\ndevices, including those able to monitor gestures [8] and track \neyegaze to identify focus of visual attention [13, 20], thus enabling \nestimations of cognitive load and physical indicators of performance\ndegradation. The mechanisms described in this paper also presume\nthe existence of such sensors. However, in contrast to prior work,\nwhich relies on these devices operating continuously, our \nmechanism presumes that fast-paced environments only allow for the\nactivation of sensors for short periods of time on an ad-hoc basis,\nbecause agents\" resources are severely limited.\nMethods that depend on predicting what a person will do next\nbased only on what the user is currently doing (e.g., MDPs) are not\nappropriate for modeling focus of attention in fast-paced domains,\nbecause an agent cannot rely on a person\"s attentional state being\nwell structured and monitoring can only be done on a sporadic,\nnon-continuous basis. Thus, at any given time, the cost of \ninteraction with the user is essentially probabilistic, as reflected over a\nsingle random monitoring event, and can be assigned a probability\ndistribution function. Consequently, in fast-paced environments,\nan agent needs a sampling strategy by which the CA samples its\nowner\"s interruptibility level (with some cost) and decides whether\nto initiate an interaction at this specific time or to delay until a lower\ncost is observed in future samplings. The method we describe in\nthe remainder of this subsection applies concepts from economic\nsearch theory [16] to this problem. The CA\"s cost estimation uses\na mechanism that integrates the distribution of an owner\"s \ninterruptibility level (as estimated by the CA) into an economic search\nstrategy, in a way that the overall combined cost of sensor costs and\ninteraction costs is minimized.\nIn its most basic form, the economic search problem aims to\nidentify an opportunity that will minimize expected cost or \nmaximize expected utility. The search process itself is associated with\na cost, and opportunities (in our case, interruption opportunities)\nare associated with a stationary distribution function. We use a \nsequential search strategy [16] in which one observation is drawn at\na time, over multiple search stages. The dominating strategy in\nthis model is a reservation-value based strategy which determines a\nlower bound, and keeps drawing samples as long as no opportunity\nabove the bound was drawn.\nIn particular, we consider the situation in which an agent\"s owner\nhas an interruption cost described by a probability distribution \nfunction (pdf) f(x) and a cumulative distribution function (cdf) F(x).\nThe agent can activate sensing devices to get an estimation of the\ninterruption cost, x, at the current time, but there is a cost c of \noperating the sensing devices for a single time unit. The CA module\nsets a reservation value and as long as the sensor-based \nobservation x is greater than this reservation value, the CA will wait and\nre-sample the user for a new estimation.\nThe expected cost, V (xrv), using such a strategy with \nreservation value xrv is described by Equation 1,\nV (xrv) =\nc +\nR xrv\ny=0\nyf(y)\nF(xrv)\n, (1)\nwhich decomposes into two parts. The first part, c divided by\nF(xrv), represents the expected sampling cost. The second, the\nintegral divided by F(xrv), represents the expected cost of \ninterruption, because the expected number of search cycles is \n(random) geometric and the probability of success is F(xrv). Taking\nthe derivative of the left-hand-side of Equation 1 and equating it\nto zero, yields the characteristics of the optimal reservation value,\nnamely x\u00e2\u02c6\u2014\nrv must satisfy,\nV (x\u00e2\u02c6\u2014\nrv) = x\u00e2\u02c6\u2014\nrv. (2)\nSubstituting (2) in Equation 1 yields Equation 3 (after integration\nby parts) from which the optimal reservation value, x\u00e2\u02c6\u2014\nrv, and \nconsequently (from Equation 2) V (x\u00e2\u02c6\u2014\nrv) can be computed.\nc =\nZ x\u00e2\u02c6\u2014\nrv\ny=0\nF(y) (3)\nThis method, which depends on extracting the optimal sequence\nof sensor-based user sampling, relies heavily on the structure of the\ndistribution function, f(x). However, we need only a portion of\nthe distribution function, namely from the origin to the reservation\nvalue. (See Equation 1 and Figure 1.) Thus, when we consider\nsharing data, it is not necessary to rely on complete similarity in\nthe distribution function of different users. For some parameters,\nincluding the user\"s interruptibility level, it is enough to rely on\nsimilarity in the relevant portion of the distribution function. The\nimplementation described in Sections 4-5 relies on this fact.\nFigure 1: The distribution structure affecting the expected\ncost\"s calculation\n2.2 Estimating the Probability of Having \nInformation\nOne way an agent can estimate the probability a user will have\ninformation it needs (e.g., will know at a specific interruption time,\nwith some level of reliability, the actual outcome of a task currently\nbeing executed) is to rely on prior interactions with this user, \ncalculating the ratio between the number of times the user had the\ninformation and the total number of interactions. Alternatively, the\nagent can attempt to infer this probability from measurable \ncharacteristics of the user\"s behavior, which it can assess without \nrequiring an interruption. This indirect approach, which does not require\ninterrupting the user, is especially useful in fast-paced domains.\nThe CA module we designed uses such an indirect method: \nownerenvironment interactions are used as a proxy for measuring whether\nthe owner has certain information. For instance, in \nCoordinatorslike scenarios, owners may obtain a variety of information through\noccasional coordination meetings of all owners, direct \ncommunication with other individual owners participating in the execution\nof a joint task (through which they may learn informally about the\nexistence or status of other actions they are executing), open \ncommunications they overhear (e.g. if commanders leave their radios\nopen, they can listen to messages associated with other teams in\ntheir area), and other formal or informal communication channels\n[24]. Thus, owners\" levels of communication with others, which\ncan be obtained without interrupting them, provide some indication\nof the frequency with which they obtain new information. Given\noccasional updates about its owner\"s level of communication, the\nCA can estimate the probability that a random interaction with the\nowner will yield the information it needs. Denoting the \nprobability distribution function of the amount of communication the user\ngenerally maintains with its environment by g(x), and using the\ntransformation function Z(x), mapping from a level of \ncommunication, x, to a probability of having the information, the expected\nprobability of getting the information that is needed from the owner\nwhen interrupting at a given time can be calculated from\nP =\nZ \u00e2\u02c6\u017e\n0\nZ(x)g(x)dy. (4)\n204 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\nThe more observations an agent can accumulate about the \ndistribution of the frequency of an owner\"s interaction with the \nenvironment at a given time, the better it can estimate the probability the\nowner has the information needed by the system.\n3. THE SELECTIVE-SHARING MECHANISM\nThis section presents the selective-sharing mechanism by which\nthe CA learns the distribution function of a probabilistic parameter\nby taking advantage of data collected by other CAs in its \nenvironment. We first explain the need for increasing the number of \nobservations used as the basis of estimation and then present a method\nfor determining how much data to adopt from other agents.\nThe most straightforward method for the CA to learn the \ndistribution functions associated with the different parameters \ncharacterizing an owner is by building a histogram based on the \nobservations it has accumulated up to the estimation point. Based on\nthis histogram, the CA can estimate the parameter either by \ntaking into account the entire range of values (e.g., to estimate the\nmean) or a portion of it (e.g., to find the expected cost when using\na reservation-value-based strategy). The accuracy of the estimation\nwill vary widely if it is based on only a small number of \nobservations.\nFor example, Figure 2 illustrates the reservation-value-based cost\ncalculated according to observations received from an owner with\na uniform interruption cost distribution U(0, 100) as a function of\nthe number of accumulated observations used for generating the\ndistribution histogram. (In this simulation, device activation cost\nwas taken to be c = 0.5).\nFigure 2: The convergence of a single CA to its optimal strategy\nThese deviations from the actual (true) value (which is 10 in this\ncase, according to Equation 3) is because the sample used in each\nstage cannot accurately capture the actual structure of the \ndistribution function. Eventually this method yields a very accurate \nestimation for the expected interruption cost. However, in the \ninitial stages of the process, its estimation deviates significantly from\nthe true value. This error could seriously degrade the CA\"s \ndecision making process: underestimating the cost may result in \ninitiating costly, non-beneficial interactions, and overestimating the cost\nmight result in missing opportunities for valuable interactions. Any\nimprovement that can be achieved in predicting the cost values,\nespecially in the initial stages of learning, can make a significant\ndifference in performance, especially because the agent is severely\nlimited in the number of times it can interact with its owner in \nfastpaced domains.\nOne way to decrease the deviation from the actual value is by\naugmenting the data the CA acquires by observing its owner with\nobservations made by other owners\" agents. Such an approach \ndepends on identifying other owners with distribution functions for\nthe characteristic of interest similar to the CA\"s owner. This \ndataaugmentation idea is simple: different owners may exhibit \nsimilar basic behaviors or patterns in similar fast-paced task scenarios.\nSince they are all coordinating on a common overall task and are\noperating in the same environment, it is reasonable to assume some\nlevel of similarity in the distribution function of their modeled \nparameters. People vary in their behavior, so, obviously, there may\nbe different types of owners: some will emphasize communication\nwith their teams, and some will spend more time on map-based\nplanning; some will dislike being disturbed while trying to evaluate\ntheir team\"s progress, while others may be more open to \ninterruptions. Consequently, an owner\"s CA is likely to be able to find some\nCAs that are working with owners who are similar to its owner.\nWhen adopting data collected by other agents, the two main\nquestions are which agents the CA should rely on and to what \nextent it should rely on each of them. The selective-sharing \nmechanism relies on a statistical measure of similarity that allows the CA\nof any specific user to identify the similarity between its owner and\nother owners dynamically. Based on this similarity level, the CA\ndecides if and to what degree to import other CAs\" data in order to\naugment its direct observations, and thus to enable better modeling\nof its owner\"s characteristics.\nIt is notable that the cost of transferring observations between\ndifferent CA modules of different agents is relatively small. This\ninformation can be transferred as part of regular negotiation \ncommunication between agents. The volume of such communication\nis negligible: it involves just the transmission of new observations\"\nvalues.\nIn our learning mechanism, the CA constantly updates its \nestimation of the level of similarity between its owner and the owners\nrepresented by other CAs in the environment. Each new \nobservation obtained either by that CA or any of the other CAs updates this\nestimation. The similarity level is determined using the Wilcoxon\nrank-sum test (Subsection 3.1).\nWhenever it is necessary to produce a parameter estimate, the\nCA decides on the number of additional observations it intends to\nrely on for extracting its estimation. The number of additional \nobservations to be taken from each other agent is a function of the\nnumber of observations it currently has from former interactions\nwith its owner and the level of confidence the CA has in the \nsimilarity between its owner and other owners. In most cases, the \nnumber of observations the CA will want to take from another agent is\nsmaller than the overall number of observations the other agent has;\nthus, it randomly samples (without repetitions) the required \nnumber of observations from this other agent\"s database. The additional\nobservations the CA takes from other agents are used only to model\nits owner\"s characteristics. Future similarity level determination is\nnot affected by this information augmentation procedure.\n3.1 The Wilcoxon Test\nWe use a nonparametric method (i.e., one that makes no \nassumptions about the parametric form of the distributions each set is\ndrawn from), because user characteristics in fast-paced domains do\nnot have the structure needed for parametric approaches. Two \nadditional advantages of a non-parametric approach are their usefulness\nfor dealing with unexpected, outlying observations (possibly \nproblematic for a parametric approach), and the fact that non-parametric\napproaches are computationally very simple and thus ideal for \nsettings in which computational resources are scarce.\nThe Wilcoxon rank-sum test we use is a nonparametric \nalternative to the two-sample t-test [22, 14]2\n. While the t-test \ncompares means, the Wilcoxon test can be used to test the null \nhypothesis that two populations X and Y have the same continuous\ndistribution. We assume that we have independent random \nsamples {x1, x2, ..., xm} and {y1, y2, ..., yn}, of sizes m and n \nrespectively, from each population. We then merge the data and rank\neach measurement from lowest to highest. All sequences of ties are\nassigned an average rank. From the sum of the ranks of the smaller\n2\nChi-Square Goodness-of-Fit Test is for a single sample and thus not \nsuitable.\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 205\nsample, we calculate the test statistic and extract the level of \nconfidence for rejecting the null hypothesis. This level of confidence\nbecomes the measure for the level of similarity between the two\nowners. The Wilcoxon test does not require that the data originates\nfrom a normally distributed population or that the distribution is\ncharacterized by a finite set of parameters.\n3.2 Determining Required Information\nCorrectly identifying the right number of additional observations\nto gather is a key determinant of success of the selective-sharing\nmechanism. Obviously, if the CA can identify another owner who\nhas identical characteristics to the owner it represents, then it should\nuse all of the observations collected by that owner\"s agent. \nHowever, cases of identical matches are likely to be very uncommon.\nFurthermore, even to establish that another user is identical to its\nown, the CA would need substantial sample sizes to have a \nrelatively high level of confidence. Thus, usually the CA needs to\ndecide how much to rely on another agent\"s data while estimating\nvarious levels of similarity with a changing level of confidence.\nAt the beginning of its process, the selective-sharing mechanism\nhas almost no data to rely on, and thus no similarity measure can\nbe used. In this case, the CA module relies heavily on other agents,\nin the expectation that all owners have some basic level of \nsimilarity in their distribution (see Section 2). As the number of its\ndirect observations increases, the CA module refines the number of\nadditional observations required. Again, there are two conflicting\neffects. On one hand, the more data the CA has, the better it can\ndetermine its level of confidence in the similarity ratings it has for\nother owners. On the other hand, assuming there is some difference\namong owners (even if not noticed yet), as the number of its direct\nobservations increases, the owner\"s own data should gain weight in\nits analysis. Therefore, when CAi decides how many additional\nobservations, Oi\nj should be adopted from CAj\"s database, it \ncalculates Oi\nj as follows:\nOi\nj = N \u00e2\u02c6\u2014 (1 \u00e2\u02c6\u2019 \u00ce\u00b1i,j)\n\u00e2\u02c6\u0161\nN\n+\n2 + ln(N)\nN\n(5)\nwhere N is the number of observations CAi already has (which is\nsimilar in magnitude to the number of observations CAj has) and\n\u00ce\u00b1i,j is the confidence of rejecting the Wilcoxon null hypothesis.\nThe function in Equation 5 ensures that the number of additional\nobservations to be taken from another CA module increases as the\nconfidence in the similarity with the source for these additional \nobservations increases. At the same time, it ensures that the level of\ndependency on external observations decreases as the number of \ndirect observations increases. When calculating the parameter \u00ce\u00b1i,j,\nwe always perform the test over the interval relevant to the \noriginating CA\"s distribution function. For example, when estimating the\ncost of interrupting the user, we apply the Wilcoxon test only for\nobservations in the interval that starts from zero and ends slightly\nto the right of the formerly estimated RV (see Figure 1).\n4. EMPIRICAL SETTING\nWe tested the selective-sharing mechanism in a system that \nsimulates a distributed, Coordinators-like MAS. This testbed \nenvironment includes a variable number of agents, each corresponding to a\nsingle CA module. Each agent is assigned an external source \n(simulating an owner) which it periodically samples to obtain a value\nfrom the distribution being estimated. The simulation system \nenabled us to avoid unnecessary inter-agent scheduling and \ncommunication overhead (which are an inherent part of the Coordinators\nenvironment) and thus to better isolate the performance and \neffectiveness of the estimation and decision-making mechanisms.\nThe distribution functions used in the experiments (i.e., the \ndistribution functions assigned to each user in the simulated \nenvironment) are multi-rectangular shaped. This type of function is ideal\nfor representing empirical distribution functions. It is composed\nof k rectangles, where each rectangle i is defined over the interval\n(xi\u00e2\u02c6\u20191, xi), and represents a probability pi, (\nPk\ni=1 pi =1). For any\nvalue x in rectangle i, we can formulate F(x) and f(x) as:\nf(x) =\npi\nxi \u00e2\u02c6\u2019 xi\u00e2\u02c6\u20191\nF(x) =\ni\u00e2\u02c6\u20191X\nj=1\npj +\n(x \u00e2\u02c6\u2019 xi\u00e2\u02c6\u20191)pi\nxi \u00e2\u02c6\u2019 xi\u00e2\u02c6\u20191\n(6)\nFor example, the multi-rectangular function in Figure 3 depicts a\npossible interruption cost distribution for a specific user. Each \nrectangle is associated with one of the user\"s typical activities, \ncharacterized by a set of typical interruption costs. (We assume the\ndistribution of cost within each activity is uniform.) The \nrectangular area represents the probability of the user being engaged in\nthis type of activity when she is randomly interrupted. Any overlap\nbetween the interruption costs of two or more activities results in\na new rectangle for the overlapped interval. The user associated\nwith the above distribution function spends most of her time in \nreporting (notice that this is the largest rectangle in terms of area), an\nactivity associated with a relatively high cost of interruption. The\nuser also spends a large portion of her time in planning (associated\nwith a very high cost of interruption), monitoring his team (with a\nrelatively small interruption cost) and receiving reports (mid-level\ncost of interruption). The user spends a relatively small portion\nof her time in scouting the enemy (associated with relatively high\ninterruption cost) and resting.\nFigure 3: Representing interruption cost distribution using a\nmulti-rectangular function\nMulti-rectangular functions are modular and allow the \nrepresentation of any distribution shape by controlling the number and \ndimensions of the rectangles used. Furthermore, these functions have\ncomputational advantages, mostly due to the ability to re-use many\nof their components when calculating the optimal reservation value\nin economical search models. They also fit well the parameters\nthe CA is trying to estimate in fast-paced domains, because these\nparameters are mostly influenced by activities in which the user is\nengaged.\nThe testbed system enabled us to define either hand-crafted or\nautomatically generated multi-rectangular distribution functions. At\neach step of a simulation, each of the CAs samples its owner (i.e.,\nall CAs in the system collect data at a similar rate) and then \nestimates the parameter (either the expected cost when using the \nsequential interruption technique described in Section 2 or the \nprobability that the owner will have the required information) using one\nof the following methods: (a) relying solely on direct observation\n(self-learning) data; (b) relying on the combined data of all other\nagents (average all); and, (c) relying on its own data and \nselective portions of the other agents\" data based on the selective-sharing\nmechanism described in Section 3.\n5. RESULTS\nWe present the results in two parts: (1) using a specific \nsample environment for illustrating the basic behavior of the \nselectivesharing mechanism; and (2) using general environments that were\nautomatically generated.\n206 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\n5.1 Sample Environment\nTo illustrate the gain obtained by using the selective-sharing \nmechanism, we used an environment of 10 agents, associated with 5 \ndifferent interruptibility cost distribution function types. The table in\nFigure 4 details the division of the 10 agents into types, the \ndimensions of the rectangles that form the distribution functions, and\nthe theoretical mean and reservation value (RV) (following \nEquation 3) with a cost c = 2 for sensing the interruption cost. Even\nthough the means of the five types are relatively similar, the use\nof a reservation-value based interruption strategy yields relatively\ndifferent expected interruption costs (RV , following Equation 2).\nThe histogram in this figure depicts the number of observations \nobtained for each bin of size 1 out of a sample of 100000 observations\ntaken from each type\"s distribution function.\nType Agents Rect. Range prob mean RV\nI 1,2 1 0-20 0.40 50 14.1\n2 20-80 0.20\n3 80-100 0.40\nII 3,4,5,6 1 0-40 0.25 50 25.3\n2 40-60 0.50\n3 60-100 0.25\nIII 7 1 0-80 0.10 85 56.6\n2 80-100 0.90\nIV 8,9 1 0-60 0.60 48 20.0\n2 60-90 0.40\nV 10 1 0-100 1.00 50 20.0\n0\n500\n1000\n1500\n2000\n2500\n1 8 15 22 29 36 43 50 57 64 71 78 85 92 99\ntype I type II type III type IV type V\n#ofobservations\nrange\nFigure 4: Users\" interruptibility cost distribution functions (5\ntypes)\nFigure 5 gives CA performance in estimating the expected cost\nof interruption when using the reservation-value based interruption\ninitiation technique. Each graph presents the average prediction\naccuracy (in terms of the absolute deviation from the theoretical\nvalue, so the lower the curve the better the performance) of a \ndifferent type, based on 10000 simulation runs. The three curves in\neach graph represent the methods being compared (self-learning,\naverage all, and selective-sharing). The data is given as a function\nof the accumulated number of observations collected. The sixth\ngraph in the figure is the average for all types, weighted according\nto the number of agents of each type. Similarly, the following table\nsummarizes the overall average performance in terms of the \nabsolute deviation from the theoretical value of each of the different\nmethods:\nIterations Self-Learning Averaging-All Selective-Sharing % Improvement3\n5 20.08 8.70 9.51 53%\n15 12.62 7.84 8.14 36%\n40 8.16 7.42 6.35 22%\nTable 1: Average absolute error along time\nSeveral observations may be made from Figure 5. First, \nalthough the average-all method may produce relatively good results,\nit quickly reaches stagnation, while the other two methods exhibit\ncontinuous improvement as a function of the amount of \naccumulated data. For the Figure 4 environment, average-all is a good \nstrategy for agents of type II, IV and V, because the theoretical \nreservation value of each of these types is close to the one obtained based\non the aggregated distribution function (i.e., 21.27).4\nHowever, for\ntypes I and III for which the optimal RV differs from that value, the\naverage-all method performs significantly worse. Overall, the sixth\ngraph and the table above show that while in this specific \nenvironment the average-all method works well in the first interactions, it\n3\nThe improvement is measured in percentages relative to the self-learning\nmethod.\n4\nThe value is obtained by constructing the weighted aggregated distributed\nfunction according to the different agents\" types and extracting the optimal\nRV using Equation 3.\n0\n4\n8\n12\n16\n20\n1 6 11 16 21 26 31 36\nType I\n0\n4\n8\n12\n16\n20\n1 6 11 16 21 26 31 36\nselective sharing self-learning average all\n0\n4\n8\n12\n16\n20\n1 6 11 16 21 26 31 36\nType II\n0\n8\n16\n24\n32\n40\n1 6 11 16 21 26 31 36\nType III\n0\n4\n8\n12\n16\n20\n1 6 11 16 21 26 31 36\nType IV\n0\n4\n8\n12\n16\n20\n1 6 11 16 21 26 31 36\nType V\n0\n4\n8\n12\n16\n20\n1 6 11 16 21 26 31 36\nWeighted Average\nFigure 5: Average absolute deviation from the theoretical RV\nin each method (10000 runs)\nis quickly outperformed by the selective-sharing mechanism. \nFurthermore, the more user observations the agents accumulate (i.e., as\nwe extend the horizontal axis), the better the other two methods are\nin comparison to average-all. In the long run (and as shown in the\nfollowing subsection for the general case), the average-all method\nexhibits the worst performance.\nSecond, the selective-sharing mechanism starts with a significant\nimprovement in comparison to relying on the agent\"s own \nobservations, and then this improvement gradually decreases until finally\nits performance curve coincides with the self-learning method\"s\ncurve. The selective-sharing mechanism performs better or worse,\ndepending on the type, because the Wilcoxon test cannot guarantee\nan exact identification of similarity; different combinations of \ndistribution function can result in an inability to exactly identify the\nsimilar users for some of the specific types. For example, for type I\nagents, the selective-sharing mechanism actually performs worse\nthan self-learning in the short term (in the long run the two \nmethods\" performance converge). Nevertheless, for the other types in\nour example, the selective-sharing mechanism is the most efficient\none, and outperforms the other two methods overall.\nThird, it is notable that for agents that have a unique type (e.g.,\nagent III), the selective-sharing mechanism quickly converges \ntowards relying on self-collected data. This behavior guarantees that\neven in scenarios in which users are completely different, the method\nexhibits a graceful initial degradation but manages, within a few\ntime steps, to adopt the proper behavior of counting exclusively on\nself-generated data.\nLast, despite the difference in their overall distribution function,\nagents of type IV and V exhibit similar performance because the\nrelevant portion of their distribution functions (i.e., the effective\nparts that affect the RV calculation as explained in Figure 1) is\nidentical. Thus, the selective-sharing mechanism enables the agent\nof type V, despite its unique distribution function, to adopt \nrelevant information collected by agents of types IV which improves\nits estimation of the expected interruption cost.\n5.2 General Evaluation\nTo evaluate selective-sharing, we ran a series of simulations in\nwhich the environment was randomly generated. These \nexperiments focused on the CAs\" estimations of the probability that the\nuser would have the required information if interrupted. They used\na multi-rectangular probability distribution function to represent\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 207\nthe amount of communication the user is engaged in with its \nenvironment. We models the growth of the probability the user has\nthe required information as a function of the amount of \ncommunication using the logistic function,5\nG(x) =\n1 + e\n\u00e2\u02c6\u2019x\n12\n1 + 60e\n\u00e2\u02c6\u2019x\n12\n. (7)\nThe expected (mean) value of the parameter representing the\nprobability the user has the required information is thus\n\u00ce\u00bc =\nZ \u00e2\u02c6\u017e\ny=0\nG(y)f(y)dy =\nkX\ni=1\nhx + 708ln(60 + e\nx\n12 )pi\n60(xi \u00e2\u02c6\u2019 xi\u00e2\u02c6\u20191)\nixi\nxi\u00e2\u02c6\u20191\n(8)\nwhere k is the number of rectangles used. We ran 10000 simulation\nruns. For each simulation, a new 20-agent environment was \nautomatically generated by the system, and the agents were randomly\ndivided into a random number of different types.6\nFor each type,\na random 3-rectangle distribution function was generated. Each\nsimulation ran 40 time steps. At each time step each one of the\nagents accumulated one additional observation. Each CA \ncalculated an estimate of the probability its user had the necessary \ninformation according to the three methods, and the absolute error\n(difference from the theoretical value calculated according to \nEquation 8) was recorded. The following table summarizes the average\nperformance of the three mechanisms along different time horizons\n(measured at 5, 15 and 40 time steps):\nIterations Self-Learning Averaging-All Selective-Sharing % Improvement\n5 0.176 0.099 0.103 41.4%\n15 0.115 0.088 0.087 23.9%\n40 0.075 0.082 0.065 13.6%\nTable 2: Average absolute error along time steps\nAs can be seen in the table above, the proposed selective-sharing\nmethod outperforms the two other methods over any execution in\nwhich more than 15 observations are collected by each of the agents.\nAs in the sample environment, the average-all method performs\nwell in the initial few time steps, but does not exhibit further \nimprovement. Thus, the more data collected, the greater the difference\nbetween this latter method and the two other methods. The average\ndifference between selective-sharing and self-learning decreases as\nmore data is collected.\nFinally, we measured the effect of the number of types in the\nenvironment. For this purpose, we used the same self-generation\nmethod, but controlled the number of types generated for each run.\nThe number of types is a good indication for the level of \nheterogeneity in the environment. For each number of types, we ran\n10000 simulations. Figure 6 depicts the performance of the \ndifferent methods (for a 40-observation collection period for each agent).\nSince all simulation runs used for generating Figure 6 are based\non the same seed, the performance of the self-learning mechanism\nis constant regardless of the number of types in the environment. As\nexpected, the average-all mechanism performs best when all agents\nare of the same type; however its performance deteriorates as the\nnumber of types increases. Similarly, the selective-sharing \nmechanism exhibits good results when all agents are of the same type,\nand as the number of types increases, its performance deteriorates.\nHowever, the performance decrease is significantly more modest in\ncomparison to the one experienced in the average-all mechanism.\n5\nThe specific coefficients used guarantee an S-like curve of growth, along\nthe interval (0, 100), where the initial stage of growth is approximately\nexponential, followed by asymptotically slowing growth.\n6\nIn this suggested environment-generation scheme there is no guarantee\nthat every agent will have a potential similar agent to share information\nwith. In those non-rare scenarios where the CA is the only one of its type,\nit will rapidly need to stop relying on others.\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n1 2 3 4 5\nSelf Learning Average All Selective Sharing\nnumber of types\naverageabsoluteerror\nFigure 6: Average absolute deviation from actual value in 20\nagent scenarios as a function of the agents\" heterogeneity level\nOverall, the selective-sharing mechanism outperforms both other\nmethods for any number of types greater than one.\n6. RELATED WORK\nIn addition to the interruption management literature reviewed\nin Section 2, several other areas of prior work are relevant to the\nselective-sharing mechanism described in this paper.\nCollaborative filtering, which makes predictions (filtering) about\nthe interests of a user [7], operates similarly to selective-sharing.\nHowever, collaborative filtering systems exhibit poor performance\nwhen there is not sufficient information about the users and when\nthere is not sufficient information about a new user whose taste the\nsystem attempts to predict [7].\nSelective-sharing relies on the ability to find similarity between\nspecific parts of the probability distribution function associated with\na characteristic of different users. This capability is closely related\nto clustering and classification, an area widely studied in machine\nlearning. Given space considerations, our review of this area is \nrestricted to some representative approaches for clustering. In spite of\nthe richness of available clustering algorithms (such as the famous\nK-means clustering algorithm [11], hierarchical methods, Bayesian\nclassifiers [6], and maximum entropy), various characteristics of\nfast-paced domains do not align well with the features of \nattributesbased clustering mechanisms, suggesting these mechanisms would\nnot perform well in such domains. Of particular importance is that\nthe CA needs to find similarity between functions, defined over a\ncontinuous interval, with no distinct pre-defined attributes. An \nadditional difficulty is defining the distance measure.\nMany clustering techniques have been used in data mining [2],\nwith particular focus on incremental updates of the clustering, due\nto the very large size of the databases [3]. However the \napplicability of these to fast-paced domains is quite limited because they rely\non a large set of existing data. Similarly, clustering algorithms \ndesigned for the task of class identification in spatial databases (e.g.,\nrelying on a density-based notion [4]) are not useful for our case,\nbecause our data has no spatial attributes.\nThe most relevant method for our purposes is the Kullback-Leibler\nrelative entropy index that is used in probability theory and \ninformation theory [12]. This measure, which can also be applied on\ncontinuous random variables, relies on a natural distance measure\nfrom a true probability distribution (either observation-based or\ncalculated) to an arbitrary probability distribution. However, the\nmethod will perform poorly in scenarios in which the functions \nalternate between different levels while keeping the general \nstructure and moments. For example, consider the two functions f(x) =\n( x mod2)/100 and g(x) = ( x mod2)/100 defined over the \ninterval (0, 200). While these two functions are associated with \nalmost identical reservation values (for any sampling cost) and mean,\nthe Kullback-Leibler method will assign a poor correlation between\n208 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\nthem, while our Wilcoxon-based approach will give them the \nhighest rank in terms of similarity.\nWhile the Wilcoxon test is a widely used statistical procedure\n[22, 14], it is usually used for comparing two sets of single-variate\ndata. To our knowledge, no attempt has been made yet to \nextend its properties as an infrastructure for determining with whom\nand to what extent information should be shared, as presented in\nthis paper. Typical use of this non-parametric tool includes \ndetection of rare events in time series (e.g., a hard drive failure \nprediction [17]) and bioinformatics applications (e.g., finding informative\ngenes from microarray data). In these applications, it is used \nprimarily as an identification tool and ranking criterion.\n7. DISCUSSION AND CONCLUSIONS\nThe selective-sharing mechanism presented in this paper does\nnot make any assumptions about the format of the data used or\nabout the structure of the distribution function of the parameter to\nbe estimated. It is computationally lightweight and very simple to\nexecute. Selective-sharing allows an agent to benefit from other\nagents\" observations in scenarios in which data sources of the same\ntype are available. It also guarantees, as a fallback, performance\nequivalent to that of a self-learner when the information source is\nunique. Furthermore, selective-sharing does not require any prior\nknowledge about the types of information sources available in the\nenvironment or of the number of agents associated with each type.\nThe results of our simulations demonstrate the selective-sharing\nmechanism\"s effectiveness in improving the estimation produced\nfor probabilistic parameters based on a limited set of observations.\nFurthermore, most of the improvement is achieved in initial \ninteractions, which is of great importance for agents operating in\nfast-paced environments. Although we tested the selective-sharing\nmechanism in the context of the Coordinators project, it is \napplicable in any MAS environment having the characteristics of a\nfast-paced environment (e.g., rescue environments). Evidence for\nits general effectiveness is given in the general evaluation section,\nwhere environments were continuously randomly generated.\nThe Wilcoxon statistic used as described in this paper to provide\na classifier for similarity between users provides high flexibility\nwith low computational costs and is applicable for any \ncharacteristic being learned. Its use provides a good measure of similarity\nwhich an agent can use to decide how much external information\nto adopt for its assessments.\n8. ACKNOWLEDGEMENT\nThe research reported in this paper was supported in part by\ncontract number 55-000720, a subcontract to SRI International\"s\nDARPA Contract No. FA8750-05-C-0033. Any opinions, findings\nand conclusions, or recommendations expressed in this material\nare those of the authors and do not necessarily reflect the views of\nDARPA or the U.S. Government. We are grateful to an anonymous\nAAMAS reviewer for an exceptionally comprehensive review of\nthis paper.\n9. REFERENCES\n[1] P. Adamczyk, S. Iqbal, and B. Bailey. A method, system, and\ntools for intelligent interruption management. In TAMODIA\n\"05, pages 123-126, New York, NY, USA, 2005. ACM Press.\n[2] P. Berkhin. Survey of clustering data mining techniques.\nTechnical report, Accrue Software, San Jose, CA, 2002.\n[3] M. Ester, H. Kriegel, J. Sander, M. Wimmer, and X. Xu.\nIncremental clustering for mining in a data warehousing\nenvironment. In Proc. 24th Int. Conf. Very Large Data Bases,\nVLDB, pages 323-333, 24-27 1998.\n[4] M. Ester, H. Kriegel, J. Sander, and X. Xu. A density-based\nalgorithm for discovering clusters in large spatial databases\nwith noise. In KDD-96, pages 226-231, 1996.\n[5] M. Fleming and R. Cohen. A decision procedure for\nautonomous agents to reason about interaction with humans.\nIn AAAI Spring Symp. on Interaction between Humans and\nAutonomous Systems over Extended Operation, 2004.\n[6] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian\nnetwork classifiers. Machine Learning, 29:131-163, 1997.\n[7] N. Good, J. Ben Schafer, J. Konstan, A. Borchers, B. Sarwar,\nJ. Herlocker, and J. Riedl. Combining collaborative filtering\nwith personal agents for better recommendations. In\nAAAI/IAAI, pages 439-446, 1999.\n[8] K. Hinckley, J. Pierce, M. Sinclair, and E. Horvitz. Sensing\ntechniques for mobile interaction. In UIST \"00, pages\n91-100, New York, NY, USA, 2000. ACM Press.\n[9] E. Horvitz, C. Kadie, T. Paek, and D. Hovel. Models of\nattention in computing and communication: from principles\nto applications. Commun. ACM, 46(3):52-59, 2003.\n[10] B. Hui and C. Boutilier. Who\"s asking for help?: a bayesian\napproach to intelligent assistance. In IUI \"06, 2006.\n[11] J. Jang, C. Sun, and E. Mizutani. Neuro-Fuzzy and Soft\nComputing A Computational Approach to Learning and\nMachine Intelligence. Prentice Hall, 1997.\n[12] S. Kullback and R. Leibler. On information and sufficiency.\nAnn. Math. Statist., 22:79-86, 1951.\n[13] P. Maglio, T. Matlock, C. Campbell, S. Zhai, and B. Smith.\nGaze and speech in attentive user interfaces. In ICMI, pages\n1-7, 2000.\n[14] H. Mann and D. Whitney. On a test of whether one of 2\nrandom variables is stochastically larger than the other.\nAnnals of Mathematical Statistics, 18:50-60, 1947.\n[15] W. McClure. Technology and command: Implications for\nmilitary operations in the twenty-first century. Maxwell Air\nForce Base, Center for Strategy and Technology, 2000.\n[16] J. McMillan and M. Rothschild. Search. In Robert J. Aumann\nand Amsterdam Sergiu Hart, editors, Handbook of Game\nTheory with Economic Applications, pages 905-927. 1994.\n[17] J. Murray, G. Hughes, and K. Kreutz-Delgado. Machine\nlearning methods for predicting failures in hard drives: A\nmultiple-instance application. J. Mach. Learn. Res.,\n6:783-816, 2005.\n[18] D. Sarne and B. J. Grosz. Estimating information value in\ncollaborative multi-agent planning systems. In AAMAS\"07,\npage (to appear), 2007.\n[19] D. Sarne and B. J. Grosz. Timing interruptions for better\nhuman-computer coordinated planning. In AAAI Spring\nSymp. on Distributed Plan and Schedule Management, 2006.\n[20] R. Vertegaal. The GAZE groupware system: Mediating joint\nattention in multiparty communication and collaboration. In\nCHI, pages 294-301, 1999.\n[21] T. Wagner, J. Phelps, V. Guralnik, and R. VanRiper. An\napplication view of coordinators: Coordination managers for\nfirst responders. In AAAI, pages 908-915, 2004.\n[22] F Wilcoxon. Individual comparisons by ranking methods.\nBiometrics, 1:80-83, 1945.\n[23] D. Zeng and K. Sycara. Bayesian learning in negotiation. In\nAAAI Symposium on Adaptation, Co-evolution and Learning\nin Multiagent Systems, pages 99-104, 1996.\n[24] Y. Zhang, K. Biggers, L. He, S. Reddy, D. Sepulvado, J. Yen,\nand T. Ioerger. A distributed intelligent agent architecture for\nsimulating aggregate-level behavior and interactions on the\nbattlefield. In SCI-2001, pages 58-63, 2001.\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 209\n": ["probabilistic parameter", "agent", "information sharing", "decision making", "fast-paced environment", "multi-agent distributed system", "learning mechanism", "selective-sharing", "parameter estimation", "adjustable autonomy", "interruption management", ""]}