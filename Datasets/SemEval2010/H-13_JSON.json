{"The Influence of Caption Features on\nClickthrough Patterns in Web Search\nCharles L. A. Clarke Eugene Agichtein Susan Dumais and Ryen W. White\nUniversity of Waterloo Emory University Microsoft Research\nABSTRACT\nWeb search engines present lists of captions, comprising \ntitle, snippet, and URL, to help users decide which search\nresults to visit. Understanding the influence of features of\nthese captions on Web search behavior may help validate\nalgorithms and guidelines for their improved generation. In\nthis paper we develop a methodology to use clickthrough\nlogs from a commercial search engine to study user behavior\nwhen interacting with search result captions. The findings\nof our study suggest that relatively simple caption features\nsuch as the presence of all terms query terms, the \nreadability of the snippet, and the length of the URL shown in the\ncaption, can significantly influence users\" Web search \nbehavior.\nCategories and Subject Descriptors\nH.3.3 [Information Storage and Retrieval]: Information\nSearch and Retrieval-search process\nGeneral Terms\nExperimentation, Human Factors\n1. INTRODUCTION\nThe major commercial Web search engines all present\ntheir results in much the same way. Each search result is\ndescribed by a brief caption, comprising the URL of the \nassociated Web page, a title, and a brief summary (or \nsnippet) describing the contents of the page. Often the snippet\nis extracted from the Web page itself, but it may also be\ntaken from external sources, such as the human-generated\nsummaries found in Web directories.\nFigure 1 shows a typical Web search, with captions for the\ntop three results. While the three captions share the same\nbasic structure, their content differs in several respects. The\nsnippet of the third caption is nearly twice as long as that\nof the first, while the snippet is missing entirely from the\nsecond caption. The title of the third caption contains all\nof the query terms in order, while the titles of the first and\nsecond captions contain only two of the three terms. One of\nthe query terms is repeated in the first caption. All of the\nquery terms appear in the URL of the third caption, while\nnone appear in the URL of the first caption. The snippet\nof the first caption consists of a complete sentence that \nconcisely describes the associated page, while the snippet of the\nthird caption consists of two incomplete sentences that are\nlargely unrelated to the overall contents of the associated\npage and to the apparent intent of the query.\nWhile these differences may seem minor, they may also\nhave a substantial impact on user behavior. A principal\nmotivation for providing a caption is to assist the user in\ndetermining the relevance of the associated page without\nactually having to click through to the result. In the case of\na navigational query - particularly when the destination is\nwell known - the URL alone may be sufficient to identify\nthe desired page. But in the case of an informational query,\nthe title and snippet may be necessary to guide the user in\nselecting a page for further study, and she may judge the\nrelevance of a page on the basis of the caption alone.\nWhen this judgment is correct, it can speed the search\nprocess by allowing the user to avoid unwanted material.\nWhen it fails, the user may waste her time clicking through\nto an inappropriate result and scanning a page containing\nlittle or nothing of interest. Even worse, the user may be\nmisled into skipping a page that contains desired \ninformation.\nAll three of the results in figure 1 are relevant, with some\nlimitations. The first result links to the main Yahoo Kids!\nhomepage, but it is then necessary to follow a link in a menu\nto find the main page for games. Despite appearances, the\nsecond result links to a surprisingly large collection of \nonline games, primarily with environmental themes. The third\nresult might be somewhat disappointing to a user, since it\nleads to only a single game, hosted at the Centers for Disease\nControl, that could not reasonably be described as online.\nUnfortunately, these page characteristics are not entirely \nreflected in the captions.\nIn this paper, we examine the influence of caption \nfeatures on user\"s Web search behavior, using clickthroughs\nextracted from search engines logs as our primary \ninvestigative tool. Understanding this influence may help to validate\nalgorithms and guidelines for the improved generation of the\nFigure 1: Top three results for the query: kids online games.\ncaptions themselves. In addition, these features can play a\nrole in the process of inferring relevance judgments from user\nbehavior [1]. By better understanding their influence, better\njudgments may result.\nDifferent caption generation algorithms might select \nsnippets of different lengths from different areas of a page. \nSnippets may be generated in a query-independent fashion, \nproviding a summary of the page as a whole, or in a \nquerydependent fashion, providing a summary of how the page\nrelates to the query terms. The correct choice of snippet\nmay depend on aspects of both the query and the result\npage. The title may be taken from the HTML header or\nextracted from the body of the document [8]. For links that\nre-direct, it may be possible to display alternative URLs.\nMoreover, for pages listed in human-edited Web directories\nsuch as the Open Directory Project1\n, it may be possible\nto display alternative titles and snippets derived from these\nlistings.\nWhen these alternative snippets, titles and URLs are \navailable, the selection of an appropriate combination for display\nmay be guided by their features. A snippet from a Web \ndirectory may consist of complete sentences and be less \nfragmentary than an extracted snippet. A title extracted from\nthe body may provide greater coverage of the query terms.\nA URL before re-direction may be shorter and provide a\nclearer idea of the final destination.\nThe work reported in this paper was undertaken in the\ncontext of the Windows Live search engine. The image in \nfigure 1 was captured from Windows Live and cropped to \neliminate branding, advertising and navigational elements. The\nexperiments reported in later sections are based on \nWindows Live query logs, result pages and relevance judgments\ncollected as part of ongoing research into search engine \nperformance [1,2]. Nonetheless, given the similarity of caption\nformats across the major Web search engines we believe the\nresults are applicable to these other engines. The query in\n1\nwww.dmoz.org\nfigure 1 produces results with similar relevance on the other\nmajor search engines. This and other queries produce \ncaptions that exhibit similar variations. In addition, we believe\nour methodology may be generalized to other search \napplications when sufficient clickthrough data is available.\n2. RELATED WORK\nWhile commercial Web search engines have followed \nsimilar approaches to caption display since their genesis, \nrelatively little research has been published about methods for\ngenerating these captions and evaluating their impact on\nuser behavior. Most related research in the area of document\nsummarization has focused on newspaper articles and \nsimilar material, rather than Web pages, and has conducted \nevaluations by comparing automatically generated summaries\nwith manually generated summaries. Most research on the\ndisplay of Web results has proposed substantial interface\nchanges, rather than addressing details of the existing \ninterfaces.\n2.1 Display of Web results\nVaradarajan and Hristidis [16] are among the few who\nhave attempted to improve directly upon the snippets \ngenerated by commercial search systems, without introducing\nadditional changes to the interface. They generated \nsnippets from spanning trees of document graphs and \nexperimentally compared these snippets against the snippets \ngenerated for the same documents by the Google desktop search\nsystem and MSN desktop search system. They evaluated\ntheir method by asking users to compare snippets from the\nvarious sources.\nCutrell and Guan [4] conducted an eye-tracking study to\ninvestigate the influence of snippet length on Web search\nperformance and found that the optimal snippet length \nvaried according to the task type, with longer snippets leading\nto improved performance for informational tasks and shorter\nsnippets for navigational tasks.\nMany researchers have explored alternative methods for\ndisplaying Web search results. Dumais et al. [5] compared an\ninterface typical of those used by major Web search engines\nwith one that groups results by category, finding that users\nperform search tasks faster with the category interface. Paek\net al. [12] propose an interface based on a fisheye lens, in\nwhich mouse hovers and other events cause captions to zoom\nand snippets to expand with additional text.\nWhite et al. [17] evaluated three alternatives to the \nstandard Web search interface: one that displays expanded \nsummaries on mouse hovers, one that displays a list of top \nranking sentences extracted from the results taken as a group,\nand one that updates this list automatically through \nimplicit feedback. They treat the length of time that a user\nspends viewing a summary as an implicit indicator of \nrelevance. Their goal was to improve the ability of users to\ninteract with a given result set, helping them to look \nbeyond the first page of results and to reduce the burden of\nquery re-formulation.\n2.2 Document summarization\nOutside the narrow context of Web search considerable \nrelated research has been undertaken on the problem of \ndocument summarization. The basic idea of extractive \nsummarization - creating a summary by selecting sentences or\nfragments - goes back to the foundational work of Luhn [11].\nLuhn\"s approach uses term frequencies to identify \nsignificant words within a document and then selects and extracts\nsentences that contain significant words in close proximity.\nA considerable fraction of later work may be viewed as\nextending and tuning this basic approach, developing \nimproved methods for identifying significant words and \nselecting sentences. For example, a recent paper by Sun et\nal. [14] describes a variant of Luhn\"s algorithm that uses\nclickthrough data to identify significant words. At its \nsimplest, snippet generation for Web captions might also be\nviewed as following this approach, with query terms taking\non the role of significant words.\nSince 2000, the annual Document Understanding \nConference (DUC) series, conducted by the US National Institute\nof Standards and Technology, has provided a vehicle for\nevaluating much of the research in document \nsummarization2\n. Each year DUC defines a methodology for one or\nmore experimental tasks, and supplies the necessary test\ndocuments, human-created summaries, and automatically\nextracted baseline summaries. The majority of \nparticipating systems use extractive summarization, but a number\nattempt natural language generation and other approaches.\nEvaluation at DUC is achieved through comparison with\nmanually generated summaries. Over the years DUC has\nincluded both single-document summarization and \nmultidocument summarization tasks. The main DUC 2007 task\nis posed as taking place in a question answering context.\nGiven a topic and 25 documents, participants were asked\nto generate a 250-word summary satisfying the information\nneed enbodied in the topic. We view our approach of \nevaluating summarization through the analysis of Web logs as\ncomplementing the approach taken at DUC.\nA number of other researchers have examined the value\nof query-dependent summarization in a non-Web context.\nTombros and Sanderson [15] compared the performance of\n20 subjects searching a collection of newspaper articles when\n2\nduc.nist.gov\nguided by query-independent vs. query-dependent snippets.\nThe query-independent snippets were created by extracting\nthe first few sentences of the articles; the query-dependent\nsnippets were created by selecting the highest scoring \nsentences under a measure biased towards sentences containing\nquery terms. When query-dependent summaries were \npresented, subjects were better able to identify relevant \ndocuments without clicking through to the full text.\nGoldstein et al. [6] describe another extractive system for\ngenerating query-dependent summaries from newspaper \narticles. In their system, sentences are ranked by combining\nstatistical and linguistic features. They introduce \nnormalized measures of recall and precision to facilitate evaluation.\n2.3 Clickthroughs\nQueries and clickthroughs taken from the logs of \ncommercial Web search engines have been widely used to improve\nthe performance of these systems and to better understand\nhow users interact with them. In early work, Broder [3]\nexamined the logs of the AltaVista search engine and \nidentified three broad categories of Web queries: informational,\nnavigational and transactional. Rose and Levinson [13] \nconducted a similar study, developing a hierarchy of query goals\nwith three top-level categories: informational, navigational\nand resource. Under their taxonomy, a transactional query\nas defined by Broder might fall under either of their three\ncategories, depending on details of the desired transaction.\nLee et al. [10] used clickthrough patterns to \nautomatically categorize queries into one of two categories: \ninformational - for which multiple Websites may satisfy all or part\nof the user\"s need - and navigational - for which users\nhave a particular Website in mind. Under their taxonomy,\na transactional or resource query would be subsumed under\none of these two categories.\nAgichtein et al. interpreted caption features, clickthroughs\nand other user behavior as implicit feedback to learn \npreferences [2] and improve ranking [1] in Web search. Xue et\nal. [18] present several methods for associating queries with\ndocuments by analyzing clickthrough patterns and links \nbetween documents. Queries associated with documents in\nthis way are treated as meta-data. In effect, they are added\nto the document content for indexing and ranking purposes.\nOf particular interest to us is the work of Joachims et\nal. [9] and Granka et al. [7]. They conducted eye-tracking\nstudies and analyzed log data to determine the extent to\nwhich clickthrough data may be treated as implicit relevance\njudgments. They identified a trust bias, which leads users\nto prefer the higher ranking result when all other factors are\nequal. In addition, they explored techniques that treat clicks\nas pairwise preferences. For example, a click at position\nN + 1 - after skipping the result at position N - may be\nviewed as a preference for the result at position N+1 relative\nto the result at position N. These findings form the basis of\nthe clickthrough inversion methodology we use to interpret\nuser interactions with search results. Our examination of\nlarge search logs compliments their detailed analysis of a\nsmaller number of participants.\n3. CLICKTHROUGH INVERSIONS\nWhile other researchers have evaluated the display of Web\nsearch results through user studies - presenting users with\na small number of different techniques and asking them to\ncomplete experimental tasks - we approach the problem\nby extracting implicit feedback from search engine logs. \nExamining user behavior in situ allows us to consider many\nmore queries and caption characteristics, with the volume\nof available data compensating for the lack of a controlled\nlab environment.\nThe problem remains of interpreting the information in\nthese logs as implicit indicators of user preferences, and in\nthis matter we are guided by the work of Joachims et al. [9].\nWe consider caption pairs, which appear adjacent to one\nanother in the result list.\nOur primary tool for examining the influence of caption\nfeatures is a type of pattern observed with respect to these\ncaption pairs, which we call a clickthrough inversion. A\nclickthrough inversion occurs at position N when the result\nat position N receives fewer clicks than the result at position\nN + 1. Following Joachims et al. [9], we interpret a \nclickthrough inversion as indicating a preference for the lower\nranking result, overcoming any trust bias. For simplicity,\nin the remainder of this paper we refer to the higher \nranking caption in a pair as caption A and the lower ranking\ncaption as caption B.\n3.1 Extracting clickthroughs\nFor the experiments reported in this paper, we sampled\na subset of the queries and clickthroughs from the logs of\nthe Windows Live search engine over a period of 3-4 days\non three separate occasions: once for results reported in \nsection 3.3, once for a pilot of our main experiment, and once\nfor the experiment itself (sections 4 and 5). For simplicity\nwe restricted our sample to queries submitted to the US \nEnglish interface and ignored any queries containing complex\nor non-alphanumeric terms (e.g. operators and phrases). At\nthe end of each sampling period, we downloaded captions\nfor the queries associated with the clickthrough sample.\nWhen identifying clickthroughs in search engine logs, we\nconsider only the first clickthrough action taken by a user\nafter entering a query and viewing the result page. Users\nare identified by IP address, which is a reasonably reliable\nmethod of eliminating multiple results from a single user,\nat the cost of falsely eliminating results from multiple users\nsharing the same address.\nBy focusing on the initial clickthrough, we hope to \ncapture a user\"s impression of the relative relevance within a\ncaption pair when first encountered. If the user later clicks\non other results or re-issues the same query, we ignore these\nactions. Any preference captured by a clickthrough \ninversion is therefore a preference among a group of users issuing\na particular query, rather than a preference on the part of a\nsingle user. In the remainder of the paper, we use the term\nclickthrough to refer only to this initial action.\nGiven the dynamic nature of the Web and the volumes of\ndata involved, search engine logs are bound to contain \nconsiderable noise. For example, even over a period of hours\nor minutes the order of results for a given query can change,\nwith some results dropping out of the top ten and new ones\nappearing. For this reason, we retained clickthroughs for\na specific combination of a query and a result only if this\nresult appears in a consistent position for at least 50% of\nthe clickthroughs. Clickthroughs for the same result when\nit appeared at other positions were discarded. For \nsimilar reasons, if we did not detect at least ten clickthroughs\nfor a particular query during the sampling period, no \nclickthroughs for that query were retained.\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n1 2 3 4 5 6 7 8 9 10\nclickthroughpercent\nposition\na) craigslist\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n1 2 3 4 5 6 7 8 9 10\nclickthroughpercent\nposition\nb) periodic table of elements\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n1 2 3 4 5 6 7 8 9 10\nclickthroughpercent\nposition\nc) kids online games\nFigure 2: Clickthrough curves for three queries: a)\na stereotypical navigational query, b) a stereotypical\ninformational query, and c) a query exhibiting \nclickthrough inversions.\nThe outcome at the end of each sampling period is a set\nof records, with each record describing the clickthroughs for\na given query/result combination. Each record includes a\nquery, a result position, a title, a snippet, a URL, the \nnumber of clickthroughs for this result, and the total number of\nclickthroughs for this query. We then processed this set to\ngenerate clickthrough curves and identify inversions.\n3.2 Clickthrough curves\nIt could be argued that under ideal circumstances, \nclickthrough inversions would not be present in search engine\nlogs. A hypothetical perfect search engine would respond\nto a query by placing the result most likely to be relevant\nfirst in the result list. Each caption would appropriately\nsummarize the content of the linked page and its \nrelationship to the query, allowing users to make accurate \njudgments. Later results would complement earlier ones, linking\nto novel or supplementary material, and ordered by their\ninterest to the greatest number of users.\nFigure 2 provides clickthrough curves for three example\nqueries. For each example, we plot the percentage of \nclickthroughs against position for the top ten results. The first\nquery (craigslist) is stereotypically navigational, showing\na spike at the correct answer (www.craigslist.org). The\nsecond query is informational in the sense of Lee et al. [10]\n(periodic table of elements). Its curve is flatter and less\nskewed toward a single result. For both queries, the number\nof clickthroughs is consistent with the result positions, with\nthe percentage of clickthroughs decreasing monotonically as\nposition increases, the ideal behavior.\nRegrettably, no search engine is perfect, and clickthrough\ninversions are seen for many queries. For example, for the\nthird query (kids online games) the clickthrough curve \nexhibits a number of clickthrough inversions, with an apparent\npreference for the result at position 4.\nSeveral causes may be enlisted to explain the presence of\nan inversion in a clickthrough curve. The search engine may\nhave failed in its primary goal, ranking more relevant results\nbelow less relevant results. Even when the relative ranking\nis appropriate, a caption may fail to reflect the content of\nthe underlying page with respect to the query, leading the\nuser to make an incorrect judgment. Before turning to the\nsecond case, we address the first, and examine the extent to\nwhich relevance alone may explain these inversions.\n3.3 Relevance\nThe simplest explanation for the presence of a clickthrough\ninversion is a relevance difference between the higher \nranking member of caption pair and the lower ranking member.\nIn order to examine the extent to which relevance plays a\nrole in clickthrough inversions, we conducted an initial \nexperiment using a set of 1,811 queries with associated \njudgments created as part of on-going work. Over a four-day \nperiod, we sampled the search engine logs and extracted over\none hundred thousand clicks involving these queries. From\nthese clicks we identified 355 clickthrough inversions, \nsatisfying the criteria of section 3.1, where relevance judgments\nexisted for both pages.\nThe relevance judgments were made by independent \nassessors viewing the pages themselves, rather than the captions.\nRelevance was assessed on a 6-point scale. The outcome is\npresented in figure 3, which shows the explicit judgments\nfor the 355 clickthrough inversions. In all of these cases,\nthere were more clicks on the lower ranked member of the\nRelationship Number Percent\nrel(A) < rel(B) 119 33.5%\nrel(A) = rel(B) 134 37.7%\nrel(A) > rel(B) 102 28.7%\nFigure 3: Relevance relationships at clickthrough \ninversions. Compares relevance between the higher\nranking member of a caption pair (rel(A)) to the \nrelevance of the lower ranking member (rel(B)), where\ncaption A received fewer clicks than caption B.\npair (B). The figure shows the corresponding relevance \njudgments. For example, the first row rel(A) < rel(B), indicates\nthat the higher ranking member of pair (A) was rated as\nless relevant than the lower ranking member of the pair (B).\nAs we see in the figure, relevance alone appears inadequate\nto explain the majority of clickthrough inversions. For \ntwothirds of the inversions (236), the page associated with \ncaption A is at least as relevant as the page associated with \ncaption B. For 28.7% of the inversions, A has greater relevance\nthan B, which received the greater number of clickthroughs.\n4. INFLUENCE OF CAPTION FEATURES\nHaving demonstrated that clickthrough inversions cannot\nalways be explained by relevance differences, we explore\nwhat features of caption pairs, if any, lead users to prefer\none caption over another. For example, we may \nhypothesize that the absence of a snippet in caption A and the\npresence of a snippet in caption B (e.g. captions 2 and 3\nin figure 1) leads users to prefer caption A. Nonetheless,\ndue to competing factors, a large set of clickthrough \ninversions may also include pairs where the snippet is missing in\ncaption B and not in caption A. However, if we compare a\nlarge set of clickthrough inversions to a similar set of pairs\nfor which the clickthroughs are consistent with their \nranking, we would expect to see relatively more pairs where the\nsnippet was missing in caption A.\n4.1 Evaluation methodology\nFollowing this line of reasoning, we extracted two sets\nof caption pairs from search logs over a three day period.\nThe first is a set of nearly five thousand clickthrough \ninversions, extracted according to the procedure described in\nsection 3.1. The second is a corresponding set of caption\npairs that do not exhibit clickthrough inversions. In other\nwords, for pairs in this set, the result at the higher rank\n(caption A) received more clickthroughs than the result at\nthe lower rank (caption B). To the greatest extent possible,\neach pair in the second set was selected to correspond to a\npair in the first set, in terms of result position and number\nof clicks on each result. We refer to the first set, containing\nclickthrough inversions, as the INV set; we refer to the \nsecond set, containing caption pairs for which the clickthroughs\nare consistent with their rank order, as the CON set.\nWe extract a number of features characterizing snippets\n(described in detail in the next section) and compare the\npresence of each feature in the INV and CON sets. We\ndescribe the features as a hypothesized preference (e.g., a\npreference for captions containing a snippet). Thus, in \neither set, a given feature may be present in one of two forms:\nfavoring the higher ranked caption (caption A) or favoring\nthe lower ranked caption (caption B). For example, the \nabFeature Tag Description\nMissingSnippet snippet missing in caption A and present in caption B\nSnippetShort short snippet in caption A (< 25 characters) with long snippet (> 100 characters) in caption B\nTermMatchTitle title of caption A contains matches to fewer query terms than the title of caption B\nTermMatchTS title+snippet of caption A contains matches to fewer query terms than the title+snippet of caption B\nTermMatchTSU title+snippet+URL of caption A contains matches to fewer query terms than caption B\nTitleStartQuery title of caption B (but not A) starts with a phrase match to the query\nQueryPhraseMatch title+snippet+url contains the query as a phrase match\nMatchAll caption B contains one match to each term; caption A contains more matches with missing terms\nURLQuery caption B URL is of the form www.query.com where the query matches exactly with spaces removed\nURLSlashes caption A URL contains more slashes (i.e. a longer path length) than the caption B URL\nURLLenDIff caption A URL is longer than the caption B URL\nOfficial title or snippet of caption B (but not A) contains the term official (with stemming)\nHome title or snippet of caption B (but not A) contains the phrase home page\nImage title or snippet of caption B (but not A) contains a term suggesting the presence of an image gallery\nReadable caption B (but not A) passes a simple readability test\nFigure 4: Features measured in caption pairs (caption A and caption B), with caption A as the higher ranked\nresult. These features are expressed from the perspective of the prevalent relationship predicted for clickthrough\ninversions.\nsence of a snippet in caption A favors caption B, and the\nabsence of a snippet in caption B favors caption A. When\nthe feature favors caption B (consistent with a clickthrough\ninversion) we refer to the caption pair as a positive pair.\nWhen the feature favors caption A, we refer to it as a \nnegative pair. For missing snippets, a positive pair has the\ncaption missing in caption A (but not B) and a negative\npair has the caption missing in B (but not A).\nThus, for a specific feature, we can construct four subsets:\n1) INV+, the set of positive pairs from INV; 2) INV\u00e2\u02c6\u2019, the\nset of negative pairs from INV; 3) CON+; the set of positive\npairs from CON; and 4) CON\u00e2\u02c6\u2019 the set of negative pairs\nfrom CON. The sets INV+, INV\u00e2\u02c6\u2019, CON+, and CON\u00e2\u02c6\u2019 will\ncontain different subsets of INV and CON for each feature.\nWhen stating a feature corresponding to a hypothesized user\npreference, we follow the practice of stating the feature with\nthe expectation that the size of INV+ relative to the size\nof INV\u00e2\u02c6\u2019 should be greater than the size of CON+ relative\nto the size of CON\u00e2\u02c6\u2019. For example, we state the missing\nsnippet feature as snippet missing in caption A and present\nin caption B.\nThis evaluation methodology allows us to construct a \ncontingency table for each feature, with INV essentially forming\nthe experimental group and CON the control group. We can\nthen apply Pearson\"s chi-square test for significance.\n4.2 Features\nFigure 4 lists the features tested. Many of the features on\nthis list correspond to our own assumptions regarding the\nimportance of certain caption characteristics: the presence\nof query terms, the inclusion of a snippet, and the \nimportance of query term matches in the title. Other features\nsuggested themselves during the examination of the snippets\ncollected as part of the study described in section 3.3 and\nduring a pilot of the evaluation methodology (section 4.1).\nFor this pilot we collected INV and CON sets of similar sizes,\nand used these sets to evaluate a preliminary list of features\nand to establish appropriate parameters for the \nSnippetShort and Readable features. In the pilot, all of the features\nlist in figure 4 were significant at the 95% level. A small\nnumber of other features were dropped after the pilot.\nThese features all capture simple aspects of the captions.\nThe first feature concerns the existence of a snippet and the\nsecond concerns the relative size of snippets. Apart from\nthis first feature, we ignore pairs where one caption has a\nmissing snippet. These pairs are not included in the sets\nconstructed for the remaining features, since captions with\nmissing snippets do not contain all the elements of a \nstandard caption and we wanted to avoid their influence.\nThe next six features concern the location and number of\nmatching query terms. For the first five, a match for each\nquery term is counted only once, additional matches for the\nsame term are ignored. The MatchAll feature tests the idea\nthat matching all the query terms exactly once is preferable\nto matching a subset of the terms many times with a least\none query term unmatched.\nThe next three features concern the URLs, capturing \naspects of their length and complexity, and the last four \nfeatures concern caption content. The first two of these content\nfeatures (Official and Home) suggest claims about the \nimportance or significance of the associated page. The third\ncontent feature (Image) suggests the presence of an image\ngallery, a popular genre of Web page. Terms represented by\nthis feature include pictures, pics, and gallery.\nThe last content feature (Readable) applies an ad-hoc\nreadability metric to each snippet. Regular users of Web\nsearch engines may notice occasional snippets that consist\nof little more than lists of words and phrases, rather than a\ncoherent description. We define our own metric, since the\nFlesch-Kincaid readability score and similar measures are \nintended for entire documents not text fragments. While the\nmetric has not been experimentally validated, it does reflect\nour intuitions and observations regarding result snippets. In\nEnglish, the 100 most frequent words represent about 48%\nof text, and we would expect readable prose, as opposed to\na disjointed list of words, to contain these words in roughly\nthis proportion. The Readable feature computes the \npercentage of these top-100 words appearing in each caption.\nIf these words represent more than 40% of one caption and\nless than 10% of the other, the pair is included in the \nappropriate set.\nFeature Tag INV+ INV\u00e2\u02c6\u2019 %+ CON+ CON\u00e2\u02c6\u2019 %+ \u00cf\u20212\np-value\nMissingSnippet 185 121 60.4 144 133 51.9 4.2443 0.0393\nSnippetShort 20 6 76.9 12 16 42.8 6.4803 0.0109\nTermMatchTitle 800 559 58.8 660 700 48.5 29.2154 <.0001\nTermMatchTS 310 213 59.2 269 216 55.4 1.4938 0.2216\nTermMatchTSU 236 138 63.1 189 149 55.9 3.8088 0.0509\nTitleStartQuery 1058 933 53.1 916 1096 45.5 23.1999 <.0001\nQueryPhraseMatch 465 346 57.3 427 422 50.2 8.2741 0.0040\nMatchAll 8 2 80.0 1 4 20.0 0.0470\nURLQuery 277 188 59.5 159 315 33.5 63.9210 <.0001\nURLSlashes 1715 1388 55.2 1380 1758 43.9 79.5819 <.0001\nURLLenDiff 2288 2233 50.6 2062 2649 43.7 43.2974 <.0001\nOfficial 215 142 60.2 133 215 38.2 34.1397 <.0001\nHome 62 49 55.8 64 82 43.8 3.6458 0.0562\nImage 391 270 59.1 315 335 48.4 15.0735 <.0001\nReadable 52 43 54.7 31 48 39.2 4.1518 0.0415\nFigure 5: Results corresponding to the features listed in figure 4 with \u00cf\u20212\nand p-values (df = 1). Features supported\nat the 95% confidence level are bolded. The p-value for the MatchAll feature is computed using Fisher\"s Exact\nTest.\n4.3 Results\nFigure 5 presents the results. Each row lists the size of\nthe four sets (INV+, INV\u00e2\u02c6\u2019, CON+, and CON\u00e2\u02c6\u2019) for a given\nfeature and indicates the percentage of positive pairs (%+)\nfor INV and CON. In order to reject the null hypothesis,\nthis percentage should be significantly greater for INV than\nCON. Except in one case, we applied the chi-squared test\nof independence to these sizes, with p-values shown in the\nlast column. For the MatchAll feature, where the sum of\nthe set sizes is 15, we applied Fisher\"s exact test. Features\nsupported at the 95% confidence level are bolded.\n5. COMMENTARY\nThe results support claims that missing snippets, short\nsnippets, missing query terms and complex URLs negatively\nimpact clickthroughs. While this outcome may not be \nsurprising, we are aware of no other work that can provide \nsupport for claims of this type in the context of a commercial\nWeb search engine.\nThis work was originally motivated by our desire to \nvalidate some simple guidelines for the generation of \ncaptionssummarizing opinions that we formulated while working on\nrelated issues. While our results do not direct address all\nof the many variables that influence users understanding\nof captions, they are consistent with the major guidelines.\nFurther work is needed to provide additional support for\nthe guidelines and to understand the relationships among\nvariables.\nThe first of these guidelines underscores the importance of\ndisplaying query terms in context: Whenever possible all of\nthe query terms should appear in the caption, reflecting their\nrelationship to the associated page. If a query term is \nmissing from a caption, the user may have no idea why the result\nwas returned. The results for the MatchAll feature directly\nsupport this guideline. The results for TermMatchTitle and\nTermMatchTSU confirm that matching more terms is \ndesirable. Other features provide additional indirect support for\nthis guideline, and none of the results are inconsistent with\nit.\nA second guideline speaks to the desirability of \npresenting the user with a readable snippet: When query terms are\npresent in the title, they need not be repeated in the \nsnippet. In particular, when a high-quality query-independent\nsummary is available from an external source, such as a\nWeb directory, it may be more appropriate to display this\nsummary than a lower-quality query-dependent fragment \nselected on-the-fly. When titles are available from multiple\nsources -the header, the body, Web directories - a caption\ngeneration algorithm might a select a combination of title,\nsnippet and URL that includes as many of the query terms\nas possible. When a title containing all query terms can be\nfound, the algorithm might select a query-independent \nsnippet. The MatchAll and Readable features directly support\nthis guideline. Once again, other features provide indirect\nsupport, and none of the results are inconsistent with it.\nFinally, the length and complexity of a URL influences\nuser behavior. When query terms appear in the URL they\nshould highlighted or otherwise distinguished. When \nmultiple URLs reference the same page (due to re-directions,\netc.) the shortest URL should be preferred, provided that\nall query terms will still appear in the caption. In other\nwords, URLs should be selected and displayed in a manner\nthat emphasizes their relationship to the query. The three\nURL features, as well as TermMatchTSU, directly support\nthis guideline.\nThe influence of the Official and Image features led us to\nwonder what other terms are prevalent in the captions of\nclickthrough inversions. As an additional experiment, we\ntreated each of the terms appearing in the INV and CON\nsets as a separate feature (case normalized), ranking them by\ntheir \u00cf\u20212\nvalues. The results are presented in figure 6. Since\nwe use the \u00cf\u20212\nstatistic as a divergence measure, rather than\na significance test, no p-values are given. The final column\nof the table indicates the direction of the influence, whether\nthe presence of the terms positively or negatively influence\nclickthroughs.\nThe positive influence of official has already been \nobserved (the difference in the \u00cf\u20212\nvalue from that of figure 5 is\ndue to stemming). None of the terms included in the Image\nRank Term \u00cf\u20212\ninfluence\n1 encyclopedia 114.6891 \u00e2\u2020\u201c\n2 wikipedia 94.0033 \u00e2\u2020\u201c\n3 official 36.5566 \u00e2\u2020\u2018\n4 and 28.3349 \u00e2\u2020\u2018\n5 tourism 25.2003 \u00e2\u2020\u2018\n6 attractions 24.7283 \u00e2\u2020\u2018\n7 free 23.6529 \u00e2\u2020\u201c\n8 sexy 21.9773 \u00e2\u2020\u2018\n9 medlineplus 19.9726 \u00e2\u2020\u201c\n10 information 19.9115 \u00e2\u2020\u2018\nFigure 6: Words exhibiting the greatest positive (\u00e2\u2020\u2018)\nand negative (\u00e2\u2020\u201c) influence on clickthrough patterns.\nfeature appear in the top ten, but pictures and photos\nappear at positions 21 and 22. The high rank given to and\nmay be related to readability (the term the appears in \nposition 20).\nMost surprising to us is the negative influence of the terms:\nencyclopedia, wikipedia, free, and medlineplus. The\nfirst three terms appear in the title of Wikipedia articles3\nand the last appears in the title of MedlinePlus articles4\n.\nThese individual word-level features provide hints about \nissues. More detailed analyses and further experiments will\nbe required to understand these features.\n6. CONCLUSIONS\nClickthrough inversions form an appropriate tool for \nassessing the influence of caption features. Using clickthrough\ninversions, we have demonstrated that relatively simple \ncaption features can significantly influence user behavior. To\nour knowledge, this is first methodology validated for \nassessing the quality of Web captions through implicit \nfeedback. In the future, we hope to substantially expand this\nwork, considering more features over larger datasets. We\nalso hope to directly address the goal of predicting relevance\nfrom clickthoughs and other information present in search\nengine logs.\n7. ACKNOWLEDGMENTS\nThis work was conducted while the first author was \nvisiting Microsoft Research. The authors thank members of\nthe Windows Live team for their comments and assistance,\nparticularly Girish Kumar, Luke DeLorme, Rohit Wad and\nRamez Naam.\n8. REFERENCES\n[1] E. Agichtein, E. Brill, and S. Dumais. Improving web\nsearch ranking by incorporating user behavior\ninformation. In 29th ACM SIGIR, pages 19-26,\nSeattle, August 2006.\n[2] E. Agichtein, E. Brill, S. Dumais, and R. Ragno.\nLearning user interaction models for predicting Web\nsearch result preferences. In 29th ACM SIGIR, pages\n3-10, Seattle, August 2006.\n[3] A. Broder. A taxonomy of Web search. SIGIR Forum,\n36(2):3-10, 2002.\n3\nwww.wikipedia.org\n4\nwww.nlm.nih.gov/medlineplus/\n[4] E. Cutrell and Z. Guan. What are you looking for?\nAn eye-tracking study of information usage in Web\nsearch. In SIGCHI Conference on Human Factors in\nComputing Systems, pages 407-416, San Jose,\nCalifornia, April-May 2007.\n[5] S. Dumais, E. Cutrell, and H. Chen. Optimizing\nsearch by showing results in context. In SIGCHI\nConference on Human Factors in Computing Systems,\npages 277-284, Seattle, March-April 2001.\n[6] J. Goldstein, M. Kantrowitz, V. Mittal, and\nJ. Carbonell. Summarizing text documents: Sentence\nselection and evaluation metrics. In 22nd ACM\nSIGIR, pages 121-128, Berkeley, August 1999.\n[7] L. A. Granka, T. Joachims, and G. Gay. Eye-tracking\nanalysis of user behavior in WWW search. In 27th\nACM SIGIR, pages 478-479, Sheffield, July 2004.\n[8] Y. Hu, G. Xin, R. Song, G. Hu, S. Shi, Y. Cao, and\nH. Li. Title extraction from bodies of HTML\ndocuments and its application to Web page retrieval.\nIn 28th ACM SIGIR, pages 250-257, Salvador, Brazil,\nAugust 2005.\n[9] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and\nG. Gay. Accurately interpreting clickthrough data as\nimplicit feedback. In 28th ACM SIGIR, pages\n154-161, Salvador, Brazil, August 2005.\n[10] U. Lee, Z. Liu, and J. Cho. Automatic identification of\nuser goals in Web search. In 14th International World\nWide Web Conference, pages 391-400, Edinburgh,\nMay 2005.\n[11] H. P. Luhn. The automatic creation of literature\nabstracts. IBM Journal of Research and Development,\n2(2):159-165, April 1958.\n[12] T. Paek, S. Dumais, and R. Logan. WaveLens: A new\nview onto Internet search results. In SIGCHI\nConference on Human Factors in Computing Systems,\npages 727-734, Vienna, Austria, April 2004.\n[13] D. Rose and D. Levinson. Understanding user goals in\nWeb search. In 13th International World Wide Web\nConference, pages 13-19, New York, May 2004.\n[14] J.-T. Sun, D. Shen, H.-J. Zeng, Q. Yang, Y. Lu, and\nZ. Chen. Web-page summarization using clickthrough\ndata. In 28th ACM SIGIR, pages 194-201, Salvador,\nBrazil, August 2005.\n[15] A. Tombros and M. Sanderson. Advantages of query\nbiased summaries in information retrieval. In 21st\nACM SIGIR, pages 2-10, Melbourne, Australia,\nAugust 1998.\n[16] R. Varadarajan and V. Hristidis. A system for\nquery-specific document summarization. In 15th ACM\ninternational conference on Information and\nknowledge management (CIKM), pages 622-631,\nArlington, Virginia, November 2006.\n[17] R. W. White, I. Ruthven, and J. M. Jose. Finding\nrelevant documents using top ranking sentences: An\nevaluation of two alternative schemes. In 25th ACM\nSIGIR, pages 57-64, Tampere, Finland, August 2002.\n[18] G.-R. Xue, H.-J. Zeng, Z. Chen, Y. Yu, W.-Y. Ma,\nW. Xi, and W. Fan. Optimizing web search using Web\nclick-through data. In 13th ACM Conference on\nInformation and Knowledge Management (CIKM),\npages 118-126, Washington, DC, November 2004.\n": ["clickthrough pattern", "caption feature", "web search behavior", "human factor", "extractive summarization", "snippet", "query log", "query re-formulation", "significant word", "clickthrough inversion", "query term match", "web search", "summarization", ""]}