{"A Reinforcement Learning based Distributed Search\nAlgorithm For Hierarchical Peer-to-Peer Information\nRetrieval Systems\nHaizheng Zhang\nCollege of Information Science and Technology\nPennsylvania State University\nUniversity Park, PA 16803\nhzhang@ist.psu.edu\nVictor Lesser\nDepartment of Computer Science\nUniversity Of Massachusetts\nAmherst, MA 01003\nlesser@cs.umass.edu\nABSTRACT\nThe dominant existing routing strategies employed in \npeerto-peer(P2P) based information retrieval(IR) systems are\nsimilarity-based approaches. In these approaches, agents\ndepend on the content similarity between incoming queries\nand their direct neighboring agents to direct the distributed\nsearch sessions. However, such a heuristic is myopic in that\nthe neighboring agents may not be connected to more \nrelevant agents. In this paper, an online reinforcement-learning\nbased approach is developed to take advantage of the \ndynamic run-time characteristics of P2P IR systems as \nrepresented by information about past search sessions. \nSpecifically, agents maintain estimates on the downstream agents\"\nabilities to provide relevant documents for incoming queries.\nThese estimates are updated gradually by learning from the\nfeedback information returned from previous search sessions.\nBased on this information, the agents derive corresponding\nrouting policies. Thereafter, these agents route the queries\nbased on the learned policies and update the estimates based\non the new routing policies. Experimental results \ndemonstrate that the learning algorithm improves considerably the\nrouting performance on two test collection sets that have\nbeen used in a variety of distributed IR studies.\nCategories and Subject Descriptors\nI.2.11 [Distributed Artificial Intelligence]: Multiagent\nSystems\nGeneral Terms\nAlgorithms, Performance, Experimentation\n1. INTRODUCTION\nOver the last few years there have been increasing \ninterests in studying how to control the search processes in\npeer-to-peer(P2P) based information retrieval(IR) systems\n[6, 13, 14, 15]. In this line of research, one of the core \nproblems that concerns researchers is to efficiently route user\nqueries in the network to agents that are in possession of\nappropriate documents. In the absence of global \ninformation, the dominant strategies in addressing this problem are\ncontent-similarity based approaches [6, 13, 14, 15]. While\nthe content similarity between queries and local nodes \nappears to be a creditable indicator for the number of \nrelevant documents residing on each node, these approaches\nare limited by a number of factors. First of all, \nsimilaritybased metrics can be myopic since locally relevant nodes\nmay not be connected to other relevant nodes. Second, the\nsimilarity-based approaches do not take into account the\nrun-time characteristics of the P2P IR systems, including\nenvironmental parameters, bandwidth usage, and the \nhistorical information of the past search sessions, that provide\nvaluable information for the query routing algorithms.\nIn this paper, we develop a reinforcement learning based\nIR approach for improving the performance of distributed\nIR search algorithms. Agents can acquire better search\nstrategies by collecting and analyzing feedback information\nfrom previous search sessions. Particularly, agents \nmaintain estimates, namely expected utility, on the downstream\nagents\" capabilities of providing relevant documents for \nspecific types of incoming queries. These estimates are \nupdated gradually by learning from the feedback information\nreturned from previous search sessions. Based on the \nupdated expected utility information, the agents derive \ncorresponding routing policies. Thereafter, these agents route\nthe queries based on the learned policies and update the\nestimates on the expected utility based on the new routing\npolicies. This process is conducted in an iterative manner.\nThe goal of the learning algorithm, even though it consumes\nsome network bandwidth, is to shorten the routing time so\nthat more queries are processed per time unit while at the\nsame time finding more relevant documents. This contrasts\nwith the content-similarity based approaches where similar\noperations are repeated for every incoming query and the\nprocessing time keeps largely constant over time.\nAnother way of viewing this paper is that our basic \napproach to distributed IR search is to construct a hierarchical\noverlay network(agent organization) based on the \ncontentsimilarity measure among agents\" document collections in a\nbottom-up fashion. In the past work, we have shown that\nthis organization improves search performance significantly.\nHowever, this organizational structure does not take into\naccount the arrival patterns of queries, including their \nfrequency, types, and where they enter the system, nor the\navailable communication bandwidth of the network and \nprocessing capabilities of individual agents. The intention of\nthe reinforcement learning is to adapt the agents\" routing\ndecisions to the dynamic network situations and learn from\npast search sessions. Specifically, the contributions of this\npaper include: (1) a reinforcement learning based approach\nfor agents to acquire satisfactory routing policies based on\nestimates of the potential contribution of their neighboring\nagents; (2) two strategies to speed up the learning process.\nTo our best knowledge, this is one of the first reinforcement\nlearning applications in addressing distributed content \nsharing problems and it is indicative of some of the issues in\napplying reinforcement in a complex application.\nThe remainder of this paper is organized as follows: \nSection 2 reviews the hierarchical content sharing systems and\nthe two-phase search algorithm based on such topology. \nSection 3 describes a reinforcement learning based approach to\ndirect the routing process; Section 4 details the experimental\nsettings and analyze the results. Section 5 discusses related\nstudies and Section 6 concludes the paper.\n2. SEARCH IN HIERARCHICAL\nP2P IR SYSTEMS\nThis section briefly reviews our basic approaches to \nhierarchical P2P IR systems. In a hierarchical P2P IR \nsystem illustrated in Fig.1, agents are connected to each other\nthrough three types of links: upward links, downward links,\nand lateral links. In the following sections, we denote the\nset of agents that are directly connected to agent Ai as\nDirectConn(Ai), which is defined as\nDirectConn(Ai) = NEI(Ai) \u00e2\u02c6\u00aa PAR(Ai) \u00e2\u02c6\u00aa CHL(Ai)\n, where NEI(Ai) is the set of neighboring agents connected\nto Ai through lateral links; PAR(Ai) is the set of agents\nwhom agent Ai is connected to through upward links and\nCHL(Ai) is the set of agents that agent Ai connects to\nthrough downward links. These links are established through\na bottom-up content-similarity based distributed clustering\nprocess[15]. These links are then used by agents to locate\nother agents that contain documents relevant to the given\nqueries.\nA typical agent Ai in our system uses two queues: a local\nsearch queue, LSi, and a message forwarding queue MFi.\nThe states of the two queues constitute the internal states of\nan agent. The local search queue LSi stores search sessions\nthat are scheduled for local processing. It is a priority queue\nand agent Ai always selects the most promising queries to\nprocess in order to maximize the global utility. MFi \nconsists of a set of queries to forward on and is processed in\na FIFO (first in first out) fashion. For the first query in\nMFi, agent Ai determines which subset of its neighboring\nagents to forward it to based on the agent\"s routing policy\n\u00cf\u20aci. These routing decisions determine how the search \nprocess is conducted in the network. In this paper, we call Ai\nas Aj\"s upstream agent and Aj as Ai\"s downstream agent if\nA4 A5 A6 A7\nA2\nA3\nA9\nNEI(A2)={A3}\nPAR(A2)={A1}\nCHL(A2)={A4,A5}\nA1\nA8\nFigure 1: A fraction of a hierarchical P2PIR system\nan agent Ai routes a query to agent Aj.\nThe distributed search protocol of our hierarchical agent\norganization is composed of two steps. In the first step, upon\nreceipt of a query qk at time tl from a user, agent Ai \ninitiates a search session si by probing its neighboring agents\nAj \u00e2\u02c6\u02c6 NEI(Ai) with the message PROBE for the similarity\nvalue Sim(qk, Aj) between qk and Aj. Here, Ai is defined as\nthe query initiator of search session si. In the second step,\nAi selects a group of the most promising agents to start\nthe actual search process with the message SEARCH. These\nSEARCH messages contain a TTL (Time To Live) \nparameter in addition to the query. The TTL value decreases by 1\nafter each hop. In the search process, agents discard those\nqueries that either have been previously processed or whose\nTTL drops to 0, which prevents queries from looping in the\nsystem forever. The search session ends when all the agents\nthat receive the query drop it or TTL decreases to 0. Upon\nreceipt of SEARCH messages for qk, agents schedule local\nactivities including local searching, forwarding qk to their\nneighbors, and returning search results to the query \ninitiator. This process and related algorithms are detailed in [15,\n14].\n3. A BASIC REINFORCEMENTLEARNING\nBASED SEARCH APPROACH\nIn the aforementioned distributed search algorithm, the\nrouting decisions of an agent Ai rely on the similarity \ncomparison between incoming queries and Ai\"s neighboring agents\nin order to forward those queries to relevant agents \nwithout flooding the network with unnecessary query messages.\nHowever, this heuristic is myopic because a relevant \ndirect neighbor is not necessarily connected to other relevant\nagents. In this section, we propose a more general approach\nby framing this problem as a reinforcement learning task.\nIn pursuit of greater flexibility, agents can switch between\ntwo modes: learning mode and non-learning mode. In the\nnon-learning mode, agents operate in the same way as they\ndo in the normal distributed search processes described in\n[14, 15]. On the other hand, in the learning mode, in \nparallel with distributed search sessions, agents also participate\nin a learning process which will be detailed in this section.\nNote that in the learning protocol, the learning process does\nnot interfere with the distributed search process. Agents can\nchoose to initiate and stop learning processes without \naffecting the system performance. In particular, since the learning\nprocess consumes network resources (especially bandwidth),\nagents can choose to initiate learning only when the network\nload is relatively low, thus minimizing the extra \ncommunication costs incurred by the learning algorithm.\nThe section is structured as follows, Section 3.1 describes\n232 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\na reinforcement learning based model. Section 3.2 describes\na protocol to deploy the learning algorithm in the network.\nSection 3.3 discusses the convergence of the learning \nalgorithm.\n3.1 The Model\nAn agent\"s routing policy takes the state of a search \nsession as input and output the routing actions for that query.\nIn our work, the state of a search session sj is stipulated as:\nQSj = (qk, ttlj)\nwhere ttlj is the number of hops that remains for the\nsearch session sj , qk is the specific query. QL is an attribute\nof qk that indicates which type of queries qk most likely\nbelong to. The set of QL can be generated by running a\nsimple online classification algorithm on all the queries that\nhave been processed by the agents, or an o\u00ef\u00ac\u201eine algorithm on\na pre-designated training set. The assumption here is that\nthe set of query types is learned ahead of time and belongs to\nthe common knowledge of the agents in the network. Future\nwork includes exploring how learning can be accomplished\nwhen this assumption does not hold. Given the query types\nset, an incoming query qi can be classified to one query class\nQ(qi) by the formula:\nQ(qi) = arg max\nQj\nP(qi|Qj) (1)\nwhere P(qi|Qj ) indicates the likelihood that the query qi is\ngenerated by the query class Qj [8].\nThe set of atomic routing actions of an agent Ai is denoted\nas {\u00ce\u00b1i}, where {\u00ce\u00b1i} is defined as \u00ce\u00b1i = {\u00ce\u00b1i0 , \u00ce\u00b1i1 , ..., \u00ce\u00b1in }. An\nelement \u00ce\u00b1ij represents an action to route a given query to\nthe neighboring agent Aij \u00e2\u02c6\u02c6 DirectConn(Ai). The routing\npolicy \u00cf\u20aci of agent Ai is stochastic and its outcome for a\nsearch session with state QSj is defined as:\n\u00cf\u20aci(QSj) = {(\u00ce\u00b1i0 , \u00cf\u20aci(QSi, \u00ce\u00b1i0 )), (\u00ce\u00b1i1 , \u00cf\u20aci(QSi, \u00ce\u00b1i1 )), ...} (2)\nNote that operator \u00cf\u20aci is overloaded to represent either the\nprobabilistic policy for a search session with state QSj, \ndenoted as \u00cf\u20aci(QSj); or the probability of forwarding the query\nto a specific neighboring agent Aik \u00e2\u02c6\u02c6 DirectConn(Ai) \nunder the policy \u00cf\u20aci(QSj), denoted as \u00cf\u20aci(QSj, \u00ce\u00b1ik ). \nTherefore, equation (2) means that the probability of \nforwarding the search session to agent Ai0 is \u00cf\u20aci(QSi, \u00ce\u00b1i0 ) and so\non. Under this stochastic policy, the routing action is \nnondeterministic. The advantage of such a strategy is that\nthe best neighboring agents will not be selected repeatedly,\nthereby mitigating the potential hot spots situations.\nThe expected utility, Un\ni (QSj), is used to estimate the \npotential utility gain of routing query type QSj to agent Ai\nunder policy \u00cf\u20acn\ni . The superscript n indicates the value at the\nnth iteration in an iterative learning process. The expected\nutility provides routing guidance for future search sessions.\nIn the search process, each agent Ai maintains partial \nobservations of its neighbors\" states, as shown in Fig. 2. The\npartial observation includes non-local information such as\nthe potential utility estimation of its neighbor Am for query\nstate QSj, denoted as Um(QSj), as well as the load \ninformation, Lm. These observations are updated periodically\nby the neighbors. The estimated utility information will be\nused to update Ai\"s expected utility for its routing policy.\nLoad Information\nExpected Utility For Different Query Types\nNeighboring Agents\n...\nA0\nA1\nA3\nA2\nUn\n0 (QS0) ...\n...\n...\n...\n......\nUn\n0 (QS1)\nUn\n1 (QS1)\nUn\n2 (QS1)\nUn\n3 (QS1)\nUn\n1 (QS0)\nUn\n2 (QS0)\nUn\n3 (QS0)\nLn\n0\nLn\n1\nLn\n2\nLn\n3\n...\nQS0 QS1 ...\nFigure 2: Agent Ai\"s Partial Observation about its\nneighbors(A0, A1...)\nThe load information of Am, Lm, is defined as\nLm =\n|MFm|\nCm\n, where |MFm| is the length of the message-forward queue\nand Cm is the service rate of agent Am\"s message-forward\nqueue. Therefore Lm characterizes the utilization of an\nagent\"s communication channel, and thus provide non-local\ninformation for Am\"s neighbors to adjust the parameters of\ntheir routing policy to avoid inundating their downstream\nagents. Note that based on the characteristics of the queries\nentering the system and agents\" capabilities, the loading of\nagents may not be uniform. After collecting the utilization\nrate information from all its neighbors, agent Ai computes\nLi as a single measure for assessing the average load \ncondition of its neighborhood:\nLi =\nP\nk Lk\n|DirectConn(Ai)|\nAgents exploit Li value in determining the routing \nprobability in its routing policy.\nNote that, as described in Section 3.2, information about\nneighboring agents is piggybacked with the query message\npropagated among the agents whenever possible to reduce\nthe traffic overhead.\n3.1.1 Update the Policy\nAn iterative update process is introduced for agents to\nlearn a satisfactory stochastic routing policy. In this \niterative process, agents update their estimates on the potential\nutility of their current routing policies and then propagate\nthe updated estimates to their neighbors. Their neighbors\nthen generate a new routing policy based on the updated\nobservation and in turn they calculate the expected utility\nbased on the new policies and continue this iterative process.\nIn particular, at time n, given a set of expected \nutility, an agent Ai, whose directly connected agents set is\nDirectConn(Ai) = {Ai0 , ..., Aim }, determines its \ncorresponding stochastic routing policy for a search session of state QSj\nbased on the following steps:\n(1) Ai first selects a subset of agents as the potential\ndownstream agents from set DirectConn(Ai), denoted as\nPDn(Ai, QSj). The size of the potential downstream agent\nis specified as\n|PDn(Ai, QSj)| = min(|NEI(Ai), dn\ni + k)|\nwhere k is a constant and is set to 3 in this paper; dn\ni ,\nthe forward width, is defined as the expected number of\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233\nneighboring agents that agent Ai can forward to at time\nn. This formula specifies that the potential downstream\nagent set PDn(Ai, QSj) is either the subset of neighboring\nagents with dn\ni + k highest expected utility value for state\nQSj among all the agents in DirectConn(Ai), or all their\nneighboring agents. The k is introduced based on the idea\nof a stochastic routing policy and it makes the forwarding\nprobability of the dn\ni +k highest agent less than 100%. Note\nthat if we want to limit the number of downstream agents\nfor search session sj as 5, the probability of forwarding the\nquery to all neighboring agents should add up to 5. \nSetting up dn\ni value properly can improve the utilization rate\nof the network bandwidth when much of the network is idle\nwhile mitigating the traffic load when the network is highly\nloaded. The dn+1\ni value is updated based on dn\ni , the \nprevious and current observations on the traffic situation in the\nneighborhood. Specifically, the update formula for dn+1\ni is\ndn+1\ni = dn\ni \u00e2\u02c6\u2014 (1 +\n1 \u00e2\u02c6\u2019 Li\n|DirectConn(Ai)|\n)\nIn this formula, the forward width is updated based on the\ntraffic conditions of agent Ai\"s neighborhood, i.e Li, and its\nprevious value.\n(2) For each agent Aik in the PDn(Ai, QSj), the \nprobability of forwarding the query to Aik is determined in the\nfollowing way in order to assign higher forwarding \nprobability to the neighboring agents with higher expected utility\nvalue:\n\u00cf\u20acn+1\ni (QSj, \u00ce\u00b1ik ) =\ndn+1\ni\n|PDn(Ai, QSj)|\n+\n\u00ce\u00b2 \u00e2\u02c6\u2014\n`\nUik (QSj) \u00e2\u02c6\u2019\nPDU(Ai, QSj)\n|PDn(Ai, QSj)|\n\u00c2\u00b4\n(3)\nwhere\nPDUn(Ai, QSj) =\nX\no\u00e2\u02c6\u02c6P Dn(Ai,QSj )\nUo(QSj)\nand QSj is the subsequent state of agent Aik after agent\nAi forwards the search session with state QSj to its \nneighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 \u00e2\u02c6\u2019\n1).\nIn formula 3, the first term on the right of the equation,\ndn+1\ni\n|P Dn(Ai,QSj )|\n, is used to to determine the forwarding \nprobability by equally distributing the forward width, dn+1\ni , to\nthe agents in PDn(Ai, QSj) set. The second term is used to\nadjust the probability of being chosen so that agents with\nhigher expected utility values will be favored. \u00ce\u00b2 is \ndetermined according to:\n\u00ce\u00b2 = min\n` m \u00e2\u02c6\u2019 dn+1\ni\nm \u00e2\u02c6\u2014 umax \u00e2\u02c6\u2019 PDUn(Ai, QSj)\n,\ndn+1\ni\nPDUn(Ai, QSj) \u00e2\u02c6\u2019 m \u00e2\u02c6\u2014 umin\n\u00c2\u00b4\n(4)\nwhere m = |PDn(Ai, QSj)|,\numax = max\no\u00e2\u02c6\u02c6P Dn(Ao,QSj )\nUo(QSj)\nand\numin = min\no\u00e2\u02c6\u02c6P Dn(Ao,QSj )\nUo(QSj)\nThis formula guarantees that the final \u00cf\u20acn+1\ni (QSj, \u00ce\u00b1ik ) value\nis well defined, i.e,\n0 \u00e2\u2030\u00a4 \u00cf\u20acn+1\ni (QSj, \u00ce\u00b1ik ) \u00e2\u2030\u00a4 1\nand\nX\ni\n\u00cf\u20acn+1\ni (QSj, \u00ce\u00b1ik ) = dn+1\ni\nHowever, such a solution does not explore all the \npossibilities. In order to balance between exploitation and \nexploration, a \u00ce\u00bb-Greedy approach is taken. In the \u00ce\u00bb-Greedy \napproach, in addition to assigning higher probability to those\nagents with higher expected utility value, as in the equation\n(3). Agents that appear to be not-so-good choices will\nalso be sent queries based on a dynamic exploration rate.\nIn particular, for agents in the set PDn(Ai, QSj), \u00cf\u20acn+1\ni1\n(QSj)\nis determined in the same way as the above, with the only\ndifference being that dn+1\ni is replaced with dn+1\ni \u00e2\u02c6\u2014 (1 \u00e2\u02c6\u2019 \u00ce\u00bbn).\nThe remaining search bandwidth is used for learning by\nassigning probability \u00ce\u00bbn evenly to agents Ai2 in the set\nDirectConn(Ai) \u00e2\u02c6\u2019 PDn(Ai, QSj).\n\u00cf\u20acn+1\ni2\n(QSj, \u00ce\u00b1ik ) =\ndn+1\ni \u00e2\u02c6\u2014 \u00ce\u00bbn\n|DirectConn(Ai) \u00e2\u02c6\u2019 PDn(Ai, QSj)|\n(5)\nwhere PDn(Ai, QSj) \u00e2\u0160\u201a DirectConn(Ai). Note that the\nexploration rate \u00ce\u00bb is not a constant and it decreases \novertime. The \u00ce\u00bb is determined according to the following \nequation:\n\u00ce\u00bbn+1 = \u00ce\u00bb0 \u00e2\u02c6\u2014 e\u00e2\u02c6\u2019c1n\n(6)\nwhere \u00ce\u00bb0 is the initial exploration rate, which is a \nconstant; c1 is also a constant to adjust the decreasing rate of\nthe exploration rate; n is the current time unit.\n3.1.2 Update Expected Utility\nOnce the routing policy at step n+1, \u00cf\u20acn+1\ni , is determined\nbased on the above formula, agent Ai can update its own \nexpected utility, Un+1\ni (QSi), based on the the updated routing\npolicy resulted from the formula 5 and the updated U values\nof its neighboring agents. Under the assumption that after a\nquery is forwarded to Ai\"s neighbors the subsequent search\nsessions are independent, the update formula is similar to\nthe Bellman update formula in Q-Learning:\nUn+1\ni (QSj) = (1 \u00e2\u02c6\u2019 \u00ce\u00b8i) \u00e2\u02c6\u2014 Un\ni (QSj) +\n\u00ce\u00b8i \u00e2\u02c6\u2014 (Rn+1\ni (QSj) +\nX\nk\n\u00cf\u20acn+1\ni (QSj, \u00ce\u00b1ik )Un\nk (QSj)) (7)\nwhere QSj = (Qj, ttl \u00e2\u02c6\u2019 1) is the next state of QSj =\n(Qj, ttl); Rn+1\ni (QSj) is the expected local reward for query\nclass Qk at agent Ai under the routing policy \u00cf\u20acn+1\ni ; \u00ce\u00b8i is the\ncoefficient for deciding how much weight is given to the old\nvalue during the update process: the smaller \u00ce\u00b8i value is, the\nfaster the agent is expected to learn the real value, while the\ngreater volatility of the algorithm, and vice versa. Rn+1\n(s)\nis updated according to the following equation:\n234 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\nRn+1\ni (QSj) = Rn\ni (QSj)\n+\u00ce\u00b3i \u00e2\u02c6\u2014 (r(QSj) \u00e2\u02c6\u2019 Rn\ni (QSj)) \u00e2\u02c6\u2014 P(qj|Qj ) (8)\nwhere r(QSj) is the local reward associated with the search\nsession. P(qj|Qj ) indicates how relevant the query qj is to\nthe query type Qj, and \u00ce\u00b3i is the learning rate for agent Ai.\nDepending on the similarity between a specific query qi and\nits corresponding query type Qi, the local reward associated\nwith the search session has different impact on the Rn\ni (QSj)\nestimation. In the above formula, this impact is reflected by\nthe coefficient, the P(qj|Qj) value.\n3.1.3 Reward function\nAfter a search session stops when its TTL values expires,\nall search results are returned back to the user and are \ncompared against the relevance judgment. Assuming the set of\nsearch results is SR, the reward Rew(SR) is defined as:\nRew(SR) =\nj\n1 if |Rel(SR)| > c\n|Rel(SR)|\nc\notherwise.\nwhere SR is the set of returned search results, Rel(SR)\nis the set of relevant documents in the search results. This\nequation specifies that users give 1.0 reward if the number\nof returned relevant documents reaches a predefined number\nc. Otherwise, the reward is in proportion to the number of\nrelevant documents returned. This rationale for setting up\nsuch a cut-off value is that the importance of recall ratio\ndecreases with the abundance of relevant documents in real\nworld, therefore users tend to focus on only a limited number\nof searched results.\nThe details of the actual routing protocol will be \nintroduced in Section 3.2 when we introduce how the learning\nalgorithm is deployed in real systems.\n3.2 Deployment of the Learning algorithm\nThis section describes how the learning algorithm can be\nused in either a single-phase or a two-phase search process.\nIn the single-phase search algorithm, search sessions start\nfrom the initiators of the queries. In contrast, in the two-step\nsearch algorithm, the query initiator first attempts to seek a\nmore appropriate starting point for the query by introducing\nan exploratory step as described in Section 2. Despite the\ndifference in the quality of starting points, the major part\nof the learning process for the two algorithms is largely the\nsame as described in the following paragraphs.\nBefore learning starts, each agent initializes the expected\nutility value for all possible states as 0. Thereafter, upon\nreceipt of a query, in addition to the normal operations \ndescribed in the previous section, an agent Ai also sets up a\ntimer to wait for the search results returned from its \ndownstream agents. Once the timer expires or it has received\nresponse from all its downstream agents, Ai merges and \nforwards the search results accrued from its downstream agents\nto its upstream agent. Setting up the timer speeds up the\nlearning because agents can avoid waiting too long for the\ndownstream agents to return search results. Note that these\ndetailed results and corresponding agent information will\nstill be stored at Ai until the feedback information is passed\nfrom its upstream agent and the performance of its \ndownstream agents can be evaluated. The duration of the timer\nis related to the TTL value. In this paper, we set the timer\nto\nttimer = ttli \u00e2\u02c6\u2014 2 + tf\n, where ttli \u00e2\u02c6\u2014 2 is the sum of the travel time of the queries in\nthe network, and tf is the expected time period that users\nwould like to wait.\nThe search results will eventually be returned to the search\nsession initiator A0. They will be compared to the relevance\njudgment that is provided by the final users (as described\nin the experiment section, the relevance judgement for the\nquery set is provided along with the data collections). The\nreward will be calculated and propagated backward to the\nagents along the way that search results were passed. This\nis a reverse process of the search results propagation. In the\nprocess of propagating reward backward, agents update \nestimates of their own potential utility value, generate an \nupto-dated policy and pass their updated results to the \nneighboring agents based on the algorithm described in Section 3.\nUpon change of expected utility value, agent Ai sends out its\nupdated utility estimation to its neighbors so that they can\nact upon the changed expected utility and corresponding\nstate. This update message includes the potential reward\nas well as the corresponding state QSi = (qk, ttll) of agent\nAi. Each neighboring agent, Aj, reacts to this kind of \nupdate message by updating the expected utility value for state\nQSj(qk, ttll + 1) according to the newly-announced changed\nexpected utility value. Once they complete the update, the\nagents would again in turn inform related neighbors to \nupdate their values. This process goes on until the TTL value\nin the update message increases to the TTL limit.\nTo speed up the learning process, while updating the \nexpected utility values of an agent Ai\"s neighboring agents we\nspecify that\nUm(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1\nThus, when agent Ai receives an updated expected utility\nvalue with ttl1, it also updates the expected utility values\nwith any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed\nup convergence. This heuristic is based on the fact that\nthe utility of a search session is a non-decreasing function of\ntime t.\n3.3 Discussion\nIn formalizing the content routing system as a learning\ntask, many assumptions are made. In real systems, these\nassumptions may not hold, and thus the learning algorithm\nmay not converge. Two problems are of particular note,\n(1) This content routing problem does not have Markov\nproperties. In contrast to IP-level based packet routing, the\nrouting decision of each agent for a particular search \nsession sj depends on the routing history of sj. Therefore,\nthe assumption that all subsequent search sessions are \nindependent does not hold in reality. This may lead to \ndouble counting problem that the relevant documents of some\nagents will be counted more than once for the state where\nthe TTL value is more than 1. However, in the context\nof the hierarchical agent organizations, two factors mitigate\nthis problems: first, the agents in each content group form\na tree-like structure. With the absense of the cycles, the \nestimates inside the tree would be close to the accurate value.\nSecondly, the stochastic nature of the routing policy partly\nremedies this problem.\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235\n(2) Another challenge for this learning algorithm is that\nin a real network environment observations on neighboring\nagents may not be able to be updated in time due to the\ncommunication delay or other situations. In addition, when\nneighboring agents update their estimates at the same time,\noscillation may arise during the learning process[1].\nThis paper explores several approaches to speed up the\nlearning process. Besides the aforementioned strategy of\nupdating the expected utility values, we also employ an\nactive update strategy where agents notify their \nneighbors whenever its expected utility is updated. Thus a faster\nconvergence speed can be achieved. This strategy contrasts\nto the Lazy update, where agents only echo their \nneighboring agents with their expected utility change when they\nexchange information. The trade off between the two \napproaches is the network load versus learning speed.\nThe advantage of this learning algorithm is that once a\nrouting policy is learned, agents do not have to repeatedly\ncompare the similarity of queries as long as the network\ntopology remains unchanged. Instead, agent just have to\ndetermine the classification of the query properly and follow\nthe learned policies. The disadvantage of this learning-based\napproach is that the learning process needs to be conducted\nwhenever the network structure changes. There are many\npotential extensions for this learning model. For example, a\nsingle measure is currently used to indicate the traffic load\nfor an agent\"s neighborhood. A simple extension would be to\nkeep track of individual load for each neighbor of the agent.\n4. EXPERIMENTSSETTINGSAND RESULTS\nThe experiments are conducted on TRANO simulation\ntoolkit with two sets of datasets, TREC-VLC-921 and \nTREC123-100. The following sub-sections introduce the TRANO\ntestbed, the datasets, and the experimental results.\n4.1 TRANO Testbed\nTRANO (Task Routing on Agent Network Organization)\nis a multi-agent based network based information retrieval\ntestbed. TRANO is built on top of the Farm [4], a time\nbased distributed simulator that provides a data \ndissemination framework for large scale distributed agent network\nbased organizations. TRANO supports importation and \nexportation of agent organization profiles including topological\nconnections and other features. Each TRANO agent is \ncomposed of an agent view structure and a control unit. In \nsimulation, each agent is pulsed regularly and the agent checks\nthe incoming message queues, performs local operations and\nthen forwards messages to other agents .\n4.2 Experimental Settings\nIn our experiment, we use two standard datasets, \nTRECVLC-921 and TREC-123-100 datasets, to simulate the \ncollections hosted on agents. The TREC-VLC-921 and \nTREC123-100 datasets were created by the U.S. National Institute\nfor Standard Technology(NIST) for its TREC conferences.\nIn distributed information retrieval domain, the two data\ncollections are split to 921 and 100 sub-collections. It is \nobserved that dataset TREC-VLC-921 is more heterogeneous\nthan TREC-123-100 in terms of source, document length,\nand relevant document distribution from the statistics of the\ntwo data collections listed in [13]. Hence, TREC-VLC-921 is\nmuch closer to real document distributions in P2P \nenvironments. Furthermore, TREC-123-100 is split into two sets of\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0 500 1000 1500 2000 2500 3000\nARSS\nQuery number\nARSS versus the number of incoming queries for TREC-VLC-921\nSSLA-921\nSSNA-921\nFigure 3: ARSS(Average reward per search \nsession) versus the number of search sessions for 1phase\nsearch in TREC-VLC-921\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0 500 1000 1500 2000 2500 3000\nARSS\nQuery number\nARSS versus query number for TREC-VLC-921\nTSLA-921\nTSNA-921\nFigure 4: ARSS(Average reward per search \nsession) versus the number of search sessions for 2phase\nsearch in TREC-VLC-921\nsub-collections in two ways: randomly and by source. The\ntwo partitions are denoted as TREC-123-100-Random and\nTREC-123-100-Source respectively. The documents in each\nsubcollection in dataset TREC-123-100-Source are more \ncoherent than those in TREC-123-100-Random. The two \ndifferent sets of partitions allow us to observe how the \ndistributed learning algorithm is affected by the homogeneity\nof the collections.\nThe hierarchical agent organization is generated by the\nalgorithm described in our previous algorithm [15]. During\nthe topology generation process, degree information of each\nagent is estimated by the algorithm introduced by Palmer\net al. [9] with parameters \u00ce\u00b1 = 0.5 and \u00ce\u00b2 = 0.6. In our\nexperiments, we estimate the upward limit and downward\ndegree limit using linear discount factors 0.5, 0.8 and 1.0.\nOnce the topology is built, queries randomly selected from\nthe query set 301\u00e2\u02c6\u2019350 on TREC-VLC-921 and query set 1\u00e2\u02c6\u2019\n50 on TREC-123-100-Random and TREC-123-100-Source\nare injected to the system based on a Poisson distribution\nP(N(t) = n) =\n(\u00ce\u00bbt)n\nn!\ne\u00e2\u02c6\u2019\u00ce\u00bb\n236 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\n0\n50\n100\n150\n200\n250\n300\n350\n400\n0 500 1000 1500 2000 2500 3000\nCumulativeutility\nQuery number\nCumulative utility over the number of incoming queries\nTSLA-921\nSSNA-921\nSSLA-921\nTSNA-921\nFigure 5: The cumulative utility versus the number\nof search sessions TREC-VLC-921\nIn addition, we assume that all agents have an equal chance\nof getting queries from the environment, i.e, \u00ce\u00bb is the same\nfor every agent. In our experiments, \u00ce\u00bb is set as 0.0543 so\nthat the mean of the incoming queries from the environment\nto the agent network is 50 per time unit. The service time\nfor the communication queue and local search queue, i.e tQij\nand trs, is set as 0.01 time unit and 0.05 time units \nrespectively. In our experiments, there are ten types of queries\nacquired by clustering the query set 301 \u00e2\u02c6\u2019 350 and 1 \u00e2\u02c6\u2019 50.\n4.3 Results analysis and evaluation\nFigure 3 demonstrates the ARSS(Average Reward per\nSearch Session) versus the number of incoming queries over\ntime for the the Single-Step based Non-learning Algorithm\n(SSNA), and the Single-Step Learning Algorithm(SSLA) for\ndata collection TREC-VLC-921. It shows that the average\nreward for SSNA algorithm ranges from 0.02 \u00e2\u02c6\u2019 0.06 and the\nperformance changes little over time. The average reward\nfor SSLA approach starts at the same level with the SSNA\nalgorithm. But the performance increases over time and\nthe average performance gain stabilizes at about 25% after\nquery range 2000 \u00e2\u02c6\u2019 3000.\nFigure 4 shows the ARSS(Average Reward per Search \nSession) versus the number of incoming queries over time for the\nthe Two-Step based Non-learning Algorithm(TSNA), and\nthe Two-Step Learning Algorithm(TSLA) for data \ncollection TREC-VLC-921. The TSNA approach has a relatively\nconsistent performance with the average reward ranges from\n0.05 \u00e2\u02c6\u2019 0.15. The average reward for TSLA approach, where\nlearning algorithm is exploited, starts at the same level with\nthe TSNA algorithm and improves the average reward over\ntime until 2000\u00e2\u02c6\u20192500 queries joining the system. The results\nshow that the average performance gain for TSLA approach\nover TNLA approach is 35% after stabilization.\nFigure 5 shows the cumulative utility versus the number\nof incoming queries over time for SSNA, SSLA,TSNA, and\nTSLA respectively. It illustrates that the cumulative \nutility of non-learning algorithms increases largely linearly over\ntime, while the gains of learning-based algorithms \naccelerate when more queries enter the system. These experimental\nresults demonstrate that learning-based approaches \nconsistently perform better than non-learning based routing \nalgorithm. Moreover, two-phase learning based algorithm is\nbetter than single-phase based learning algorithm because\nthe maximal reward an agent can receive from searching its\nneighborhood within TTL hops is related to the total \nnumber of the relevant documents in that area. Thus, even the\noptimal routing policy can do little beyond reaching these\nrelevant documents faster. On the contrary, the \ntwo-stepbased learning algorithm can relocate the search session to\na neighborhood with more relevant documents. The TSLA\ncombines the merits of both approaches and outperforms\nthem.\nTable 1 lists the cumulative utility for datasets \nTREC123-100-Random and TREC-123-100-Source with \nhierarchical organizations. The five columns show the results for four\ndifferent approaches. In particular, column TSNA-Random\nshows the results for dataset TREC-123-100-Random with\nthe TSNA approach. The column TSLA-Random shows the\nresults for dataset TREC-123-100-Random with the TSLA\napproach. There are two numbers in each cell in the \ncolumn TSLA-Random. The first number is the actual \ncumulative utility while the second number is the percentage\ngain in terms of the utility over TSNA approach. Columns\nTSNA-Source and TSLA-Source show the results for dataset\nTREC-123-100-Source with TSNA and TSLA approaches\nrespectively. Table 1 shows that the performance \nimprovement for TREC-123-100-Random is not as significant as the\nother datasets. This is because that the documents in the\nsub-collection of TREC-123-100-Random are selected \nrandomly which makes the collection model, the signature of\nthe collection, less meaningful. Since both algorithms are\ndesigned based on the assumption that document collections\ncan be well represented by their collection model, this result\nis not surprising.\nOverall, Figures 4, 5, and Table 1 demonstrate that the\nreinforcement learning based approach can considerably \nenhance the system performance for both data collections.\nHowever, it remains as future work to discover the \ncorrelation between the magnitude of the performance gains and\nthe size of the data collection and/or the extent of the \nheterogeneity between the sub-collections.\n5. RELATED WORK\nThe content routing problem differs from the \nnetworklevel routing in packet-switched communication networks in\nthat content-based routing occurs in application-level \nnetworks. In addition, the destination agents in our \ncontentrouting algorithms are multiple and the addresses are not\nknown in the routing process. IP-level Routing problems\nhave been attacked from the reinforcement learning \nperspective[2, 5, 11, 12]. These studies have explored fully\ndistributed algorithms that are able, without central \ncoordination to disseminate knowledge about the network, to\nfind the shortest paths robustly and efficiently in the face of\nchanging network topologies and changing link costs. There\nare two major classes of adaptive, distributed packet \nrouting algorithms in the literature: distance-vector algorithms\nand link-state algorithms. While this line of studies carry a\ncertain similarity with our work, it has mainly focused on\npacket-switched communication networks. In this domain,\nthe destination of a packet is deterministic and unique. Each\nagent maintains estimations, probabilistically or \ndeterministically, on the distance to a certain destination through its\nneighbors. A variant of Q-Learning techniques is deployed\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237\nTable 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical\nOrganization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate\nthe performance gain over the algorithm without learning\nQuery number TSNA-Random TSLA-Random TSNA-Source TSLA-Source\n500 25.15 28.45 13% 24.00 21.05 -13%\n1000 104.99 126.74 20% 93.95 96.44 2.6%\n1250 149.79 168.40 12% 122.64 134.05 9.3%\n1500 188.94 211.05 12% 155.30 189.60 22%\n1750 235.49 261.60 11% 189.14 243.90 28%\n2000 275.09 319.25 16% 219.0 278.80 26%\nto update the estimations to converge to the real distances.\nIt has been discovered that the locality property is an \nimportant feature of information retrieval systems in user \nmodeling studies[3]. In P2P based content sharing systems, this\nproperty is exemplified by the phenomenon that users tend\nto send queries that represent only a limited number of \ntopics and conversely, users in the same neighborhood are likely\nto share common interests and send similar queries [10]. The\nlearning based approach is perceived to be more beneficial\nfor real distributed information retrieval systems which \nexhibit locality property. This is because the users\" traffic and\nquery patterns can reduce the state space and speed up the\nlearning process. Related work in taking advantage of this\nproperty include [7], where the authors attempted to address\nthis problem by user modeling techniques.\n6. CONCLUSIONS\nIn this paper, a reinforcement-learning based approach\nis developed to improve the performance of distributed IR\nsearch algorithms. Particularly, agents maintain estimates,\nnamely expected utility, on the downstream agents\" ability to\nprovide relevant documents for incoming queries. These \nestimates are updated gradually by learning from the feedback\ninformation returned from previous search sessions. Based\non the updated expected utility information, the agents \nmodify their routing policies. Thereafter, these agents route the\nqueries based on the learned policies and update the \nestimates on the expected utility based on the new routing\npolicies. The experiments on two different distributed IR\ndatasets illustrates that the reinforcement learning approach\nimproves considerably the cumulative utility over time.\n7. REFERENCES\n[1] S. Abdallah and V. Lesser. Learning the task\nallocation game. In AAMAS \"06: Proceedings of the\nfifth international joint conference on Autonomous\nagents and multiagent systems, pages 850-857, New\nYork, NY, USA, 2006. ACM Press.\n[2] J. A. Boyan and M. L. Littman. Packet routing in\ndynamically changing networks: A reinforcement\nlearning approach. In Advances in Neural Information\nProcessing Systems, volume 6, pages 671-678. Morgan\nKaufmann Publishers, Inc., 1994.\n[3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles,\nT. Emmitt, K. J. Prey, and Y. Mou. Comparing the\nperformance of database selection algorithms. In\nResearch and Development in Information Retrieval,\npages 238-245, 1999.\n[4] B. Horling, R. Mailler, and V. Lesser. Farm: A\nscalable environment for multi-agent development and\nevaluation. In Advances in Software Engineering for\nMulti-Agent Systems, pages 220-237, Berlin, 2004.\nSpringer-Verlag.\n[5] M. Littman and J. Boyan. A distributed reinforcement\nlearning scheme for network routing. In Proceedings of\nthe International Workshop on Applications of Neural\nNetworks to Telecommunications, 1993.\n[6] J. Lu and J. Callan. Federated search of text-based\ndigital libraries in hierarchical peer-to-peer networks.\nIn In ECIR\"05, 2005.\n[7] J. Lu and J. Callan. User modeling for full-text\nfederated search in peer-to-peer networks. In ACM\nSIGIR 2006. ACM Press, 2006.\n[8] C. D. Manning and H. Sch\u00c2\u00a8utze. Foundations of\nStatistical Natural Language Processing. The MIT\nPress, Cambridge, Massachusetts, 1999.\n[9] C. R. Palmer and J. G. Steffan. Generating network\ntopologies that obey power laws. In Proceedings of\nGLOBECOM \"2000, November 2000.\n[10] K. Sripanidkulchai, B. Maggs, and H. Zhang. Efficient\ncontent location using interest-based locality in\npeer-topeer systems. In INFOCOM, 2003.\n[11] D. Subramanian, P. Druschel, and J. Chen. Ants and\nreinforcement learning: A case study in routing in\ndynamic networks. In In Proceedings of the Fifteenth\nInternational Joint Conference on Artificial\nIntelligence, pages 832-839, 1997.\n[12] J. N. Tao and L. Weaver. A multi-agent, policy\ngradient approach to network routing. In In\nProceedings of the Eighteenth International Conference\non Machine Learning, 2001.\n[13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser. A\nmulti-agent approach for peer-to-peer information\nretrieval. In Proceedings of Third International Joint\nConference on Autonomous Agents and Multi-Agent\nSystems, July 2004.\n[14] H. Zhang and V. Lesser. Multi-agent based\npeer-to-peer information retrieval systems with\nconcurrent search sessions. In Proceedings of the Fifth\nInternational Joint Conference on Autonomous Agents\nand Multi-Agent Systems, May 2006.\n[15] H. Zhang and V. R. Lesser. A dynamically formed\nhierarchical agent organization for a distributed\ncontent sharing system. In 2004 IEEE/WIC/ACM\nInternational Conference on Intelligent Agent\nTechnology (IAT 2004), 20-24 September 2004,\nBeijing, China, pages 169-175. IEEE Computer\nSociety, 2004.\n238 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\n": ["peer-to-peer information retrieval system", "reinforcement learning", "distributed search algorithm", "routing decision", "utility", "network", "learning algorithm", "routing policy", "query", "peer-to-peer information retrieval", "multi-agent learn", "distribute search control", ""]}