{"Robust Classification of Rare Queries\nUsing Web Knowledge\nAndrei Broder, Marcus Fontoura, Evgeniy Gabrilovich,\nAmruta Joshi, Vanja Josifovski, Tong Zhang\nYahoo! Research, 2821 Mission College Blvd, Santa Clara, CA 95054\n{broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com\nABSTRACT\nWe propose a methodology for building a practical robust\nquery classification system that can identify thousands of\nquery classes with reasonable accuracy, while dealing in \nrealtime with the query volume of a commercial web search \nengine. We use a blind feedback technique: given a query,\nwe determine its topic by classifying the web search results\nretrieved by the query. Motivated by the needs of search \nadvertising, we primarily focus on rare queries, which are the\nhardest from the point of view of machine learning, yet in \naggregation account for a considerable fraction of search engine\ntraffic. Empirical evaluation confirms that our methodology\nyields a considerably higher classification accuracy than \npreviously reported. We believe that the proposed methodology\nwill lead to better matching of online ads to rare queries and\noverall to a better user experience.\nCategories and Subject Descriptors\nH.3.3 [Information Storage and Retrieval]: Information\nSearch and Retrieval- relevance feedback, search process\nGeneral Terms\nAlgorithms, Measurement, Performance, Experimentation\n1. INTRODUCTION\nIn its 12 year lifetime, web search had grown \ntremendously: it has simultaneously become a factor in the daily\nlife of maybe a billion people and at the same time an\neight billion dollar industry fueled by web advertising. One\nthing, however, has remained constant: people use very\nshort queries. Various studies estimate the average length of\na search query between 2.4 and 2.7 words, which by all \naccounts can carry only a small amount of information. \nCommercial search engines do a remarkably good job in \ninterpreting these short strings, but they are not (yet!) omniscient.\nTherefore, using additional external knowledge to augment\nthe queries can go a long way in improving the search results\nand the user experience.\nAt the same time, better understanding of query \nmeaning has the potential of boosting the economic underpinning\nof Web search, namely, online advertising, via the sponsored\nsearch mechanism that places relevant advertisements \nalongside search results. For instance, knowing that the query\nSD450 is about cameras while nc4200 is about laptops\ncan obviously lead to more focused advertisements even if no\nadvertiser has specifically bidden on these particular queries.\nIn this study we present a methodology for query \nclassification, where our aim is to classify queries onto a commercial\ntaxonomy of web queries with approximately 6000 nodes.\nGiven such classifications, one can directly use them to \nprovide better search results as well as more focused ads. The\nproblem of query classification is extremely difficult owing to\nthe brevity of queries. Observe, however, that in many cases\na human looking at a search query and the search query \nresults does remarkably well in making sense of it. Of course,\nthe sheer volume of search queries does not lend itself to\nhuman supervision, and therefore we need alternate sources\nof knowledge about the world. For instance, in the example\nabove, SD450 brings pages about Canon cameras, while\nnc4200 brings pages about Compaq laptops, hence to a\nhuman the intent is quite clear.\nSearch engines index colossal amounts of information, and\nas such can be viewed as very comprehensive repositories of\nknowledge. Following the heuristic described above, we \npropose to use the search results themselves to gain additional\ninsights for query interpretation. To this end, we employ\nthe pseudo relevance feedback paradigm, and assume the\ntop search results to be relevant to the query. Certainly,\nnot all results are equally relevant, and thus we use \nelaborate voting schemes in order to obtain reliable knowledge\nabout the query. For the purpose of this study we first \ndispatch the given query to a general web search engine, and\ncollect a number of the highest-scoring URLs. We crawl the\nWeb pages pointed by these URLs, and classify these pages.\nFinally, we use these result-page classifications to classify\nthe original query. Our empirical evaluation confirms that\nusing Web search results in this manner yields substantial\nimprovements in the accuracy of query classification.\nNote that in a practical implementation of our \nmethodology within a commercial search engine, all indexed pages\ncan be pre-classified using the normal text-processing and\nindexing pipeline. Thus, at run-time we only need to run\nthe voting procedure, without doing any crawling or \nclassification. This additional overhead is minimal, and therefore\nthe use of search results to improve query classification is\nentirely feasible in run-time.\nAnother important aspect of our work lies in the choice\nof queries. The volume of queries in today\"s search engines\nfollows the familiar power law, where a few queries appear\nvery often while most queries appear only a few times. While\nindividual queries in this long tail are rare, together they\naccount for a considerable mass of all searches. Furthermore,\nthe aggregate volume of such queries provides a substantial\nopportunity for income through on-line advertising.1\nSearching and advertising platforms can be trained to\nyield good results for frequent queries, including auxiliary\ndata such as maps, shortcuts to related structured \ninformation, successful ads, and so on. However, the tail queries\nsimply do not have enough occurrences to allow statistical\nlearning on a per-query basis. Therefore, we need to \naggregate such queries in some way, and to reason at the level\nof aggregated query clusters. A natural choice for such \naggregation is to classify the queries into a topical taxonomy.\nKnowing which taxonomy nodes are most relevant to the\ngiven query will aid us to provide the same type of support\nfor rare queries as for frequent queries. Consequently, in this\nwork we focus on the classification of rare queries, whose\ncorrect classification is likely to be particularly beneficial.\nEarly studies in query interpretation focused on query\naugmentation through external dictionaries [22]. More \nrecent studies [18, 21] also attempted to gather some \nadditional knowledge from the Web. However, these \nstudies had a number of shortcomings, which we overcome in\nthis paper. Specifically, earlier works in the field used very\nsmall query classification taxonomies of only a few dozens\nof nodes, which do not allow ample specificity for online \nadvertising [11]. They also used a separate ancillary taxonomy\nfor Web documents, so that an extra level of indirection had\nto be employed to establish the correspondence between the\nancillary and the main taxonomies [18].\nThe main contributions of this paper are as follows. First,\nwe build the query classifier directly for the target \ntaxonomy, instead of using a secondary auxiliary structure; this\ngreatly simplifies taxonomy maintenance and development.\nThe taxonomy used in this work is two orders of \nmagnitude larger than that used in prior studies. The \nempirical evaluation demonstrates that our methodology for \nusing external knowledge achieves greater improvements than\nthose previously reported. Since our taxonomy is \nconsiderably larger, the classification problem we face is much more\ndifficult, making the improvements we achieve particularly\nnotable. We also report the results of a thorough \nempirical study of different voting schemes and different depths of\nknowledge (e.g., using search summaries vs. entire crawled\npages). We found that crawling the search results yields\ndeeper knowledge and leads to greater improvements than\nmere summaries. This result is in contrast with prior \nfindings in query classification [20], but is supported by research\nin mainstream text classification [5].\n2. METHODOLOGY\nOur methodology has two main phases. In the first phase,\n1\nIn the above examples, SD450 and nc4200 represent\nfairly old gadget models, and hence there are advertisers\nplacing ads on these queries. However, in this paper we\nmainly deal with rare queries which are extremely difficult\nto match to relevant ads.\nwe construct a document classifier for classifying search \nresults into the same taxonomy into which queries are to be\nclassified. In the second phase, we develop a query classifier\nthat invokes the document classifier on search results, and\nuses the latter to perform query classification.\n2.1 Building the document classifier\nIn this work we used a commercial classification taxonomy\nof approximately 6000 nodes used in a major US search \nengine (see Section 3.1). Human editors populated the \ntaxonomy nodes with labeled examples that we used as training\ninstances to learn a document classifier in phase 1.\nGiven a taxonomy of this size, the computational \nefficiency of classification is a major issue. Few machine \nlearning algorithms can efficiently handle so many different \nclasses, each having hundreds of training examples. Suitable\ncandidates include the nearest neighbor and the Naive Bayes\nclassifier [3], as well as prototype formation methods such\nas Rocchio [15] or centroid-based [7] classifiers. A recent\nstudy [5] showed centroid-based classifiers to be both \neffective and efficient for large-scale taxonomies and \nconsequently, we used a centroid classifier in this work.\n2.2 Query classification by search\nHaving developed a document classifier for the query \ntaxonomy, we now turn to the problem of obtaining a \nclassification for a given query based on the initial search results\nit yields. Let\"s assume that there is a set of documents\nD = d1 . . . dm indexed by a search engine. The search engine\ncan then be represented by a function f = similarity(q, d)\nthat quantifies the affinity between a query q and a \ndocument d. Examples of such affinity scores used in this paper\nare rank-the rank of the document in the ordered list of\nsearch results; static score-the score of the goodness of\nthe page regardless of the query (e.g., PageRank); and \ndynamic score-the closeness of the query and the document.\nQuery classification is determined by first evaluating \nconditional probabilities of all possible classes P(Cj|q), and\nthen selecting the alternative with the highest probability\nCmax = arg maxCj \u00e2\u02c6\u02c6C P(Cj|q). Our goal is to estimate the\nconditional probability of each possible class using the search\nresults initially returned by the query. We use the following\nformula that incorporates classifications of individual search\nresults: P(Cj|q) =\nd\u00e2\u02c6\u02c6D\nP(Cj|q, d)\u00c2\u00b7 P(d|q) =\nd\u00e2\u02c6\u02c6D\nP(q|Cj, d)\nP(q|d)\n\u00c2\u00b7 P(Cj|d)\u00c2\u00b7 P(d|q).\nWe assume that P(q|Cj, d) \u00e2\u2030\u02c6 P(q|d), that is, a \nprobability of a query given a document can be determined without\nknowing the class of the query. This is the case for the\nmajority of queries that are unambiguous. Counter \nexamples are queries like \"jaguar\" (animal and car brand) or \n\"apple\" (fruit and computer manufacturer), but such \nambiguous queries can not be classified by definition, and usually\nconsists of common words. In this work we concentrate on\nrare queries, that tend to contain rare words, be longer, and\nmatch fewer documents; consequently in our setting this \nassumption mostly holds. Using this assumption, we can write\nP(Cj|q) = d\u00e2\u02c6\u02c6D P(Cj|d)\u00c2\u00b7 P(d|q). The conditional \nprobability of a classification for a given document P(Cj|d) is\nestimated using the output of the document classifier \n(section 2.1). While P(d|q) is harder to compute, we consider\nthe underlying relevance model for ranking documents given\na query. This issue is further explored in the next section.\n2.3 Classification-based relevance model\nIn order to describe a formal relationship of classification\nand ad placement (or search), we consider a model for using\nclassification to determine ads (or search) relevance. Let a\nbe an ad and q be a query, we denote by R(a, q) the relevance\nof a to q. This number indicates how relevant the ad a is\nto query q, and can be used to rank ads a for a given query\nq. In this paper, we consider the following approximation of\nrelevance function:\nR(a, q) \u00e2\u2030\u02c6 RC (a, q) =\nCj \u00e2\u02c6\u02c6C\nw(Cj)s(Cj, a)s(Cj, q). (1)\nThe right hand-side expresses how we use the \nclassification scheme C to rank ads, where s(c, a) is a scoring function\nthat specifies how likely a is in class c, and s(c, q) is a \nscoring function that specifies how likely q is in class c. The\nvalue w(c) is a weighting term for category c, indicating the\nimportance of category c in the relevance formula.\nThis relevance function is an adaptation of the traditional\nword-based retrieval rules. For example, we may let \ncategories be the words in the vocabulary. We take s(Cj, a) as\nthe word counts of Cj in a, s(Cj, q) as the word counts of\nCj in q, and w(Cj) as the IDF term weighting for word Cj.\nWith such choices, the method given by (1) becomes the\nstandard TFIDF retrieval rule.\nIf we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and\nw(Cj) = 1/P(Cj), and assume that q and a are \nindependently generated given a hidden concept C, then we have\nRC (a, q) =\nCj \u00e2\u02c6\u02c6C\nP(Cj|a)P(Cj|q)/P(Cj)\n=\nCj \u00e2\u02c6\u02c6C\nP(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).\nThat is, the ads are ranked according to P(q|a). This \nrelevance model has been employed in various statistical \nlanguage modeling techniques for information retrieval. The \nintuition can be described as follows. We assume that a person\nsearches an ad a by constructing a query q: the person first\npicks a concept Cj according to the weights P(Cj|a), and\nthen constructs a query q with probability P(q|Cj) based\non the concept Cj. For this query generation process, the\nads can be ranked based on how likely the observed query\nis generated from each ad.\nIt should be mentioned that in our case, each query and ad\ncan have multiple categories. For simplicity, we denote by Cj\na random variable indicating whether q belongs to category\nCj. We use P(Cj|q) to denote the probability of q belonging\nto category Cj. Here the sum Cj \u00e2\u02c6\u02c6C P(Cj|q) may not equal\nto one. We then consider the following ranking formula:\nRC (a, q) =\nCj \u00e2\u02c6\u02c6C\nP(Cj|a)P(Cj|q). (2)\nWe assume the estimation of P(Cj|a) is based on an existing\ntext-categorization system (which is known). Thus, we only\nneed to obtain estimates of P(Cj|q) for each query q.\nEquation (2) is the ad relevance model that we consider in\nthis paper, with unknown parameters P(Cj|q) for each query\nq. In order to obtain their estimates, we use search results\nfrom major US search engines, where we assume that the\nranking formula in (2) gives good ranking for search. That\nis, top results ranked by search engines should also be ranked\nhigh by this formula. Therefore given a query q, and top K\nresult pages d1(q), . . . , dK (q) from a major search engine, we\nfit parameters P(Cj|q) so that RC (di(q), q) have high scores\nfor i = 1, . . . , K. It is worth mentioning that using this\nmethod we can only compute relative strength of P(Cj|q),\nbut not the scale, because scale does not affect ranking.\nMoreover, it is possible that the parameters estimated may\nbe of the form g(P(Cj|q)) for some monotone function g(\u00c2\u00b7) of\nthe actually conditional probability g(P(Cj|q)). Although\nthis may change the meaning of the unknown parameters\nthat we estimate, it does not affect the quality of using the\nformula to rank ads. Nor does it affect query classification\nwith appropriately chosen thresholds. In what follows, we\nconsider two methods to compute the classification \ninformation P(Cj|q).\n2.4 The voting method\nWe would like to compute P(Cj|q) so that RC (di(q), q)\nare high for i = 1, . . . , K and RC (d, q) are low for a \nrandom document d. Assume that the vector [P(Cj|d)]Cj \u00e2\u02c6\u02c6C is\nrandom for an average document, then the condition that\nCj \u00e2\u02c6\u02c6C P(Cj|q)2\nis small implies that RC (d, q) is also small\naveraged over d. Thus, a natural method is to maximize\nK\ni=1 wiRC (di(q), q) subject to Cj \u00e2\u02c6\u02c6C P(Cj|q)2\nbeing\nsmall, where wi are weights associated with each rank i:\nmax\n[P (\u00c2\u00b7|q)]\n\u00ef\u00a3\u00ae\n\u00ef\u00a3\u00b0 1\nK\nK\ni=1\nwi\nCj \u00e2\u02c6\u02c6C\nP(Cj|di(q))P(Cj|q) \u00e2\u02c6\u2019 \u00ce\u00bb\nCj \u00e2\u02c6\u02c6C\nP(Cj|q)2\n\u00ef\u00a3\u00b9\n\u00ef\u00a3\u00bb ,\nwhere we assume K\ni=1 wi = 1, and \u00ce\u00bb > 0 is a tuning\nregularization parameter. The optimal solution is\nP(Cj|q) =\n1\n2\u00ce\u00bb\nK\ni=1\nwiP(Cj|di(q)).\nSince both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may\njust take \u00ce\u00bb = 0.5 to align the scale. In the experiment,\nwe will simply take uniform weights wi. A more complex\nstrategy is to let w depend on d as well:\nP(Cj|q) =\nd\nw(d, q)g(P(Cj|d)),\nwhere g(x) is a certain transformation of x.\nIn this general formulation, w(d, q) may depend on factors\nother than the rank of d in the search engine results for q.\nFor example, it may be a function of r(d, q) where r(d, q)\nis the relevance score returned by the underlying search \nengine. Moreover, if we are given a set of hand-labeled training\ncategory/query pairs (C, q), then both the weights w(d, q)\nand the transformation g(\u00c2\u00b7) can be learned using standard\nclassification techniques.\n2.5 Discriminative classification\nWe can treat the problem of estimating P(Cj|q) as a\nclassification problem, where for each q, we label di(q) for\ni = 1, . . . , K as positive data, and the remaining documents\nas negative data. That is, we assign label yi(q) = 1 for di(q)\nwhen i \u00e2\u2030\u00a4 K, and label yi(q) = \u00e2\u02c6\u20191 for di(q) when i > K.\nIn this setting, the classification scoring rule for a \ndocument di(q) is linear. Let xi(q) = [P(Cj|di(q))], and w =\n[P(Cj|q)], then Cj \u00e2\u02c6\u02c6C P(Cj|q)P(Cj|di(q)) = w\u00c2\u00b7xi(q). The\nvalues P(Cj|d) are the features for the linear classifier, and\n[P(Cj|d)] is the weight vector, which can be computed \nusing any linear classification method. In this paper, we \nconsider estimating w using logistic regression [17] as follows:\nP(\u00c2\u00b7|q) = arg minw i ln(1 + e\u00e2\u02c6\u2019w\u00c2\u00b7xi(q)yi(q)\n).\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n0 1 2 3 4 5 6 7 8 9 10\nNumberofcategories\nTaxonomy level\nFigure 1: Number of categories by level\n3. EVALUATION\nIn this section, we evaluate our methodology that uses\nWeb search results for improving query classification.\n3.1 Taxonomy\nOur choice of taxonomy was guided by a Web \nadvertising application. Since we want the classes to be useful for\nmatching ads to queries, the taxonomy needs to be \nelaborate enough to facilitate ample classification specificity. For\nexample, classifying all medical queries into one node will\nlikely result in poor ad matching, as both sore foot and\nflu queries will end up in the same node. The ads \nappropriate for these two queries are, however, very different. To\navoid such situations, the taxonomy needs to provide \nsufficient discrimination between common commercial topics.\nTherefore, in this paper we employ an elaborate taxonomy\nof approximately 6000 nodes, arranged in a hierarchy with\nmedian depth 5 and maximum depth 9. Figure 1 shows the\ndistribution of categories by taxonomy levels. Human \neditors populated the taxonomy with labeled queries (approx.\n150 queries per node), which were used as a training set; a\nsmall fraction of queries have been assigned to more than\none category.\n3.2 Digression: the basics of sponsored search\nTo discuss our set of evaluation queries, we need a brief \nintroduction to some basic concepts of Web advertising. \nSponsored search (or paid search) advertising is placing textual\nads on the result pages of web search engines, with ads \nbeing driven by the originating query. All major search \nengines (Google, Yahoo!, and MSN) support such ads and act\nsimultaneously as a search engine and an ad agency. These\ntextual ads are characterized by one or more bid phrases\nrepresenting those queries where the advertisers would like\nto have their ad displayed. (The name bid phrase comes\nfrom the fact that advertisers bid various amounts to secure\ntheir position in the tower of ads associated to a query. A\ndiscussion of bidding and placement mechanisms is beyond\nthe scope of this paper [13].\nHowever, many searches do not explicitly use phrases that\nsomeone bids on. Consequently, advertisers also buy broad\nmatches, that is, they pay to place their advertisements\non queries that constitute some modification of the desired\nbid phrase. In broad match, several syntactic modifications\ncan be applied to the query to match it to the bid phrase,\ne.g., dropping or adding words, synonym substitution, etc.\nThese transformations are based on rules and dictionaries.\nAs advertisers tend to cover high-volume and high-revenue\nqueries, broad-match queries fall into the tail of the \ndistribution with respect to both volume and revenue.\n3.3 Data sets\nWe used two representative sets of 1000 queries. Both sets\ncontain queries that cannot be directly matched to \nadvertisements, that is, none of the queries contains a bid phrase\n(this means we eliminated practically all popular queries).\nThe first set of queries can be matched to at least one ad\nusing broad match as described above. Queries in the second\nset cannot be matched even by broad match, and therefore\nthe search engine used in our study does not currently \ndisplay any advertising for them. In a sense, these are even\nmore rare queries and further away from common queries.\nAs a measure of query rarity, we estimated their frequency\nin a month worth of query logs for a major US search \nengine; the median frequency was 1 for queries in Set 1 and 0\nfor queries in Set 2.\nThe queries in the two sets differ in their classification\ndifficulty. In fact, queries in Set 2 are difficult to interpret\neven for human evaluators. Queries in Set 1 have on average\n3.50 words, with the longest one having 11 words; queries\nin Set 2 have on average 4.39 words, with the longest query\nof 81 words. Recent studies estimate the average length of\nweb queries to be just under 3 words2\n, which is lower than\nin our test sets. As another measure of query difficulty,\nwe measured the fraction of queries that contain quotation\nmarks, as the latter assist query interpretation by \nmeaningfully grouping the words. Only 8% queries in Set 1 and 14%\nin Set 2 contained quotation marks.\n3.4 Methodology and evaluation metrics\nThe two sets of queries were classified into the target \ntaxonomy using the techniques presented in section 2. Based\non the confidence values assigned, the top 3 classes for each\nquery were presented to human evaluators. These \nevaluators were trained editorial staff who possessed knowledge\nabout the taxonomy. The editors considered every \nqueryclass pair, and rated them on the scale 1 to 4, with 1 \nmeaning the classification is highly relevant and 4 meaning it is\nirrelevant for the query. About 2.4% queries in Set 1 and\n5.4% queries in Set 2 were judged to be unclassifiable (e.g.,\nrandom strings of characters), and were consequently \nexcluded from evaluation. To compute evaluation metrics, we\ntreated classifications with ratings 1 and 2 to be correct, and\nthose with ratings 3 and 4 to be incorrect.\nWe used standard evaluation metrics: precision, recall and\nF1. In what follows, we plot precision-recall graphs for all\nthe experiments. For comparison with other published \nstudies, we also report precision and F1 values corresponding to\ncomplete recall (R = 1). Owing to the lack of space, we\nonly show graphs for query Set 1; however, we show the\nnumerical results for both sets in the tables.\n3.5 Results\nWe compared our method to a baseline query classifier\nthat does not use any external knowledge. Our baseline\nclassifier expanded queries using standard query expansion\ntechniques, grouped their terms using a phrase recognizer,\nboosted certain phrases in the query based on their \nstatistical properties, and performed classification using the\n2\n\nhttp://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.00.90.80.70.60.50.40.30.20.1\nPrecision\nRecall\nBaseline\nEngine A full-page\nEngine A summary\nEngine B full-page\nEngine B summary\nFigure 2: The effect of external knowledge\nnearest-neighbor approach. This baseline classifier is \nactually a production version of the query classifier running in a\nmajor US search engine.\nIn our experiments, we varied values of pertinent \nparameters that characterize the exact way of using search results.\nIn what follows, we start with the general assessment of the\neffect of using Web search results. We then proceed to \nexploring more refined techniques, such as using only search\nsummaries versus actually crawling the returned URLs. We\nalso experimented with using different numbers of search\nresults per query, as well as with varying the number of\nclassifications considered for each search result. For lack of\nspace, we only show graphs for Set 1 queries and omit the\ngraphs for Set 2 queries, which exhibit similar phenomena.\n3.5.1 The effect of external knowledge\nQueries by themselves are very short and difficult to \nclassify. We use top search engine results for collecting \nbackground knowledge for queries. We employed two major US\nsearch engines, and used their results in two ways, either\nonly summaries or the full text of crawled result pages.\nFigure 2 and Table 1 show that such extra knowledge \nconsiderably improves classification accuracy. Interestingly, we\nfound that search engine A performs consistently better with\nfull-page text, while search engine B performs better when\nsummaries are used.\nEngine Context Prec. F1 Prec. F1\nSet 1 Set 1 Set 2 Set 2\nA full-page 0.72 0.84 0.509 0.721\nB full-page 0.706 0.827 0.497 0.665\nA summary 0.586 0.744 0.396 0.572\nB summary 0.645 0.788 0.467 0.638\nBaseline 0.534 0.696 0.365 0.536\nTable 1: The effect of using external knowledge\n3.5.2 Aggregation techniques\nThere are two major ways to use search results as \nadditional knowledge. First, individual results can be classified\nseparately, with subsequent voting among individual \nclassifications. Alternatively, individual search results can be\nbundled together as one meta-document and classified as\nsuch using the document classifier. Figure 3 presents the\nresults of these two approaches When full-text pages are\nused, the technique using individual classifications of search\nresults evidently outperforms the bundling approach by a\nwide margin. However, in the case of summaries, bundling\ntogether is found to be consistently better than individual\nclassification. This is because summaries by themselves are\ntoo short to be classified correctly individually, but when\nbundled together they are much more stable.\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.00.90.80.70.60.50.40.30.20.1\nPrecision\nRecall\nBaseline\nBundled full-page\nVoting full-page\nBundled summary\nVoting summary\nFigure 3: Voting vs. Bundling\n3.5.3 Full page text vs. summary\nTo summarize the two preceding sections, background\nknowledge for each query is obtained by using either the\nfull-page text or only the summaries of the top search \nresults. Full page text was found to be more in conjunction\nwith voted classification, while summaries were found to be\nuseful when bundled together. The best results overall were\nobtained with full-page results classified individually, with\nsubsequent voting used to determine the final query \nclassification. This observation differs from findings by Shen et al.\n[20], who found summaries to be more useful. We attribute\nthis distinction to the fact that the queries we used in this\nstudy are tail ones, which are rare and difficult to classify.\n3.5.4 Varying the number of classes per search result\nWe also varied the number of classifications per search \nresult, i.e., each result was permitted to have either 1, 3, or\n5 classes. Figure 4 shows the corresponding precision-recall\ngraphs for both full-page and summary-only settings. As can\nbe readily seen, all three variants produce very similar \nresults. However, the precision-recall curve for the 1-class \nexperiment has higher fluctuations. Using 3 classes per search\nresult yields a more stable curve, while with 5 classes per\nresult the precision-recall curve is very smooth. Thus, as we\nincrease the number of classes per result, we observe higher\nstability in query classification.\n3.5.5 Varying the number of search results obtained\nWe also experimented with different numbers of search\nresults per query. Figure 5 and Table 2 present the results\nof this experiment. In line with our intuition, we observed\nthat classification accuracy steadily rises as we increase the\nnumber of search results used from 10 to 40, with a slight\ndrop as we continue to use even more results (50). This\nis because using too few search results provides too little\nexternal knowledge, while using too many results introduces\nextra noise.\nUsing paired t-test, we assessed the statistical significance\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.00.90.80.70.60.50.40.30.20.1\nPrecision\nRecall\nBaseline\n1 class full-page\n3 classes full-page\n5 classes full-page\n1 class summary\n3 classes summary\n5 classes summary\nFigure 4: Varying the number of classes per page\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.00.90.80.70.60.50.40.30.20.1\nPrecision\nRecall\n10\n20\n30\n40\n50\nBaseline\nFigure 5: Varying the number of results per query\nof the improvements due to our methodology versus the\nbaseline. We found the results to be highly significant (p <\n0.0005), thus confirming the value of external knowledge for\nquery classification.\n3.6 Voting versus alternative methods\nAs explained in Section 2.2, one may use several methods\nto classify queries from search engine results based on our\nrelevance model. As we have seen, the voting method works\nquite well. In this section, we compare the performance of\nvoting top-ten search results to the following two methods:\n\u00e2\u20ac\u00a2 A: Discriminative learning of query-classification based\non logistic regression, described in Section 2.5.\n\u00e2\u20ac\u00a2 B: Learning weights based on quality score returned\nby a search engine. We discretize the quality score\ns(d, q) of a query/document pair into {high, medium,\nlow}, and learn the three weights w on a set of training\nqueries, and test the performance on holdout queries.\nThe classification formula, as explained at the end of\nSection 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).\nMethod B requires a training/testing split. Neither voting\nnor method A requires such a split; however, for consistency,\nwe randomly draw 50-50 training/testing splits for ten times,\nand report the mean performance \u00c2\u00b1 standard deviation on\nthe test-split for all three methods. For this experiment,\ninstead of precision and recall, we use DCG-k (k = 1, 5),\npopular in search engine evaluation. The DCG (discounted\ncumulated gain) metric, described in [8], is a ranking \nmeasure where the system is asked to rank a set of candidates (in\nNumber of results Precision F1\nbaseline 0.534 0.696\n10 0.706 0.827\n20 0.751 0.857\n30 0.796 0.886\n40 0.807 0.893\n50 0.798 0.887\nTable 2: Varying the number of search results\nour case, judged categories for each query), and computes\nfor each query q: DCGk(q) = k\ni=1 g(Ci(q))/ log2(i + 1),\nwhere Ci(q) is the i-th category for query q ranked by the\nsystem, and g(Ci) is the grade of Ci: we assign grade of\n10, 5, 1, 0 to the 4-point judgment scale described earlier to\ncompute DCG. The decaying choice of log2(i + 1) is \nconventional, which does not have particular importance. The\noverall DCG of a system is the averaged DCG over queries.\nWe use this metric instead of precision/recall in this \nexperiment because it can directly handle multi-grade output.\nTherefore as a single metric, it is convenient for comparing\nthe methods. Note that precision/recall curves used in the\nearlier sections yield some additional insights not \nimmediately apparent from the DCG numbers.\nSet 1\nMethod DCG-1 DCG-5\nOracle 7.58 \u00c2\u00b1 0.19 14.52 \u00c2\u00b1 0.40\nVoting 5.28 \u00c2\u00b1 0.15 11.80 \u00c2\u00b1 0.31\nMethod A 5.48 \u00c2\u00b1 0.16 12.22 \u00c2\u00b1 0.34\nMethod B 5.36 \u00c2\u00b1 0.18 12.15 \u00c2\u00b1 0.35\nSet 2\nMethod DCG-1 DCG-5\nOracle 5.69 \u00c2\u00b1 0.18 9.94 \u00c2\u00b1 0.32\nVoting 3.50 \u00c2\u00b1 0.17 7.80 \u00c2\u00b1 0.28\nMethod A 3.63 \u00c2\u00b1 0.23 8.11 \u00c2\u00b1 0.33\nMethod B 3.55 \u00c2\u00b1 0.18 7.99 \u00c2\u00b1 0.31\nTable 3: Voting and alternative methods\nResults from our experiments are given in Table 3. The\noracle method is the best ranking of categories for each query\nafter seeing human judgments. It cannot be achieved by any\nrealistic algorithm, but is included here as an absolute \nupper bound on DCG performance. The simple voting method\nperforms very well in our experiments. The more \ncomplicated methods may lead to moderate performance gain\n(especially method A, which uses discriminative training in\nSection 2.5). However, both methods are computationally\nmore costly, and the potential gain is minor enough to be\nneglected. This means that as a simple method, voting is\nquite effective.\nWe can observe that method B, which uses quality score\nreturned by a search engine to adjust importance weights\nof returned pages for a query, does not yield appreciable\nimprovement. This implies that putting equal weights \n(voting) performs similarly as putting higher weights to higher\nquality documents and lower weights to lower quality \ndocuments (method B), at least for the top search results. It\nmay be possible to improve this method by including other\npage-features that can differentiate top-ranked search \nresults. However, the effectiveness will require further \ninvestigation which we did not test. We may also observe that\nthe performance on Set 2 is lower than that on Set 1, which\nmeans queries in Set 2 are harder than those in Set 1.\n3.7 Failure analysis\nWe scrutinized the cases when external knowledge did\nnot improve query classification, and identified three main\ncauses for such lack of improvement. (1)Queries containing\nrandom strings, such as telephone numbers - these queries\ndo not yield coherent search results, and so the latter cannot\nhelp classification (around 5% of queries were of this kind).\n(2) Queries that yield no search results at all; there were 8%\nsuch queries in Set 1 and 15% in Set 2. (3) Queries \ncorresponding to recent events, for which the search engine did\nnot yet have ample coverage (around 5% of queries). One\nnotable example of such queries are entire names of news\narticles-if the exact article has not yet been indexed by\nthe search engine, search results are likely to be of little use.\n4. RELATED WORK\nEven though the average length of search queries is \nsteadily increasing over time, a typical query is still shorter than\n3 words. Consequently, many researchers studied possible\nways to enhance queries with additional information.\nOne important direction in enhancing queries is through\nquery expansion. This can be done either using electronic\ndictionaries and thesauri [22], or via relevance feedback \ntechniques that make use of a few top-scoring search results.\nEarly work in information retrieval concentrated on \nmanually reviewing the returned results [16, 15]. However, the\nsheer volume of queries nowadays does not lend itself to\nmanual supervision, and hence subsequent works focused\non blind relevance feedback, which basically assumes top\nreturned results to be relevant [23, 12, 4, 14].\nMore recently, studies in query augmentation focused on\nclassification of queries, assuming such classifications to be\nbeneficial for more focused query interpretation. Indeed,\nKowalczyk et al. [10] found that using query classes \nimproved the performance of document retrieval.\nStudies in the field pursue different approaches for \nobtaining additional information about the queries. Beitzel\net al. [1] used semi-supervised learning as well as unlabeled\ndata [2]. Gravano et al. [6] classified queries with respect\nto geographic locality in order to determine whether their\nintent is local or global.\nThe 2005 KDD Cup on web query classification inspired\nyet another line of research, which focused on enriching\nqueries using Web search engines and directories [11, 18, 20,\n9, 21]. The KDD task specification provided a small \ntaxonomy (67 nodes) along with a set of labeled queries, and posed\na challenge to use this training data to build a query \nclassifier. Several teams used the Web to enrich the queries and\nprovide more context for classification. The main research\nquestions of this approach the are (1) how to build a \ndocument classifier, (2) how to translate its classifications into\nthe target taxonomy, and (3) how to determine the query\nclass based on document classifications.\nThe winning solution of the KDD Cup [18] proposed \nusing an ensemble of classifiers in conjunction with searching\nmultiple search engines. To address issue (1) above, their \nsolution used the Open Directory Project (ODP) to produce\nan ODP-based document classifier. The ODP hierarchy was\nthen mapped into the target taxonomy using word matches\nat individual nodes. A document classifier was built for\nthe target taxonomy by using the pages in the ODP \ntaxonomy that appear in the nodes mapped to the particular\ntarget node. Thus, Web documents were first classified with\nrespect to the ODP hierarchy, and their classifications were\nsubsequently mapped to the target taxonomy for query \nclassification.\nCompared to this approach, we solved the problem of \ndocument classification directly in the target taxonomy by \nusing the queries to produce document classifier as described\nin Section 2. This simplifies the process and removes the\nneed for mapping between taxonomies. This also \nstreamlines taxonomy maintenance and development. Using this\napproach, we were able to achieve good performance in a\nvery large scale taxonomy. We also evaluated a few \nalternatives how to combine individual document classifications\nwhen actually classifying the query.\nIn a follow-up paper [19], Shen et al. proposed a \nframework for query classification based on bridging between two\ntaxonomies. In this approach, the problem of not having\na document classifier for web results is solved by using a\ntraining set available for documents with a different \ntaxonomy. For this, an intermediate taxonomy with a training set\n(ODP) is used. Then several schemes are tried that \nestablish a correspondence between the taxonomies or allow for\nmapping of the training set from the intermediate taxonomy\nto the target taxonomy. As opposed to this, we built a \ndocument classifier for the target taxonomy directly, without\nusing documents from an intermediate taxonomy. While we\nwere not able to directly compare the results due to the use\nof different taxonomies (we used a much larger taxonomy),\nour precision and recall results are consistently higher even\nover the hardest query set.\n5. CONCLUSIONS\nQuery classification is an important information retrieval\ntask. Accurate classification of search queries is likely to\nbenefit a number of higher-level tasks such as Web search\nand ad matching. Since search queries are usually short, by\nthemselves they usually carry insufficient information for \nadequate classification accuracy. To address this problem, we\nproposed a methodology for using search results as a source\nof external knowledge. To this end, we send the query to a\nsearch engine, and assume that a plurality of the \nhighestranking search results are relevant to the query. Classifying\nthese results then allows us to classify the original query\nwith substantially higher accuracy.\nThe results of our empirical evaluation definitively \nconfirmed that using the Web as a repository of world \nknowledge contributes valuable information about the query, and\naids in its correct classification. Notably, our method \nexhibits significantly higher accuracy than methods described\nin prior studies3\nCompared to prior studies, our approach\ndoes not require any auxiliary taxonomy, and we produce\na query classifier directly for the target taxonomy. \nFurthermore, the taxonomy used in this study is approximately\n2 orders of magnitude larger than that used in prior works.\nWe also experimented with different values of parameters\nthat characterize our method. When using search results,\none can either use only summaries of the results provided by\n3\nSince the field of query classification does not yet have \nestablished and agreed upon benchmarks, direct comparison\nof results is admittedly tricky.\nthe search engine, or actually crawl the results pages for even\ndeeper knowledge. Overall, query classification performance\nwas the best when using the full crawled pages (Table 1).\nThese results are consistent with prior studies [5], which\nfound that using full crawled pages is superior for document\nclassification than using only brief summaries. Our findings,\nhowever, are different from those reported by Shen et al. [19],\nwho found summaries to yield better results. We attribute\nour observations to using a more elaborate voting scheme\namong the classifications of individual search results, as well\nas to using a more difficult set of rare queries.\nIn this study we used two major search engines, A and B.\nInterestingly, we found notable distinctions in the quality of\ntheir output. Notably, for engine A the overall results were\nbetter when using the full crawled pages of the search \nresults, while for engine B it seems to be more beneficial to use\nthe summaries of results. This implies that while the quality\nof search results returned by engine A is apparently better,\nengine B does a better work in summarizing the pages.\nWe also found that the best results were obtained by \nusing full crawled pages and performing voting among their\nindividual classifications. For a classifier that is external to\nthe search engine, retrieving full pages may be prohibitively\ncostly, in which case one might prefer to use summaries to\ngain computational efficiency. On the other hand, for the\nowners of a search engine, full page classification is much\nmore efficient, since it is easy to preprocess all indexed pages\nby classifying them once onto the (fixed) taxonomy. Then,\npage classifications are obtained as part of the meta-data\nassociated with each search result, and query classification\ncan be nearly instantaneous.\nWhen using summaries it appears that better results are\nobtained by first concatenating individual summaries into a\nmeta-document, and then using its classification as a whole.\nWe believe the reason for this observation is that summaries\nare short and inherently noisier, and hence their aggregation\nhelps to correctly identify the main theme. Consistent with\nour intuition, using too few search results yields useful but\ninsufficient knowledge, and using too many search results\nleads to inclusion of marginally relevant Web pages. The\nbest results were obtained when using 40 top search hits.\nIn this work, we first classify search results, and then use\ntheir classifications directly to classify the original query.\nAlternatively, one can use the classifications of search results\nas features in order to learn a second-level classifier. In\nSection 3.6, we did some preliminary experiments in this\ndirection, and found that learning such a secondary classifier\ndid not yield considerably advantages. We plan to further\ninvestigate this direction in our future work.\nIt is also essential to note that implementing our \nmethodology incurs little overhead. If the search engine classifies\ncrawled pages during indexing, then at query time we only\nneed to fetch these classifications and do the voting.\nTo conclude, we believe our methodology for using Web\nsearch results holds considerable promise for substantially\nimproving the accuracy of Web search queries. This is \nparticularly important for rare queries, for which little \nperquery learning can be done, and in this study we proved\nthat such scarceness of information could be addressed by\nleveraging the knowledge found on the Web. We believe\nour findings will have immediate applications to improving\nthe handling of rare queries, both for improving the search\nresults as well as yielding better matched advertisements.\nIn our further research we also plan to make use of session\ninformation in order to leverage knowledge about previous\nqueries to better classify subsequent ones.\n6. REFERENCES\n[1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis,\nA. Chowdhury, and A. Kolcz. Automatic web query\nclassification using labeled and unlabeled training data. In\nProceedings of SIGIR\"05, 2005.\n[2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury,\nand A. Kolcz. Improving automatic query classification via\nsemi-supervised learning. In Proceedings of ICDM\"05, 2005.\n[3] R. Duda and P. Hart. Pattern Classification and Scene\nAnalysis. John Wiley and Sons, 1973.\n[4] E. Efthimiadis and P. Biron. UCLA-Okapi at TREC-2:\nQuery expansion experiments. In TREC-2, 1994.\n[5] E. Gabrilovich and S. Markovitch. Feature generation for\ntext categorization using world knowledge. In IJCAI\"05,\npages 1048-1053, 2005.\n[6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.\nCategorizing web queries according to geographical locality.\nIn CIKM\"03, 2003.\n[7] E. Han and G. Karypis. Centroid-based document\nclassification: Analysis and experimental results. In\nPKDD\"00, September 2000.\n[8] K. Jarvelin and J. Kekalainen. IR evaluation methods for\nretrieving highly relevant documents. In SIGIR\"00, 2000.\n[9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi. The ferrety\nalgorithm for the KDD Cup 2005 problem. In SIGKDD\nExplorations, volume 7. ACM, 2005.\n[10] P. Kowalczyk, I. Zukerman, and M. Niemann. Analyzing\nthe effect of query class on document retrieval performance.\nIn Proc. Australian Conf. on AI, pages 550-561, 2004.\n[11] Y. Li, Z. Zheng, and H. Dai. KDD CUP-2005 report:\nFacing a great challenge. In SIGKDD Explorations,\nvolume 7, pages 91-99. ACM, December 2005.\n[12] M. Mitra, A. Singhal, and C. Buckley. Improving automatic\nquery expansion. In SIGIR\"98, pages 206-214, 1998.\n[13] M. Moran and B. Hunt. Search Engine Marketing, Inc.:\nDriving Search Traffic to Your Company\"s Web Site.\nPrentice Hall, Upper Saddle River, NJ, 2005.\n[14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu,\nand M. Gatford. Okapi at TREC-3. In TREC-3, 1995.\n[15] J. Rocchio. Relevance feedback in information retrieval. In\nThe SMART Retrieval System: Experiments in Automatic\nDocument Processing, pages 313-323. Prentice Hall, 1971.\n[16] G. Salton and C. Buckley. Improving retrieval performance\nby relevance feedback. JASIS, 41(4):288-297, 1990.\n[17] T. Santner and D. Duffy. The Statistical Analysis of\nDiscrete Data. Springer-Verlag, 1989.\n[18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and\nQ. Yang. Q2C@UST: Our winning solution to query\nclassification in KDDCUP 2005. In SIGKDD Explorations,\nvolume 7, pages 100-110. ACM, 2005.\n[19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and\nQ. Yang. Query enrichment for web-query classification.\nACM TOIS, 24:320-352, July 2006.\n[20] D. Shen, J. Sun, Q. Yang, and Z. Chen. Building bridges for\nweb query classification. In SIGIR\"06, pages 131-138, 2006.\n[21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen,\nS. Bridges, and T. Scheffer. Classifying search engine\nqueries using the web as background knowledge. In\nSIGKDD Explorations, volume 7. ACM, 2005.\n[22] E. Voorhees. Query expansion using lexical-semantic\nrelations. In SIGIR\"94, 1994.\n[23] J. Xu and W. Bruce Croft. Improving the effectiveness of\ninformation retrieval with local context analysis. ACM\nTOIS, 18(1):79-112, 2000.\n": ["query classification", "search engine", "search advertising", "machine learning", "relevance feedback", "voting scheme", "crawling", "topical taxonomy", "affinity score", "conditional probability", "adaptation", "information retrieval", "web search", "blind relevance feedback", ""]}