{"Towards Task-based Personal Information Management\nEvaluations\nDavid Elsweiler\nDepartment Computer and Information\nSciences, University of Strathclyde\ndce@cis.strath.ac.uk\nIan Ruthven\nDepartment Computer and Information\nSciences, University of Strathclyde\nir@cis.strath.ac.uk\nABSTRACT\nPersonal Information Management (PIM) is a rapidly \ngrowing area of research concerned with how people store, \nmanage and re-find information. A feature of PIM research is\nthat many systems have been designed to assist users \nmanage and re-find information, but very few have been \nevaluated. This has been noted by several scholars and explained\nby the difficulties involved in performing PIM evaluations.\nThe difficulties include that people re-find information from\nwithin unique personal collections; researchers know little\nabout the tasks that cause people to re-find information;\nand numerous privacy issues concerning personal \ninformation. In this paper we aim to facilitate PIM evaluations by\naddressing each of these difficulties. In the first part, we\npresent a diary study of information re-finding tasks. The\nstudy examines the kind of tasks that require users to re-find\ninformation and produces a taxonomy of re-finding tasks\nfor email messages and web pages. In the second part, we\npropose a task-based evaluation methodology based on our\nfindings and examine the feasibility of the approach using\ntwo different methods of task creation.\nCategories and Subject Descriptors\nH3.3 [Information Search and Retrieval]:\nGeneral Terms\nMeasurement,Management,Experimentation, Human Factors\n1. INTRODUCTION\nPersonal Information Management (PIM) is a rapidly \ngrowing area of research concerned with how people store, \nmanage and re-find information. PIM systems - the methods\nand procedures by which people handle, categorize, and \nretrieve information on a day-to-day basis [18] - are \nbecoming increasingly popular. However the evaluation of these\nPIM systems is problematic. One of the main difficulties is\ncaused by the personal nature of PIM. People collect \ninformation as a natural consequence of completing other tasks.\nThis means that the collections people generate are unique\nto them alone and the information within a collection is \nintrinsically linked with the owner\"s personal experiences. As\npersonal collections are unique, we cannot create evaluation\ntasks that are applicable to all participants in an evaluation.\nSecondly, personal collections may contain information that\nthe participants are uncomfortable sharing within an \nevaluation. The precise nature of this information - what \ninformation individuals would prefer to keep private - varies across\nindividuals making it difficult to base search tasks on the\ncontents of individual collections. Therefore, experimenters\nface a number of challenges in order to conduct realistic but\ncontrolled PIM evaluations.\nA particular feature of PIM research is that many \nsystems have been designed to assist users with managing and\nre-finding their information, but very few have been \nevaluated; a situation noted by several scholars [1, 6, 7]. Recently,\nhowever, researchers have started to focus on ways to \naddress the problem of PIM evaluation. For example, Kelly\n[16] proposes that numerous methodologies must be taken\nto examine and understand the many issues involved in PIM,\nalthough, she makes explicit reference to the need for \nlaboratory based PIM studies and a common set of shared tasks\nto make this possible. Capra [6] also identifies the need for\ncontrolled PIM lab evaluations to complement other \nevaluation techniques, placing specific emphasis on the need to\nunderstand PIM behaviour at the task level.\nIn this paper, we attempt to address the difficulties \ninvolved to faciliate controlled laboratory PIM evaluations.\nIn the first part of this paper we present a diary study of\ninformation re-finding tasks. The study examines the kind\nof tasks that require users to re-find information and \nproduces a taxonomy of re-finding tasks for email messages and\nweb pages. We also look at the features of the tasks that\nmake re-finding difficult. In the second part, we propose\na task-based evaluation methodology based on our findings\nand examine the feasibility of the approach using different\nmethods of task creation. Thus, this paper offers two \ncontributions to the field: an increased understanding of PIM\nbehaviour at the task level and an evaluation method that\nwill facilitate further investigations.\n2. RELATED WORK\nA variety of approaches are available to study PIM. \nNaturalistic approaches study participants performing \nnaturally, completing their own tasks as they occur, within \nfamiliar environments. These approaches allow researchers to\novercome many of the difficulties caused by the personal\nnature of PIM. As the tasks performed are real and not\nsimulated, the participants can utilise their own experiences,\nprevious knowledge and information collections to complete\nthe tasks. A benefit of the approach is that data can be\ncaptured continuously over extended time periods and \nmeasurements can be taken at fixed points in time within these\n[15]. Naturalistic approaches can be applied by \nconducting fieldwork [17, 8], ethnographic methods as suggested by\n[15] or via log file analysis [9, 7]. Both ethnographic and\nfieldwork methods require the presence of an experimenter\nto assess how PIM is performed, which raises a number of\nissues. Firstly, evaluation in this way is expensive; taking\nlong time periods to study small numbers of participants\nand these small samples may not be representative of the\nbehaviour of larger populations. Secondly, because \nparticipants cannot be continually observed, experimenters must\nchoose when to observe and this may affect the findings.\nAn alternative strategy to conducting naturalistic \nevaluations is to utilise log file analysis. This approach makes use\nof logging software that captures a broad sampling of user\nactivities in the context of natural use of a system. In [9]\na novel PIM search tool was deployed to 234 users and the\nlog data provided detailed information about the nature of\nuser queries, interactions with the query interface and about\nproperties of the items retrieved. Log file analysis is a \npowerful methodology as it allows the capture of a large quantity\nof detailed information about how users behave with the\nsystem without the expense and distracting influence of an\nobserver. Nevertheless, there are limitations to this \nstrategy. Firstly, to attain useful results, the deployed prototype\nmust be something that people would use i.e. it has to be\na fully functional piece of software that offers improvement\non the systems ordinarily available to participants. \nDeveloping a research prototype to this standard is beyond the\nresources of many researchers. Further, caution must be\ntaken when analysing logs, as the captured data shows \nnothing about the goals and intentions that the user had at the\ntime. It is, therefore, difficult to make any concrete \nstatements about the reasons for the behaviour depicted in the\nlogs. This reveals a need to complement naturalistic studies\nwith controlled experiments where the experimenter can \nrelate the behaviour of study participants to goals associated\nwith known search tasks.\nLaboratory-based studies simulate users\" real world\nenvironment in the controlled setting of the laboratory, \noffering the ability to study issues that are tightly defined and\nnarrow in scope. One difficulty in performing this kind of\nevaluation is sourcing collections to evaluate. Kelly [16] \nproposes the introduction of a shared test collection that would\nprovide sharable, reusable data sets, tasks and metrics for\nthose interested in conducting PIM research. This may be\nuseful for testing algorithms in a way similar to TREC in\nmainstream IR [13]. However, a shared collection would be\nunsuitable for user studies because it would not be \npossible to incorporate the personal aspects of PIM while using\na common, unfamiliar collection. One alternative approach\nis to ask users to provide their own information collections\nto simulate familiar environments within the lab. This \napproach has been applied to study the re-finding of personal\nphotographs [11], email messages [20], and web-bookmarks\n[21]. The usefulness of this approach depends on how easy\nit is to transfer the collection or gain remote access. \nAnother solution is to use the entire web as a collection when\nstudying web page re-finding [4]. This may be appropriate\nfor studying web page re-finding because previous studies\nhave shown that people often use web search engines for\nthis purpose [5].\nA second difficulty in performing PIM laboratory \nstudies is creating tasks for participants to perform that can be\nsolved by searching a shared or personal collection. Tasks\nrelate to the activity that results in a need for information\n[14] and are acknowledged to be important in determining\nuser behaviour [26]. A large body of work has been carried\nout to understand the nature of tasks and how the type of\ntask influences user information seeking behaviour. For \nexample, tasks have been categorised in terms of increasing\ncomplexity [3] and task complexity has been suggested to\naffect how searchers perceive their information needs [25]\nand how they try to find information [3]. Other previous\nwork has provided methodologies that allow the simulation\nof tasks when studying information seeking behaviour [2].\nHowever, little is known about the kinds of tasks that cause\npeople to search their personal stores or re-find information\nthat they have seen before. Consequently, it is difficult to\ndevise simulated work task situations for PIM. The \nexception is the study of personal photograph management, where\nRodden\"s work on categorising personal photograph search\ntasks has facilitated the creation of simulated work task \nsituations [22]. There have been other suggestions as to how\nto classify PIM tasks. For example, [5] asked participants to\nclassify tasks based on how frequently they perform the task\ntype in their daily life and how familiar they were with the\nlocation of the sought after information and several scholars\nhave classified information objects by the frequency of their\nuse e.g. [24]. While these are interesting properties that\nmay affect how a task will be performed, they do not give\nexperimenters enough scope to devise tasks.\nPersonal collections are one reason why task creation is so\ndifficult. Rodden\"s photo task taxonomy provides a solution\nhere because it allows tasks, tailored to private collections\nto be categorised. Systems can then be compared across\ntask types for different users [11]. Unfortunately, no \nequivalent taxonomy exists for other types of information object.\nFurther, other types of object are more sensitive to privacy\nthan photographs; it is unlikely that participants would be\nas content to allow researchers to browse their email \ncollections to create tasks as they were with photographs in [11].\nThis presents a serious problem - how can researchers \ndevise tasks that correspond to private collections without an\nunderstanding of the kinds of tasks people perform or \njeopardising the privacy of study participants? A few methods\nhave been proposed. For example, [20] studied email search\nby asking participants to re-find emails that had been sent\nto every member in a department; allowing the same tasks\nto be used for all of the study participants. This approach\nensured that privacy issues were avoided and participants\ncould use things that they remember to complete tasks.\nNevertheless, the systems were only tested using one type of\ntask - participants were asked to find single emails, each of\nwhich shared common properties. In section 4 we show that\npeople perform a wider range of email re-finding tasks than\nthis. In [4], generic search tasks were artificially created by\nrunning evaluations over two sessions. In the first session,\nparticipants were asked to complete work tasks that involved\nfinding some unknown information. In the second session,\nparticipants completed the same tasks again, which \nnaturally involved some re-finding behaviour. The limitations of\nthis technique are that it does not allow participants to \nexploit any personal connections with the information because\nthe information they are looking for may not correspond to\nany other aspect of their lives. Further, if time is utilised by\na system or interface being tested the approach is unsuitable\nbecause all of the objects found in the first session will have\nbeen accessed within the same time period.\nOur review of evaluation approaches motivates a \nrequirement for controlled laboratory experiments that allow tightly\ndefined aspects of systems or interfaces to be tested. \nUnfortunately, it has also been shown that there are difficulties\ninvolved in performing this type of evaluation - it is difficult\nto source collections and to devise tasks that correspond to\nprivate collections, while at the same time protect the \nprivacy of the study participants.\nIn the following section we present a diary study of \nrefinding tasks for email and web pages. The outcome is a\nclassification of tasks similar to that devised by Rodden for\npersonal photographs [22]. In section 5 we build on this\nwork by examining methods for creating tasks that do not\ncompromise the privacy of participants and discuss how our\nwork can facilitate task-based PIM user evaluations. We\nshow that by collecting tasks using electronic diaries, not\nonly can we learn about the tasks that cause people to re-find\npersonal information, but we can learn about the contents of\nprivate collections without compromising the privacy of the\nparticipants. This knowledge can then be used to construct\ntasks for use in PIM evaluations.\n3. METHOD\nDiary Studies are a naturalistic technique, offering the\nability to capture factual data, in a natural setting, \nwithout the distracting influence of an observer. Limitations of\nthe technique include difficulties in maintaining participant\ndedication levels and convincing participants that seemingly\nmundane information is useful and should be reported [19].\n[12] suggest that the effects of the negatives can be \nlimited, however, with careful design and good \nimplementation. In our diary study, we followed the suggestions in [12]\nto achieve the best possible data. To this end, we restricted\nthe recorded tasks to web and email re-finding. By asking\nusers to record fewer tasks it was anticipated that \nparticipant apathy would be reduced and dedication levels \nmaintained. The participants were provided with a personalised\nweb form in which they could record details about their\ninformation needs and the contexts in which these needs\ndeveloped. Web forms were deployed rather than \npaperbased diaries because to re-find web and email information\nthe user would be at a computer with an Internet \nconnection and there would be no need to search for a paper-based\ndiary and pen.\nThe diary form solicited the following information: whether\nthe information need related to re-finding a web page or an\nemail message and a description of the task they are \nperforming. This description was to contain both the \ninformation that the participant wished to find and the reason that\nthey needed the information. To help with this, the form\ngave three example task descriptions, which were also \nexplained verbally to each participant during an introductory\nsession. The experimenter ensured that the participants \nunderstood that the tasks to be recorded were not limited to\nthe types shown in the examples. The examples were \nsupplied purely to get participants thinking about the kinds of\nthings they could record and to show the level of and type\nof details expected. The form also asked participants to rate\neach task in terms of difficulty (on a scale from 1-5, where\n1 was very easy and 5 was very hard). Finally, they were\nasked when was the last time they looked at the sought \nafter information. Again, they were able to choose from 5\noptions (less than a day ago, less than a week ago, less than\na month ago, less than a year ago, more than a year ago).\nTime information was used to examine the frequency with\nwhich the participants re-found old and new information,\nand when combined with difficulty ratings created a \npicture of whether or not the time period between accessing\nand re-accessing impacted on how difficult the participants\nperceived tasks to be.\n36 participants, recruited by mass advertisement through\ndepartmental communication channels, research group \nmeetings and undergraduate lectures, were asked to digitally\nrecord details of their information re-finding tasks over a\nperiod of approximately 3 weeks. The final population \nconsisted of 4 academic staff members, 8 research staff \nmembers, 6 research students and 18 undergraduate students.\nThe ages of participants ranged from 19-59. As both \npersonal and work tasks were recorded, the results collected\ncover a broad range of re-finding tasks.\n4. RESULTS\nSeveral analyses were performed on the captured data.\nThe following sections present the findings. Firstly, we \nexamine the kinds of re-finding tasks that were performed both\nwhen searching on email and on the web. Next, we consider\nthe distribution of tasks - which kinds of tasks were \nperformed most often by participants. Lastly, we explore the\nkinds of re-finding tasks that participants perceived as \ndifficult.\n4.1 Nature of Web and Email Re-finding Tasks\nDuring the study 412 tasks were recorded. 150 (36.41%)\nof these tasks were email based, 262 (63.59%) were \nwebbased. As with most diary studies, the number of tasks\nrecorded varied extensively between particpants. The \nmedian number of tasks per participant was 8 (interquartile\nrange (IQR)=9.5). More web tasks (median=5,IQR=7.5)\nwere recorded than email tasks (median=3, IQR=3). This\nmeans that on average each participant recorded \napproximately one task every two days.\nFrom the descriptions supplied by the participants, we\nfound similar features in the recorded tasks for both email\nand web re-finding. Based on this observation a joint \nclassification scheme was devised, encompassing both email and\nweb tasks. The tasks were classified as one of three types:\nlookup tasks, item tasks and multi-item tasks. Lookup tasks\ninvolve searching for specific information from within a \nresource, for example an email or a web page, where the \nresource may or may not be known. Some recorded examples\nof lookup tasks were:\n\u00e2\u20ac\u00a2 LU1: Looking for the course code for a class - it\"s used in a\nscript that is run to set up a practical. I\"d previously obtained\nthis about 3 weeks ago from our website.\n\u00e2\u20ac\u00a2 LU2: I am trying to determine the date by which I step down\nas an External Examiner. This is in an email somewhere\n\u00e2\u20ac\u00a2 LU3: Looking for description of log format from system R\ndeveloped for student project. I think he sent me in it an\nemail\nItem tasks involve looking for a particular email or web\npage, perhaps to pass on to someone else or when the entire\ncontents are needed to complete the task. Some recorded\nexamples of item tasks were:\n\u00e2\u20ac\u00a2 I1: Looking for SIGIR 2002 paper to give to another student\n\u00e2\u20ac\u00a2 I2: Find the receipt of an online airline purchase required to\nclaim expenses\n\u00e2\u20ac\u00a2 I3: I need the peer evaluation forms for the MIA class E sent\nme them by email\nTo clarify, lookup tasks differ from item tasks in two ways\n- in the quantity of information required and in what the\nuser knows about what they are looking for. Lookup tasks\ninvolve a need for a small piece of information e.g. a phone\nnumber or an ingredient, and the user may or may not know\nexactly the resource that contains this information. In item\ntasks the user knows exactly the resource they are looking\nfor and needs the entire contents of that resource.\nMulti-item tasks were tasks that required information that\nwas contained within numerous web pages or email \nmessages. Often these tasks required the user to process or\ncollate the information in order to solve the task. Some\nrecorded examples were:\n\u00e2\u20ac\u00a2 MI1: Looking for obituaries and other material on the novelist\nJohn Fowles, who died at the weekend. Accessed the online\nGuradian and IMES\n\u00e2\u20ac\u00a2 MI2: Trying to find details on Piccolo graphics framework.\nRemind myself of what it is and what it does. Looking to\nbuild a GUI within Eclipse\n\u00e2\u20ac\u00a2 MI3: I am trying to file my emails regarding IPM and I am\nlooking for any emails from or about this journal\nThere were a number of tasks that were difficult to classify.\nFor example, consider the following recorded task:\n\u00e2\u20ac\u00a2 LU4: re-find AS\"s paper on graded relevance assessments \nbecause I want to see how she presented her results for a paper I\nam writing\nThis task actually consists of two sub-tasks: 1 item \ntask(refind the paper) and 1 lookup task (look for specific \ninformation within the paper). It was decided to treat this as a\nlookup task because the user\"s ultimate goal was to access\nand use the information within the resource.\nThere were a number of examples of combined tasks, mainly\nof the form item then lookup, but there were also examples\nof item then multi-item. For example:\n\u00e2\u20ac\u00a2 MI4: re-find Kelkoo website so that I can re-check the prices\nof hair-straighteners for my girlfriend\nA second source of ambiguity came from tasks such as\nfinding an email containing a URL as a means of re-accessing\na web page. It was also decided to categorise these as lookup\ntasks because in all cases these were logged by participants\nas email searches and, within this context, what they were\nlooking for was information within an email.\nAnother problem was that some of the logs lacked the\ndetail required to perform a categorisation e.g.\n\u00e2\u20ac\u00a2 U1: searching for how to retrieve user\"s selection from a \nmessage box. Decided to use some other means\nSuch tasks were labelled as U for unclassifiable. To\nverify the consistency of the taxonomy, the tasks were \nrecategorised by the same researcher after a delay of two weeks.\nThe agreement between the results of the two analyses was\nlargely consistent (96.8%). Further, we asked a researcher\nwith no knowledge of the project or the field to classify a\nsample of 50 tasks. The second researcher achieved a 90%\nagreement. We feel that this high agreement on a large \nnumber of tasks by more than one researcher provides evidence\nfor the reliability of the classification scheme.\nThe distribution of task types is shown in table 1. Overall,\nlookup and item tasks were the most common, with \nmultiitem tasks only representing 8.98% of those recorded. The\ndistribution of the task types was different for web and email\nre-finding. The majority of email tasks (60%) involved \nlooking for information within an email (lookup), in contrast to\nweb tasks where the majority of tasks (52.67%) involved\nlooking for a single web page (item). Another distinction\nwas the number of recorded multi-item tasks for web and\nemail. Multi-item tasks were very rare for email re-finding\n(only 2.67% of email tasks involved searching for multiple\nresources), but comparatively common for web re-finding\n(12.6%).\nLookup Item Multi-item Unclass.\nEmail 90(60%) 52(34.67%) 4(2.67%) 4(2.67%)\nWeb 87(33.21%) 138(52.67%) 33(12.60%) 4(1.53%)\nAll 177(42.96%) 190(46.12%) 37(8.98%) 8(1.94%)\nTable 1: The distribution of task types\nIn addition to the three-way classification described above,\nthe recorded tasks were classified with respect to the \ntemperature metaphor proposed by [24], which classifies \ninformation as one of three temperatures: hot, warm and cold.\nWe classified the tasks using the form data. Information\nthat had been seen less than a day or less than a week \nbefore the task were defined as hot, information that had been\nseen less than a month before the task as warm, and \ninformation that had been seen less than a year or more than a\nyear before the task as cold. Unfortunately, a technical \ndifficulty with the form only allowed 335(81.3%) of the tasks to\nbe classified. The remainder were defined as U for \nunclassifiable. A cross-tabulation of task types and temperatures\nis shown in table 2.\nHot Warm Cold Unclass.\nEmail 50(33.33%) 36(24.00%) 37(24.67%) 27(18%)\nWeb 112(42.75%) 60(22.90%) 40(15.27%) 50(19.08%)\nAll 162(39.32%) 96(23.30%) 77(18.69%) 77(18.69%)\nTable 2: The distribution of temperatures\nMost of the tasks that caused people to re-find web pages\n(42.75%) and email messages (33.33%) involved searching\nfor information that has been accessed in the last week.\nHowever there were also a number of re-finding tasks that \ninvolved searching for older information: 23.30% of the tasks\nrecorded (24.00% for email and 22.90% for web) involved\nsearching for information accessed in the last month and\n18.69% of the tasks recorded (24.67% for email and 15.27%\nfor web) were looking for even older information. This is \nimportant with respect to evaluation because there is \npsychological evidence suggesting that people remember less over\ntime e.g. [23]. This means that users may find searching for\nolder information more difficult or perhaps alter their \nseeking strategy when looking for hot, warm or cold information.\n4.2 What tasks are difficult?\nWe looked for patterns in the recorded data to determine\nif certain tasks were perceived as more difficult than \nothers. For example, we examined whether the media type \naffected how difficult the participants perceived the task to be.\nThere was no evidence that participants found either email\n(median=2 IQR=2) or web (median=2 IQR=2) tasks more\ndifficult. We also investigated whether the type of task or\nthe length of time between accessing and re-accessing made\na task more difficult. Figure 1 shows this information \ngraphically.\nFigure 1: Difficulty ratings for task types\nFrom figure 1, it does not appear that any particular task\ntype was perceived as difficult with respect to the others,\nalthough there is a suggestion that lookup tasks were \nperceived more difficult when looking for cold information than\nhot and item tasks were perceived more difficult for warm\ninformation than hot. To assess the relationship between\ninformation temperature and the perceived difficulty, we\nused Mood\"s median tests to determine whether the rank\nof difficulty scores was in agreement for the information\ntemperatures being compared (p<0.05). For the look-up\ntask data, there was evidence that hot tasks were perceived\neasier than cold (p=0.0001) and that warm tasks were \nperceived easier than cold tasks(p=0.0041), but there was no\nevidence to distinguish between the difficulty ratings of hot\nand warm tasks(p=0.593). For the item task data, there\nwas evidence that hot and cold tasks were rated differently\n(p=0.024), but no evidence to distinguish between hot and\nwarm tasks(p=0.05) or warm and cold tasks(p=0.272).\nThese tests confirm that the length of time between \naccessing and re-accessing the sought after information indeed\ninfluenced how difficult participants perceived the task to\nbe. Nevertheless, the large number of tasks of all types and\ntemperatures rated by participants as easy i.e. < 3, suggests\nthat there are other factors that influence how difficult a task\nis perceived to be. To learn about these factors would \nrequire the kind of user evaluations proposed by [16, 6] - the\nkind of evaluations facilitated by our work.\n4.3 Summary\nIn the first part of this paper, we described a diary study\nof web and email re-finding tasks. We examined the types\nof task that caused the participants to search their personal\nstores and found three main categories of task: tasks where\nthe user requires specific information from within a single \nresource, tasks where a single resource is required, and tasks\nthat require information to be recovered from multiple \nresources. It was discovered that look-up and item tasks were\nrecorded with greater frequency than multi-item tasks. \nAlthough no evidence was found that web or email tasks were\nmore difficult, there was some evidence showing that the\ntime between accessing and re-accessing affected how \ndifficult the participants perceived tasks to be. These findings\nhave implications for evaluating PIM behaviour at the task\nlevel. The remainder of this paper concentrates on this, \ndiscussing what the findings mean with respect to performing\ntask-based PIM user evaluations.\n5. TASK-BASED PIM EVALUATIONS\nThe findings described in section 4 are useful with \nrespect to evaluation because they provide experimenters with\nenough knowledge to conduct controlled user evaluations in\nlab conditions. Greco-Latin square experimental designs can\nbe constructed where participants are assigned n tasks of the\nthree types described above to perform on their own \ncollections using x systems. This would allow the performance of\nthe systems or the behaviour of the participants using \ndifferent systems to be analysed with respect to the type of task\nbeing performed (look-up, item, or multi-item). In the \nfollowing sections we evaluate the feasibility of this approach\nwhen employing different methods of task creation.\n5.1 Using Real Tasks\nOne method of creating realistic re-finding tasks without\ncompromising the privacy of participants is to use real tasks.\nDiary-studies, similar to that described above, would allow\nexperimenters to capture a pool of tasks for participants\nto complete by searching on their own collections. This\nis extremely advantageous because it would allow \nexperimenters to evaluate the behaviour of real users, \ncompleting real search tasks on real collections while in a \ncontrolled environment. There is also the additional benefit\nthat the task descriptions would not make any assumptions\nabout what the user would remember in a real life \nsituation because they would only include the information that\nhad been recorded i.e. the information that was available\nwhen the user originally performed the task. Nevertheless,\nto gain these benefits we must, firstly, confirm that the task\ndescriptions recorded are of sufficient quality to enable the\ntask to be re-performed at a later date. Secondly, we must\nensure that a diary-study would provide experimenters with\nenough tasks to construct a balanced experimental design\nthat would satisfy their data needs.\nTo examine the quality of recorded tasks, 6 weeks after\nthe diary study had completed, we asked 6 of our \nparticipants, selected randomly from the pool of those who\nrecorded enough tasks, to re-perform 5 of their own tasks.\nThe tasks were selected randomly from the pool of those\navailable. The issued tasks consisted of 10 email and 20\nweb tasks, 9 of which were lookup tasks, 12 were item tasks,\nand 8 were multi-item tasks. The issued tasks represented a\nbroad-sampling of the complete set of recorded tasks. They\nalso included tasks with vague descriptions e.g.\n\u00e2\u20ac\u00a2 LU5:Find a software key for an application I required to \nreinstall.\n\u00e2\u20ac\u00a2 LU6:Trying to find a quote to use in a paper. Cannot \nremember the person or the exact quote\nThe usefulness of such tasks would rely on the memories of\nparticipants i.e. would the recorder of task LU5 remember\nwhich application he referred to and would the recorder of\nLU6 remember enough about the context in which the task\ntook place to re-perform the task?\nPresented with the tasks exactly as they recorded them,\nthe participants were asked to re-perform each task with any\nsystem of their choice. Of the 30 tasks issued, 26 (86.67%)\nwere completed without problems, 2 (6.67%) of the tasks\nwere not completed because the description recorded was\ninsufficent to recreate the task, and 2 tasks (6.67%) were\nnot completed because the task was too difficult or the \nrequired web page no longer existed. Experimenters are likely\nto be interested in the final group of tasks because it is \nimportant to discover what makes a task difficult and how user\nbehaviour changes in these circumstances. Therefore, from\nthe 30 tasks tested, only 2 tasks were not of sufficient \nquality to be used in an evaluation situation. Further, there did\nnot seem to be any issue of the type, temperature or \ndifficulty ratings affecting the quality of the task descriptions.\nThese findings suggest that the participants who recorded\nmost tasks in the diary study also recorded tasks with \nsufficient quality. However, did the diary study generate enough\ntasks to satisfy the needs of experimenters?\nParticipant Tasks Lookup Item Multi-item Unclass.\n10 26 16 8 2 0\n43 9 4 5 0 0\n26 9 5 4 0 0\n8 9 8 1 0 0\n40 8 5 3 0 0\n18 7 3 4 0 0\n4 6 5 1 0 0\n7 6 5 0 1 0\n12 5 4 0 0 1\n22 5 4 1 0 0\n36 5 0 5 0 0\n46 5 2 2 0 1\n3 5 3 2 0 0\nTable 3: The quantities of recorded email tasks\nParticipant Tasks Lookup Item Multi-item Unclass.\n26 32 7 20 5 0\n32 31 11 18 2 0\n10 19 0 10 7 2\n33 18 5 13 0 0\n5 15 0 7 2 4\n8 11 0 6 5 0\n22 10 0 3 5 2\n28 10 1 7 2 0\n37 10 1 9 0 0\n35 9 7 2 0 0\n6 9 0 1 8 0\n40 7 1 5 1 0\n9 7 0 0 5 2\n12 7 1 0 3 2\n42 6 0 4 2 0\n29 6 0 3 3 0\n15 5 0 2 1 2\n4 5 0 4 1 0\n43 5 2 3 0 0\n18 5 0 0 3 2\nTable 4: The quantities of recorded web tasks\nNaturally the exact number of tasks required to perform\na user evaluation will depend on the goals of the evaluation,\nthe number of users and the number of systems to be tested\netc. However, for illustrative purposes we chose 5 tasks as\na cut-off point for our data. From tables 3 and 4, which\nshow the quantities of email and web tasks recorded for each\nparticipant, we can see that of the 36 participants, only\n13 (36.1%) recorded 5 or more email tasks and 20 (55.6%)\nrecorded 5 or more web tasks. This means that many of the\nrecruited participants could not actually participate in the\nfinal evaluation. This is a major limitation of using recorded\ntasks in evaluations because participant recruitment for user\ntests is challenging and it may not be possible to recruit\nenough participants if experimenters lose between half and\ntwo-thirds of their populations.\nFurther, there was some imbalance in the numbers of\nrecorded tasks of different types. Some participants recorded\nseveral lookup tasks but very few item tasks and others\nrecorded several item tasks but few lookup tasks. There\nwas also a specific lack of multi-item email tasks. This \nsituation makes it very difficult for experimenters to prepare\nbalanced experimental designs. Therefore, even though our\nfirst test suggests that the quality of recorded tasks was \nsufficient for the participants to re-perform the tasks at a later\nstage, the number of tasks recorded was probably too low\nto make this a viable option for experimental task creation.\nHowever, it may be possible to increase the number of tasks\nrecorded by frequently reminding participants or by making\npersonal visits etc.\n5.2 Using Simulated Tasks Based on Real Tasks\nAnother benefit of diary-studies is that they provide \ninformation about the contents and uses of private \ncollections without invading participants\" privacy. This section\nexplores the possibility of using a combination of the \nknowledge gained from diary studies and other attributes known\nabout participants to artificially create re-finding tasks \ncorresponding to the taxonomy defined in section 4.1. We \nexplain the techniques used and demonstrate the feasibility of\ncreating simulated tasks within the context of a user \nevaluation investigating email re-finding behaviour. Space \nlimitations prevent us from reporting our findings; instead we\nconcentrate on the methods of task creation.\nAs preparation for the evaluation, we performed a \nsecond diary-study, where 34 new participants, consisting of\n16 post-graduate students and 18 under-graduate students,\nrecorded 150 email tasks over a period of approximately 3\nweeks. The collected data revealed several patterns that\nhelped with the creation of artificial tasks. For example,\nstudents in both groups recorded tasks relating to classes\nthat they were taking at the time and often different \nparticipants recorded tasks that involved searching for the same\ninformation. This was useful because it provided us with a\nclue that even though some of the participants did not record\na particular task, it was possible that the task may still be\napplicable to their collections. Other patterns revealed \nincluded that students within the same group often searched\nfor emails containing announcements from the same source.\nFor example, several undergraduate students recorded tasks\nthat included re-finding information relating to job \nvacancies. There were also tasks that were recorded by \nparticipants in both groups. For example, searching for an email\nthat would re-confirm the pin code required to access the\ncomputer labs.\nTo supplement our knowledge of the participants\" email\ncollections, we asked 2 participants from each group to \nprovide email tours. These consisted of short 5-10 minute \nsessions, where participants were asked to explain why they\nuse email, who sends them email, and their organisational\nstrategies. This approach has been used successfully in the\npast as a non-intrusive means to learn about how people\nstore and maintain their personal information [17]. \nOriginally, we had planned to ask more participants to provide\ntours, but we found 2 tours per group was sufficient for\nour needs. Again, patterns emerged that helped with task\ncreation. We found content overlap within and between\ngroups that confirmed many of our observations from the\ndiary study data. For example, the students who gave tours\nrevealed that they received emails from lecturers for \nparticular class assignments, receipts for completed assignments,\nand various announcements from systems support and about\njob vacancies. Importantly, the participants were also able\nto confirm which other students had received the same \ninformation. This confirmed that many of tasks recorded during\nthe diary study were applicable, not only to the recorder,\nbut to every participant in 1 or both groups.\nBased on this initial investigatory work, a set of 15 tasks\n(5 of each type in our taxonomy) was created for each group\nof participants. We also created a set of tasks for a third\ngroup of participants that consisted of research and \nacademic staff members, based on our knowledge of the emails\nour colleagues receive. Where possible we used the \ninformation recorded in the diary study descriptions to provide\na context for the task i.e. a work task or motivation that\nwould require the task to be performed. When the diary\nstudy data did not provide sufficient context information to\nsupply the participants with a robust description of the \ninformation need, we created simulated work task situations\naccording to the guidelines of [2]. A further advantage of\nusing simulated tasks in this way, rather than real-tasks,\nis that some of the users will not have performed the task\nin the recent past and this allows the examination of tasks\nthat look for information of different temperatures. If only\nreal-tasks had been used all of the participants would have\nperformed the tasks during the period of the diary study.\nThe created tasks were used in a final evaluation, where\nwe examined the email re-finding behaviour of users with\nthree different email systems. 21 users (7 in each group)\nperformed 9 tasks each (1 task of each type on each system)\nusing their own personal collections in a Greco-Latin square\nexperimental design. Performing a PIM evaluation in this\nway allowed the examination of re-finding behaviour in a\nway not possible before - we were able to observe the email\nre-finding strategies employed by real users, performing \nrealistic tasks, on their own collections in a controlled \nenvironment. The study revealed that the participants \nremembered different attributes of emails, demostrated different\nfinding behaviour, and exhibited different levels of \nperformance when asked to complete tasks of the different types\nin the taxonomy. The key to both the task creation and the\nanalysis of the results was our taxonomy, which provided the\ntemplate to create tasks and also a means to compare the\nbehaviour and performance of different users (and systems)\nperforming different tasks of the same type. Some of the\nfindings of the evaluation will be published in [10].\nSummarising the approach, to conduct a user experiment\nusing our methodology, researchers would be required to\nperform the following steps: 1)Conduct a diary study as\nabove 1\n. 2)Analyse the recorded tasks looking for overlap\nbetween the participants. 3)Supplement the gained \nknowledge about the contents of participants\" collections by asking\na selection of the participants to provide a tour of their \ncollection. 4)Use the knowledge gained to devise tasks of the\nthree different types defined within the taxonomy. More \nde1\nInformation about this and the diary forms required can be\nfound at http://www.cis.strath.ac.uk/\u00cb\u0153dce/PIMevaluations\ntailed information on how to use the research described in\nthis paper to perform task-based PIM evaluations can be\nfound at our website (see footnote 1).\n6. CONCLUSIONS\nThis paper has focused on overcoming the difficulties \ninvolved in performing PIM evaluations. The personal nature\nof PIM means that it is difficult to construct balanced \nexperiments because participants each have their own unique\ncollections that are self-generated by completing other tasks.\nWe suggested that to incorporate the personal aspects of\nPIM in evaluations, the performance of systems or users\nshould be examined when users complete tasks on their own\ncollections. This approach itself has problems because task\ncreation for personal collections is difficult: researchers don\"t\nknow much about the kinds of re-finding tasks people \nperform and they don\"t know what information is within \nindividual personal collections. In this paper we described ways\nof overcoming these challenges to facilitate task based PIM\nuser evaluations.\nIn the first part of the paper we performed a diary study\nthat examined the tasks that caused people to re-find email\nmessages and web pages. The collected data included a wide\nrange of both work and non-work related tasks, and based on\nthe data we created a taxonomy of web and email re-finding\ntasks. We discovered that people perform three main types\nof re-finding task: tasks that require specific information\nfrom within a single resource, tasks that require a single\ncomplete resource, and tasks that require information to be\nrecovered from multiple resources. In the second part of the\npaper, we discussed the significance of the taxonomy with\nrespect to PIM evaluation. We demonstrated that balanced\nexperiments could be conducted comparing system or user\nperformance on the task categories within the taxonomy.\nWe also suggested two methods of creating tasks that can\nbe completed on personal collections. These methods do\nnot compromise the privacy of study participants. We \nexamined the techniques suggested, firstly by simulating an \nexperimental situation - participants were asked to re-perform\ntheir own tasks as they recorded them, and secondly, in the\ncontext of a full evaluation. Performing evaluations in this\nway will allow systems that have been proposed to improve\nusers\" ability to manage and re-find their information to be\ntested, so that we can learn about the needs and desires\nof users. Thus, this paper has offered two contributions to\nthe field: an increased understanding of PIM behaviour at\nthe task level and an evaluation method that will facilitate\nfurther investigations.\n7. ACKNOWLEDGMENTS\nWe would like to thank Dr Mark Baillie for his insightful\ncomments and help analysing the data.\n8. REFERENCES\n[1] R. Boardman, Improving tool support for personal\ninformation management, Ph.D. thesis, Imperial\nCollege London, 2004.\n[2] P. Borlund, The iir evaluation model: A framework for\nevaluation of interactive information retrieval systems,\nInformation Research 8 (2003), no. 3, paper no. 152.\n[3] K. Bystr\u00c2\u00a8om and K. J\u00c2\u00a8arvelin, Task complexity affects\ninformation seeking and use, Information Processing\nand Management 31 (1995), no. 2, 191-213.\n[4] R. G. Capra and M. A. Perez-Quinones, Re-finding\nfound things: An exploratory study of how users\nre-find information, Tech. report, Virginia Tech, 2003.\n[5] R. G. Capra and M. A. Perez-Quinones, Using web\nsearch engines to find and refind information,\nComputer 38 (2005), no. 10, 36-42.\n[6] R. G. Capra and M. A. Perez-Quinones, Factors and\nevaluation of refinding behaviors., SIGIR 2006\nWorkshop on Personal Information Management,\nAugust 10-11, 2006, Seattle, Washington, 2006.\n[7] E. Cutrell, D.Robbins, S.Dumais, and R.Sarin, Fast,\nflexible filtering with phlat, Proc. SIGCHI \"06 (New\nYork, NY, USA), ACM Press, 2006, pp. 261-270.\n[8] M. Czerwinski, E. Horvitz, and S. Wilhite, A diary\nstudy of task switching and interruptions, Proc.\nSIGCHI \"04, 2004, pp. 175-182.\n[9] S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin,\nand D.C. Robbins, Stuff i\"ve seen: a system for\npersonal information retrieval and re-use, Proc. SIGIR\n\"03:, 2003, pp. 72-79.\n[10] D. Elsweiler and I. Ruthven, Memory and email\nre-finding, In preparation for ACM TOIS CFP special\nissue on Keeping, Re-finding, and Sharing Personal\nInformation (2007).\n[11] D. Elsweiler, I. Ruthven, and C. Jones, Dealing with\nfragmented recollection of context in information\nmanagement, Context-Based Information Retrieval\n(CIR-05) Workshop in CONTEXT-05, 2005.\n[12] D. Elsweiler, I. Ruthven, and C. Jones, Towards\nmemory supporting personal information management\ntools, (to appear in) Journal of the American Society\nfor Information Science and Technology (2007).\n[13] D. Harman, What we have learned, and not learned,\nfrom trec, Proc. ECIR 2000, 2000.\n[14] P. Ingwersen, Information retrieval interaction, Taylor\nGraham, 1992.\n[15] D. Kelly, B. Bederson, M. Czerwinski, J. Gemmell,\nW. Pratt, and M. Skeels (eds.), Pim workshop report:\nMeasurement and design, 2005.\n[16] D. Kelly and J. Teevan, (to appear in) personal\ninformation management, ch. Understanding what\nworks: Evaluating personal information management\ntools, Seattle: University of Washington Press., 2007.\n[17] B. H. Kwasnik, How a personal document\"s intended\nuse or purpose affects its classification in an office,\nSIGIR\"89 23 (1989), no. SI, 207-210.\n[18] M.W. Lansdale, The psychology of personal\ninformation management., Appl Ergon 19 (1988),\nno. 1, 55-66.\n[19] L. Palen and M. Salzman, Voice-mail diary studies for\nnaturalistic data capture under mobile conditions,\nCSCW \"02: Proceedings of the 2002 ACM conference\non Computer supported cooperative work, 2002.\n[20] M. Ringel, E. Cutrell, S. Dumais, and E. Horvitz,\nMilestones in time: The value of landmarks in\nretrieving information from personal stores., Proc.\nINTERACT 2003, 2003.\n[21] G. Robertson, M. Czerwinski, K. Larson, D. C.\nRobbins, D. Thiel, and M. van Dantzich, Data\nmountain: using spatial memory for document\nmanagement, Proc. UIST \"98:, 1998.\n[22] K. Rodden, How do people organise their photographs,\nBCS IRSG 21st Annual Colloquium on Information\nRetrieval Research,Glasgow, Scotland, 1999.\n[23] D.C. Rubin and A.E. Wenzel, One hundred years of\nforgetting: A quantitative description of retention,\nPsychological Bulletin 103 (1996), 734-760.\n[24] A. J. Sellen and R. H. R. Harper, The myth of the\npaperless office, MIT Press, Cambridge, MA, USA,\n2003.\n[25] P. Vakkari, Task complexity, problem structure and\ninformation actions: Integrating studies in on\ninformation seeking and retrieval., Information\nProcessing and Management 35 (1999), 819-837.\n[26] P. Vakkari, A theory of task-based information\nretrieval, Journal of Documentation 57 (2001), no. 1,\n44-60.\n": ["personal information management", "measurement", "experimenter", "human factor", "re-find information", "privacy issue", "taxonomy", "individual collection", "email message", "naturalistic approach", "laboratory-based study", "user evaluation", ""]}