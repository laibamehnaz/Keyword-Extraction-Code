{"Regularized Clustering for Documents \u00e2\u02c6\u2014\nFei Wang, Changshui Zhang\nState Key Lab of Intelligent Tech. and Systems\nDepartment of Automation, Tsinghua University\nBeijing, China, 100084\nfeiwang03@gmail.com\nTao Li\nSchool of Computer Science\nFlorida International University\nMiami, FL 33199, U.S.A.\ntaoli@cs.fiu.edu\nABSTRACT\nIn recent years, document clustering has been receiving more\nand more attentions as an important and fundamental \ntechnique for unsupervised document organization, automatic\ntopic extraction, and fast information retrieval or filtering.\nIn this paper, we propose a novel method for clustering \ndocuments using regularization. Unlike traditional globally \nregularized clustering methods, our method first construct a\nlocal regularized linear label predictor for each document\nvector, and then combine all those local regularizers with\na global smoothness regularizer. So we call our algorithm\nClustering with Local and Global Regularization (CLGR).\nWe will show that the cluster memberships of the \ndocuments can be achieved by eigenvalue decomposition of a\nsparse symmetric matrix, which can be efficiently solved by\niterative methods. Finally our experimental evaluations on\nseveral datasets are presented to show the superiorities of\nCLGR over traditional document clustering methods.\nCategories and Subject Descriptors\nH.3.3 [Information Storage and Retrieval]: Information\nSearch and Retrieval-Clustering; I.2.6 [Artificial \nIntelligence]: Learning-Concept Learning\nGeneral Terms\nAlgorithms\n1. INTRODUCTION\nDocument clustering has been receiving more and more\nattentions as an important and fundamental technique for\nunsupervised document organization, automatic topic \nextraction, and fast information retrieval or filtering. A good\ndocument clustering approach can assist the computers to\nautomatically organize the document corpus into a \nmeaningful cluster hierarchy for efficient browsing and navigation,\nwhich is very valuable for complementing the deficiencies of\ntraditional information retrieval technologies. As pointed\nout by [8], the information retrieval needs can be expressed\nby a spectrum ranged from narrow keyword-matching based\nsearch to broad information browsing such as what are the\nmajor international events in recent months. Traditional\ndocument retrieval engines tend to fit well with the search\nend of the spectrum, i.e. they usually provide specified\nsearch for documents matching the user\"s query, however,\nit is hard for them to meet the needs from the rest of the\nspectrum in which a rather broad or vague information is\nneeded. In such cases, efficient browsing through a good\ncluster hierarchy will be definitely helpful.\nGenerally, document clustering methods can be mainly\ncategorized into two classes: hierarchical methods and \npartitioning methods. The hierarchical methods group the data\npoints into a hierarchical tree structure using bottom-up or\ntop-down approaches. For example, hierarchical \nagglomerative clustering (HAC) [13] is a typical bottom-up \nhierarchical clustering method. It takes each data point as a single\ncluster to start off with and then builds bigger and bigger\nclusters by grouping similar data points together until the\nentire dataset is encapsulated into one final cluster. On the\nother hand, partitioning methods decompose the dataset\ninto a number of disjoint clusters which are usually \noptimal in terms of some predefined criterion functions. For \ninstance, K-means [13] is a typical partitioning method which\naims to minimize the sum of the squared distance between\nthe data points and their corresponding cluster centers. In\nthis paper, we will focus on the partitioning methods.\nAs we know that there are two main problems existing\nin partitioning methods (like Kmeans and Gaussian \nMixture Model (GMM) [16]): (1) the predefined criterion is \nusually non-convex which causes many local optimal solutions;\n(2) the iterative procedure (e.g. the Expectation \nMaximization (EM) algorithm) for optimizing the criterions usually\nmakes the final solutions heavily depend on the \ninitializations. In the last decades, many methods have been \nproposed to overcome the above problems of the partitioning\nmethods [19][28].\nRecently, another type of partitioning methods based on\nclustering on data graphs have aroused considerable \ninterests in the machine learning and data mining community.\nThe basic idea behind these methods is to first model the\nwhole dataset as a weighted graph, in which the graph nodes\nrepresent the data points, and the weights on the edges \ncorrespond to the similarities between pairwise points. Then\nthe cluster assignments of the dataset can be achieved by\noptimizing some criterions defined on the graph. For \nexample Spectral Clustering is one kind of the most representative\ngraph-based clustering approaches, it generally aims to \noptimize some cut value (e.g. Normalized Cut [22], Ratio Cut [7],\nMin-Max Cut [11]) defined on an undirected graph. After\nsome relaxations, these criterions can usually be optimized\nvia eigen-decompositions, which is guaranteed to be global\noptimal. In this way, spectral clustering efficiently avoids\nthe problems of the traditional partitioning methods as we\nintroduced in last paragraph.\nIn this paper, we propose a novel document clustering \nalgorithm that inherits the superiority of spectral clustering,\ni.e. the final cluster results can also be obtained by exploit\nthe eigen-structure of a symmetric matrix. However, unlike\nspectral clustering, which just enforces a smoothness \nconstraint on the data labels over the whole data manifold [2],\nour method first construct a regularized linear label \npredictor for each data point from its neighborhood as in [25],\nand then combine the results of all these local label \npredictors with a global label smoothness regularizer. So we call\nour method Clustering with Local and Global Regularization\n(CLGR). The idea of incorporating both local and global\ninformation into label prediction is inspired by the recent\nworks on semi-supervised learning [31], and our \nexperimental evaluations on several real document datasets show that\nCLGR performs better than many state-of-the-art clustering\nmethods.\nThe rest of this paper is organized as follows: in section 2\nwe will introduce our CLGR algorithm in detail. The \nexperimental results on several datasets are presented in section\n3, followed by the conclusions and discussions in section 4.\n2. THE PROPOSED ALGORITHM\nIn this section, we will introduce our Clustering with Local\nand Global Regularization (CLGR) algorithm in detail. First\nlet\"s see the how the documents are represented throughout\nthis paper.\n2.1 Document Representation\nIn our work, all the documents are represented by the\nweighted term-frequency vectors. Let W = {w1, w2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , wm}\nbe the complete vocabulary set of the document corpus\n(which is preprocessed by the stopwords removal and words\nstemming operations). The term-frequency vector xi of \ndocument di is defined as\nxi = [xi1, xi2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , xim]T\n, xik = tik log\nn\nidfk\n,\nwhere tik is the term frequency of wk \u00e2\u02c6\u02c6 W, n is the size\nof the document corpus, idfk is the number of documents\nthat contain word wk. In this way, xi is also called the \nTFIDF representation of document di. Furthermore, we also\nnormalize each xi (1 i n) to have a unit length, so\nthat each document is represented by a normalized TF-IDF\nvector.\n2.2 Local Regularization\nAs its name suggests, CLGR is composed of two parts:\nlocal regularization and global regularization. In this \nsubsection we will introduce the local regularization part in detail.\n2.2.1 Motivation\nAs we know that clustering is one type of learning \ntechniques, it aims to organize the dataset in a reasonable way.\nGenerally speaking, learning can be posed as a problem of\nfunction estimation, from which we can get a good \nclassification function that will assign labels to the training dataset\nand even the unseen testing dataset with some cost \nminimized [24]. For example, in the two-class classification \nscenario1\n(in which we exactly know the label of each \ndocument), a linear classifier with least square fit aims to learn\na column vector w such that the squared cost\nJ =\n1\nn\n(wT\nxi \u00e2\u02c6\u2019 yi)2\n(1)\nis minimized, where yi \u00e2\u02c6\u02c6 {+1, \u00e2\u02c6\u20191} is the label of xi. By\ntaking \u00e2\u02c6\u201aJ /\u00e2\u02c6\u201aw = 0, we get the solution\nw\u00e2\u02c6\u2014\n=\nn\ni=1\nxixT\ni\n\u00e2\u02c6\u20191 n\ni=1\nxiyi , (2)\nwhich can further be written in its matrix form as\nw\u00e2\u02c6\u2014\n= XXT\n\u00e2\u02c6\u20191\nXy, (3)\nwhere X = [x1, x2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , xn] is an m \u00c3\u2014 n document matrix,\ny = [y1, y2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , yn]T\nis the label vector. Then for a test\ndocument t, we can determine its label by\nl = sign(w\u00e2\u02c6\u2014T\nu), (4)\nwhere sign(\u00c2\u00b7) is the sign function.\nA natural problem in Eq.(3) is that the matrix XXT\nmay\nbe singular and thus not invertable (e.g. when m n). To\navoid such a problem, we can add a regularization term and\nminimize the following criterion\nJ =\n1\nn\nn\ni=1\n(wT\nxi \u00e2\u02c6\u2019 yi)2\n+ \u00ce\u00bb w 2\n, (5)\nwhere \u00ce\u00bb is a regularization parameter. Then the optimal\nsolution that minimize J is given by\nw\u00e2\u02c6\u2014\n= XXT\n+ \u00ce\u00bbnI\n\u00e2\u02c6\u20191\nXy, (6)\nwhere I is an m \u00c3\u2014 m identity matrix. It has been reported\nthat the regularized linear classifier can achieve very good\nresults on text classification problems [29].\nHowever, despite its empirical success, the regularized \nlinear classifier is on earth a global classifier, i.e. w\u00e2\u02c6\u2014\nis \nestimated using the whole training set. According to [24], this\nmay not be a smart idea, since a unique w\u00e2\u02c6\u2014\nmay not be good\nenough for predicting the labels of the whole input space. In\norder to get better predictions, [6] proposed to train \nclassifiers locally and use them to classify the testing points. For\nexample, a testing point will be classified by the local \nclassifier trained using the training points located in the vicinity\n1\nIn the following discussions we all assume that the \ndocuments coming from only two classes. The generalizations of\nour method to multi-class cases will be discussed in section\n2.5.\nof it. Although this method seems slow and stupid, it is\nreported that it can get better performances than using a\nunique global classifier on certain tasks [6].\n2.2.2 Constructing the Local Regularized Predictors\nInspired by their success, we proposed to apply the local\nlearning algorithms for clustering. The basic idea is that, for\neach document vector xi (1 i n), we train a local label\npredictor based on its k-nearest neighborhood Ni, and then\nuse it to predict the label of xi. Finally we will combine\nall those local predictors by minimizing the sum of their\nprediction errors. In this subsection we will introduce how\nto construct those local predictors.\nDue to the simplicity and effectiveness of the regularized\nlinear classifier that we have introduced in section 2.2.1, we\nchoose it to be our local label predictor, such that for each\ndocument xi, the following criterion is minimized\nJi =\n1\nni\nxj \u00e2\u02c6\u02c6Ni\nwT\ni xj \u00e2\u02c6\u2019 qj\n2\n+ \u00ce\u00bbi wi\n2\n, (7)\nwhere ni = |Ni| is the cardinality of Ni, and qj is the \ncluster membership of xj. Then using Eq.(6), we can get the\noptimal solution is\nw\u00e2\u02c6\u2014\ni = XiXT\ni + \u00ce\u00bbiniI\n\u00e2\u02c6\u20191\nXiqi, (8)\nwhere Xi = [xi1, xi2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , xini ], and we use xik to denote the\nk-th nearest neighbor of xi. qi = [qi1, qi2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , qini ]T\nwith\nqik representing the cluster assignment of xik. The problem\nhere is that XiXT\ni is an m \u00c3\u2014 m matrix with m ni, i.e.\nwe should compute the inverse of an m \u00c3\u2014 m matrix for \nevery document vector, which is computationally prohibited.\nFortunately, we have the following theorem:\nTheorem 1. w\u00e2\u02c6\u2014\ni in Eq.(8) can be rewritten as\nw\u00e2\u02c6\u2014\ni = Xi XT\ni Xi + \u00ce\u00bbiniIi\n\u00e2\u02c6\u20191\nqi, (9)\nwhere Ii is an ni \u00c3\u2014 ni identity matrix.\nProof. Since\nw\u00e2\u02c6\u2014\ni = XiXT\ni + \u00ce\u00bbiniI\n\u00e2\u02c6\u20191\nXiqi,\nthen\nXiXT\ni + \u00ce\u00bbiniI w\u00e2\u02c6\u2014\ni = Xiqi\n=\u00e2\u2021\u2019 XiXT\ni w\u00e2\u02c6\u2014\ni + \u00ce\u00bbiniw\u00e2\u02c6\u2014\ni = Xiqi\n=\u00e2\u2021\u2019 w\u00e2\u02c6\u2014\ni = (\u00ce\u00bbini)\u00e2\u02c6\u20191\nXi qi \u00e2\u02c6\u2019 XT\ni w\u00e2\u02c6\u2014\ni .\nLet\n\u00ce\u00b2 = (\u00ce\u00bbini)\u00e2\u02c6\u20191\nqi \u00e2\u02c6\u2019 XT\ni w\u00e2\u02c6\u2014\ni ,\nthen\nw\u00e2\u02c6\u2014\ni = Xi\u00ce\u00b2\n=\u00e2\u2021\u2019 \u00ce\u00bbini\u00ce\u00b2 = qi \u00e2\u02c6\u2019 XT\ni w\u00e2\u02c6\u2014\ni = qi \u00e2\u02c6\u2019 XT\ni Xi\u00ce\u00b2\n=\u00e2\u2021\u2019 qi = XT\ni Xi + \u00ce\u00bbiniIi \u00ce\u00b2\n=\u00e2\u2021\u2019 \u00ce\u00b2 = XT\ni Xi + \u00ce\u00bbiniIi\n\u00e2\u02c6\u20191\nqi.\nTherefore\nw\u00e2\u02c6\u2014\ni = Xi\u00ce\u00b2 = Xi XT\ni Xi + \u00ce\u00bbiniIi\n\u00e2\u02c6\u20191\nqi 2\nUsing theorem 1, we only need to compute the inverse of\nan ni \u00c3\u2014 ni matrix for every document to train a local label\npredictor. Moreover, for a new testing point u that falls into\nNi, we can classify it by the sign of\nqu = w\u00e2\u02c6\u2014T\ni u = uT\nwi = uT\nXi XT\ni Xi + \u00ce\u00bbiniIi\n\u00e2\u02c6\u20191\nqi.\nThis is an attractive expression since we can determine the\ncluster assignment of u by using the inner-products between\nthe points in {u \u00e2\u02c6\u00aa Ni}, which suggests that such a local\nregularizer can easily be kernelized [21] as long as we define\na proper kernel function.\n2.2.3 Combining the Local Regularized Predictors\nAfter all the local predictors having been constructed, we\nwill combine them together by minimizing\nJl =\nn\ni=1\nw\u00e2\u02c6\u2014T\ni xi \u00e2\u02c6\u2019 qi\n2\n, (10)\nwhich stands for the sum of the prediction errors for all the\nlocal predictors. Combining Eq.(10) with Eq.(6), we can get\nJl =\nn\ni=1\nw\u00e2\u02c6\u2014T\ni xi \u00e2\u02c6\u2019 qi\n2\n=\nn\ni=1\nxT\ni Xi XT\ni Xi + \u00ce\u00bbiniIi\n\u00e2\u02c6\u20191\nqi \u00e2\u02c6\u2019 qi\n2\n= Pq \u00e2\u02c6\u2019 q 2\n, (11)\nwhere q = [q1, q2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , qn]T\n, and the P is an n \u00c3\u2014 n matrix\nconstructing in the following way. Let\n\u00ce\u00b1i\n= xT\ni Xi XT\ni Xi + \u00ce\u00bbiniIi\n\u00e2\u02c6\u20191\n,\nthen\nPij =\n\u00ce\u00b1i\nj, if xj \u00e2\u02c6\u02c6 Ni\n0, otherwise\n, (12)\nwhere Pij is the (i, j)-th entry of P, and \u00ce\u00b1i\nj represents the\nj-th entry of \u00ce\u00b1i\n.\nTill now we can write the criterion of clustering by \ncombining locally regularized linear label predictors Jl in an\nexplicit mathematical form, and we can minimize it directly\nusing some standard optimization techniques. However, the\nresults may not be good enough since we only exploit the\nlocal informations of the dataset. In the next subsection, we\nwill introduce a global regularization criterion and combine\nit with Jl, which aims to find a good clustering result in a\nlocal-global way.\n2.3 Global Regularization\nIn data clustering, we usually require that the cluster \nassignments of the data points should be sufficiently smooth\nwith respect to the underlying data manifold, which implies\n(1) the nearby points tend to have the same cluster \nassignments; (2) the points on the same structure (e.g. \nsubmanifold or cluster) tend to have the same cluster assignments\n[31].\nWithout the loss of generality, we assume that the data\npoints reside (roughly) on a low-dimensional manifold M2\n,\nand q is the cluster assignment function defined on M, i.e.\n2\nWe believe that the text data are also sampled from some\nlow dimensional manifold, since it is impossible for them to\nfor \u00e2\u02c6\u20acx \u00e2\u02c6\u02c6 M, q(x) returns the cluster membership of x. The\nsmoothness of q over M can be calculated by the following\nDirichlet integral [2]\nD[q] =\n1\n2 M\nq(x) 2\ndM, (13)\nwhere the gradient q is a vector in the tangent space T Mx,\nand the integral is taken with respect to the standard \nmeasure on M. If we restrict the scale of q by q, q M = 1\n(where \u00c2\u00b7, \u00c2\u00b7 M is the inner product induced on M), then\nit turns out that finding the smoothest function \nminimizing D[q] reduces to finding the eigenfunctions of the Laplace\nBeltrami operator L, which is defined as\nLq \u00e2\u02c6\u2019div q, (14)\nwhere div is the divergence of a vector field.\nGenerally, the graph can be viewed as the discretized form\nof manifold. We can model the dataset as an weighted \nundirected graph as in spectral clustering [22], where the graph\nnodes are just the data points, and the weights on the edges\nrepresent the similarities between pairwise points. Then it\ncan be shown that minimizing Eq.(13) corresponds to \nminimizing\nJg = qT\nLq =\nn\ni=1\n(qi \u00e2\u02c6\u2019 qj)2\nwij, (15)\nwhere q = [q1, q2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , qn]T\nwith qi = q(xi), L is the graph\nLaplacian with its (i, j)-th entry\nLij =\n\u00ef\u00a3\u00b1\n\u00ef\u00a3\u00b2\n\u00ef\u00a3\u00b3\ndi \u00e2\u02c6\u2019 wii, if i = j\n\u00e2\u02c6\u2019wij, if xi and xj are adjacent\n0, otherwise,\n(16)\nwhere di = j wij is the degree of xi, wij is the similarity\nbetween xi and xj. If xi and xj are adjacent3\n, wij is usually\ncomputed in the following way\nwij = e\n\u00e2\u02c6\u2019\nxi\u00e2\u02c6\u2019xj\n2\n2\u00cf\u01922 , (17)\nwhere \u00cf\u0192 is a dataset dependent parameter. It is proved that\nunder certain conditions, such a form of wij to determine\nthe weights on graph edges leads to the convergence of graph\nLaplacian to the Laplace Beltrami operator [3][18].\nIn summary, using Eq.(15) with exponential weights can\neffectively measure the smoothness of the data assignments\nwith respect to the intrinsic data manifold. Thus we adopt\nit as a global regularizer to punish the smoothness of the\npredicted data assignments.\n2.4 Clustering with Local and Global\nRegularization\nCombining the contents we have introduced in section 2.2\nand section 2.3 we can derive the clustering criterion is\nminq J = Jl + \u00ce\u00bbJg = Pq \u00e2\u02c6\u2019 q 2\n+ \u00ce\u00bbqT\nLq\ns.t. qi \u00e2\u02c6\u02c6 {\u00e2\u02c6\u20191, +1}, (18)\nwhere P is defined as in Eq.(12), and \u00ce\u00bb is a regularization\nparameter to trade off Jl and Jg. However, the discrete\nfill in the whole high-dimensional sample space. And it has\nbeen shown that the manifold based methods can achieve\ngood results on text classification tasks [31].\n3\nIn this paper, we define xi and xj to be adjacent if xi \u00e2\u02c6\u02c6\nN(xj) or xj \u00e2\u02c6\u02c6 N(xi).\nconstraint of pi makes the problem an NP hard integer \nprogramming problem. A natural way for making the problem\nsolvable is to remove the constraint and relax qi to be \ncontinuous, then the objective that we aims to minimize becomes\nJ = Pq \u00e2\u02c6\u2019 q 2\n+ \u00ce\u00bbqT\nLq\n= qT\n(P \u00e2\u02c6\u2019 I)T\n(P \u00e2\u02c6\u2019 I)q + \u00ce\u00bbqT\nLq\n= qT\n(P \u00e2\u02c6\u2019 I)T\n(P \u00e2\u02c6\u2019 I) + \u00ce\u00bbL q, (19)\nand we further add a constraint qT\nq = 1 to restrict the scale\nof q. Then our objective becomes\nminq J = qT\n(P \u00e2\u02c6\u2019 I)T\n(P \u00e2\u02c6\u2019 I) + \u00ce\u00bbL q\ns.t. qT\nq = 1 (20)\nUsing the Lagrangian method, we can derive that the \noptimal solution q corresponds to the smallest eigenvector of\nthe matrix M = (P \u00e2\u02c6\u2019 I)T\n(P \u00e2\u02c6\u2019 I) + \u00ce\u00bbL, and the cluster \nassignment of xi can be determined by the sign of qi, i.e. xi\nwill be classified as class one if qi > 0, otherwise it will be\nclassified as class 2.\n2.5 Multi-Class CLGR\nIn the above we have introduced the basic framework of\nClustering with Local and Global Regularization (CLGR) for\nthe two-class clustering problem, and we will extending it\nto multi-class clustering in this subsection.\nFirst we assume that all the documents belong to C classes\nindexed by L = {1, 2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , C}. qc\nis the classification \nfunction for class c (1 c C), such that qc\n(xi) returns the\nconfidence that xi belongs to class c. Our goal is to obtain\nthe value of qc\n(xi) (1 c C, 1 i n), and the cluster\nassignment of xi can be determined by {qc\n(xi)}C\nc=1 using\nsome proper discretization methods that we will introduce\nlater.\nTherefore, in this multi-class case, for each document xi (1\ni n), we will construct C locally linear regularized label\npredictors whose normal vectors are\nwc\u00e2\u02c6\u2014\ni = Xi XT\ni Xi + \u00ce\u00bbiniIi\n\u00e2\u02c6\u20191\nqc\ni (1 c C), (21)\nwhere Xi = [xi1, xi2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , xini ] with xik being the k-th \nneighbor of xi, and qc\ni = [qc\ni1, qc\ni2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , qc\nini\n]T\nwith qc\nik = qc\n(xik).\nThen (wc\u00e2\u02c6\u2014\ni )T\nxi returns the predicted confidence of xi \nbelonging to class c. Hence the local prediction error for class\nc can be defined as\nJ c\nl =\nn\ni=1\n(wc\u00e2\u02c6\u2014\ni )\nT\nxi \u00e2\u02c6\u2019 qc\ni\n2\n, (22)\nAnd the total local prediction error becomes\nJl =\nC\nc=1\nJ c\nl =\nC\nc=1\nn\ni=1\n(wc\u00e2\u02c6\u2014\ni )\nT\nxi \u00e2\u02c6\u2019 qc\ni\n2\n. (23)\nAs in Eq.(11), we can define an n\u00c3\u2014n matrix P (see Eq.(12))\nand rewrite Jl as\nJl =\nC\nc=1\nJ c\nl =\nC\nc=1\nPqc\n\u00e2\u02c6\u2019 qc 2\n. (24)\nSimilarly we can define the global smoothness regularizer\nin multi-class case as\nJg =\nC\nc=1\nn\ni=1\n(qc\ni \u00e2\u02c6\u2019 qc\nj )2\nwij =\nC\nc=1\n(qc\n)T\nLqc\n. (25)\nThen the criterion to be minimized for CLGR in multi-class\ncase becomes\nJ = Jl + \u00ce\u00bbJg\n=\nC\nc=1\nPqc\n\u00e2\u02c6\u2019 qc 2\n+ \u00ce\u00bb(qc\n)T\nLqc\n=\nC\nc=1\n(qc\n)T\n(P \u00e2\u02c6\u2019 I)T\n(P \u00e2\u02c6\u2019 I) + \u00ce\u00bbL qc\n= trace QT\n(P \u00e2\u02c6\u2019 I)T\n(P \u00e2\u02c6\u2019 I) + \u00ce\u00bbL Q , (26)\nwhere Q = [q1\n, q2\n, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , qc\n] is an n \u00c3\u2014 c matrix, and trace(\u00c2\u00b7)\nreturns the trace of a matrix. The same as in Eq.(20), we\nalso add the constraint that QT\nQ = I to restrict the scale\nof Q. Then our optimization problem becomes\nminQ J = trace QT\n(P \u00e2\u02c6\u2019 I)T\n(P \u00e2\u02c6\u2019 I) + \u00ce\u00bbL Q\ns.t. QT\nQ = I, (27)\nFrom the Ky Fan theorem [28], we know the optimal solution\nof the above problem is\nQ\u00e2\u02c6\u2014\n= [q\u00e2\u02c6\u2014\n1, q\u00e2\u02c6\u2014\n2, \u00c2\u00b7 \u00c2\u00b7 \u00c2\u00b7 , q\u00e2\u02c6\u2014\nC ]R, (28)\nwhere q\u00e2\u02c6\u2014\nk (1 k C) is the eigenvector corresponds to the\nk-th smallest eigenvalue of matrix (P \u00e2\u02c6\u2019 I)T\n(P \u00e2\u02c6\u2019 I) + \u00ce\u00bbL,\nand R is an arbitrary C \u00c3\u2014 C matrix. Since the values of the\nentries in Q\u00e2\u02c6\u2014\nis continuous, we need to further discretize Q\u00e2\u02c6\u2014\nto get the cluster assignments of all the data points. There\nare mainly two approaches to achieve this goal:\n1. As in [20], we can treat the i-th row of Q as the \nembedding of xi in a C-dimensional space, and apply some\ntraditional clustering methods like kmeans to \nclustering these embeddings into C clusters.\n2. Since the optimal Q\u00e2\u02c6\u2014\nis not unique (because of the\nexistence of an arbitrary matrix R), we can pursue an\noptimal R that will rotate Q\u00e2\u02c6\u2014\nto an indication matrix4\n.\nThe detailed algorithm can be referred to [26].\nThe detailed algorithm procedure for CLGR is summarized\nin table 1.\n3. EXPERIMENTS\nIn this section, experiments are conducted to empirically\ncompare the clustering results of CLGR with other 8 \nrepresentitive document clustering algorithms on 5 datasets.\nFirst we will introduce the basic informations of those datasets.\n3.1 Datasets\nWe use a variety of datasets, most of which are frequently\nused in the information retrieval research. Table 2 \nsummarizes the characteristics of the datasets.\n4\nHere an indication matrix T is a n\u00c3\u2014c matrix with its (i, \nj)th entry Tij \u00e2\u02c6\u02c6 {0, 1} such that for each row of Q\u00e2\u02c6\u2014\nthere is\nonly one 1. Then the xi can be assigned to the j-th cluster\nsuch that j = argjQ\u00e2\u02c6\u2014\nij = 1.\nTable 1: Clustering with Local and Global \nRegularization (CLGR)\nInput:\n1. Dataset X = {xi}n\ni=1;\n2. Number of clusters C;\n3. Size of the neighborhood K;\n4. Local regularization parameters {\u00ce\u00bbi}n\ni=1;\n5. Global regularization parameter \u00ce\u00bb;\nOutput:\nThe cluster membership of each data point.\nProcedure:\n1. Construct the K nearest neighborhoods for each\ndata point;\n2. Construct the matrix P using Eq.(12);\n3. Construct the Laplacian matrix L using Eq.(16);\n4. Construct the matrix M = (P \u00e2\u02c6\u2019 I)T\n(P \u00e2\u02c6\u2019 I) + \u00ce\u00bbL;\n5. Do eigenvalue decomposition on M, and construct\nthe matrix Q\u00e2\u02c6\u2014\naccording to Eq.(28);\n6. Output the cluster assignments of each data point\nby properly discretize Q\u00e2\u02c6\u2014\n.\nTable 2: Descriptions of the document datasets\nDatasets Number of documents Number of classes\nCSTR 476 4\nWebKB4 4199 4\nReuters 2900 10\nWebACE 2340 20\nNewsgroup4 3970 4\nCSTR. This is the dataset of the abstracts of technical\nreports published in the Department of Computer Science\nat a university. The dataset contained 476 abstracts, which\nwere divided into four research areas: Natural Language\nProcessing(NLP), Robotics/Vision, Systems, and Theory.\nWebKB. The WebKB dataset contains webpages \ngathered from university computer science departments. There\nare about 8280 documents and they are divided into 7 \ncategories: student, faculty, staff, course, project, department\nand other. The raw text is about 27MB. Among these\n7 categories, student, faculty, course and project are four\nmost populous entity-representing categories. The \nassociated subset is typically called WebKB4.\nReuters. The Reuters-21578 Text Categorization Test\ncollection contains documents collected from the Reuters\nnewswire in 1987. It is a standard text categorization \nbenchmark and contains 135 categories. In our experiments, we\nuse a subset of the data collection which includes the 10\nmost frequent categories among the 135 topics and we call\nit Reuters-top 10.\nWebACE. The WebACE dataset was from WebACE project\nand has been used for document clustering [17][5]. The \nWebACE dataset contains 2340 documents consisting news \narticles from Reuters new service via the Web in October 1997.\nThese documents are divided into 20 classes.\nNews4. The News4 dataset used in our experiments are\nselected from the famous 20-newsgroups dataset5\n. The topic\nrec containing autos, motorcycles, baseball and hockey was\nselected from the version 20news-18828. The News4 dataset\ncontains 3970 document vectors.\n5\nhttp://people.csail.mit.edu/jrennie/20Newsgroups/\nTo pre-process the datasets, we remove the stop words\nusing a standard stop list, all HTML tags are skipped and all\nheader fields except subject and organization of the posted\narticles are ignored. In all our experiments, we first select\nthe top 1000 words by mutual information with class labels.\n3.2 Evaluation Metrics\nIn the experiments, we set the number of clusters equal\nto the true number of classes C for all the clustering \nalgorithms. To evaluate their performance, we compare the\nclusters generated by these algorithms with the true classes\nby computing the following two performance measures.\nClustering Accuracy (Acc). The first performance \nmeasure is the Clustering Accuracy, which discovers the \none-toone relationship between clusters and classes and measures\nthe extent to which each cluster contained data points from\nthe corresponding class. It sums up the whole matching \ndegree between all pair class-clusters. Clustering accuracy can\nbe computed as:\nAcc =\n1\nN\nmax\n\u00ef\u00a3\u00ab\n\u00ef\u00a3\u00ad\nCk,Lm\nT(Ck, Lm)\n\u00ef\u00a3\u00b6\n\u00ef\u00a3\u00b8 , (29)\nwhere Ck denotes the k-th cluster in the final results, and Lm\nis the true m-th class. T(Ck, Lm) is the number of entities\nwhich belong to class m are assigned to cluster k. \nAccuracy computes the maximum sum of T(Ck, Lm) for all pairs\nof clusters and classes, and these pairs have no overlaps.\nThe greater clustering accuracy means the better clustering\nperformance.\nNormalized Mutual Information (NMI). Another \nevaluation metric we adopt here is the Normalized Mutual \nInformation NMI [23], which is widely used for determining\nthe quality of clusters. For two random variable X and Y,\nthe NMI is defined as:\nNMI(X, Y) =\nI(X, Y)\nH(X)H(Y)\n, (30)\nwhere I(X, Y) is the mutual information between X and\nY, while H(X) and H(Y) are the entropies of X and Y\nrespectively. One can see that NMI(X, X) = 1, which is the\nmaximal possible value of NMI. Given a clustering result,\nthe NMI in Eq.(30) is estimated as\nNMI =\nC\nk=1\nC\nm=1 nk,mlog\nn\u00c2\u00b7nk,m\nnk \u00cb\u2020nm\nC\nk=1 nklog nk\nn\nC\nm=1 \u00cb\u2020nmlog \u00cb\u2020nm\nn\n, (31)\nwhere nk denotes the number of data contained in the cluster\nCk (1 k C), \u00cb\u2020nm is the number of data belonging to the\nm-th class (1 m C), and nk,m denotes the number of\ndata that are in the intersection between the cluster Ck and\nthe m-th class. The value calculated in Eq.(31) is used as\na performance measure for the given clustering result. The\nlarger this value, the better the clustering performance.\n3.3 Comparisons\nWe have conducted comprehensive performance \nevaluations by testing our method and comparing it with 8 other\nrepresentative data clustering methods using the same data\ncorpora. The algorithms that we evaluated are listed below.\n1. Traditional k-means (KM).\n2. Spherical k-means (SKM). The implementation is based\non [9].\n3. Gaussian Mixture Model (GMM). The implementation\nis based on [16].\n4. Spectral Clustering with Normalized Cuts (Ncut). The\nimplementation is based on [26], and the variance of\nthe Gaussian similarity is determined by Local Scaling\n[30]. Note that the criterion that Ncut aims to \nminimize is just the global regularizer in our CLGR \nalgorithm except that Ncut used the normalized Laplacian.\n5. Clustering using Pure Local Regularization (CPLR).\nIn this method we just minimize Jl (defined in Eq.(24)),\nand the clustering results can be obtained by doing\neigenvalue decomposition on matrix (I \u00e2\u02c6\u2019 P)T\n(I \u00e2\u02c6\u2019 P)\nwith some proper discretization methods.\n6. Adaptive Subspace Iteration (ASI). The \nimplementation is based on [14].\n7. Nonnegative Matrix Factorization (NMF). The \nimplementation is based on [27].\n8. Tri-Factorization Nonnegative Matrix Factorization\n(TNMF) [12]. The implementation is based on [15].\nFor computational efficiency, in the implementation of\nCPLR and our CLGR algorithm, we have set all the local\nregularization parameters {\u00ce\u00bbi}n\ni=1 to be identical, which is\nset by grid search from {0.1, 1, 10}. The size of the k-nearest\nneighborhoods is set by grid search from {20, 40, 80}. For\nthe CLGR method, its global regularization parameter is\nset by grid search from {0.1, 1, 10}. When constructing the\nglobal regularizer, we have adopted the local scaling method\n[30] to construct the Laplacian matrix. The final \ndiscretization method adopted in these two methods is the same as\nin [26], since our experiments show that using such method\ncan achieve better results than using kmeans based methods\nas in [20].\n3.4 Experimental Results\nThe clustering accuracies comparison results are shown in\ntable 3, and the normalized mutual information comparison\nresults are summarized in table 4. From the two tables we\nmainly observe that:\n1. Our CLGR method outperforms all other document\nclustering methods in most of the datasets;\n2. For document clustering, the Spherical k-means method\nusually outperforms the traditional k-means clustering\nmethod, and the GMM method can achieve \ncompetitive results compared to the Spherical k-means method;\n3. The results achieved from the k-means and GMM type\nalgorithms are usually worse than the results achieved\nfrom Spectral Clustering. Since Spectral Clustering can\nbe viewed as a weighted version of kernel k-means, it\ncan obtain good results the data clusters are arbitrarily\nshaped. This corroborates that the documents vectors\nare not regularly distributed (spherical or elliptical).\n4. The experimental comparisons empirically verify the\nequivalence between NMF and Spectral Clustering, which\nTable 3: Clustering accuracies of the various \nmethods\nCSTR WebKB4 Reuters WebACE News4\nKM 0.4256 0.3888 0.4448 0.4001 0.3527\nSKM 0.4690 0.4318 0.5025 0.4458 0.3912\nGMM 0.4487 0.4271 0.4897 0.4521 0.3844\nNMF 0.5713 0.4418 0.4947 0.4761 0.4213\nNcut 0.5435 0.4521 0.4896 0.4513 0.4189\nASI 0.5621 0.4752 0.5235 0.4823 0.4335\nTNMF 0.6040 0.4832 0.5541 0.5102 0.4613\nCPLR 0.5974 0.5020 0.4832 0.5213 0.4890\nCLGR 0.6235 0.5228 0.5341 0.5376 0.5102\nTable 4: Normalized mutual information results of\nthe various methods\nCSTR WebKB4 Reuters WebACE News4\nKM 0.3675 0.3023 0.4012 0.3864 0.3318\nSKM 0.4027 0.4155 0.4587 0.4003 0.4085\nGMM 0.4034 0.4093 0.4356 0.4209 0.3994\nNMF 0.5235 0.4517 0.4402 0.4359 0.4130\nNcut 0.4833 0.4497 0.4392 0.4289 0.4231\nASI 0.5008 0.4833 0.4769 0.4817 0.4503\nTNMF 0.5724 0.5011 0.5132 0.5328 0.4749\nCPLR 0.5695 0.5231 0.4402 0.5543 0.4690\nCLGR 0.6012 0.5434 0.4935 0.5390 0.4908\nhas been proved theoretically in [10]. It can be \nobserved from the tables that NMF and Spectral \nClustering usually lead to similar clustering results.\n5. The co-clustering based methods (TNMF and ASI)\ncan usually achieve better results than traditional purely\ndocument vector based methods. Since these methods\nperform an implicit feature selection at each iteration,\nprovide an adaptive metric for measuring the \nneighborhood, and thus tend to yield better clustering results.\n6. The results achieved from CPLR are usually better\nthan the results achieved from Spectral Clustering, which\nsupports Vapnik\"s theory [24] that sometimes local\nlearning algorithms can obtain better results than global\nlearning algorithms.\nBesides the above comparison experiments, we also test\nthe parameter sensibility of our method. There are mainly\ntwo sets of parameters in our CLGR algorithm, the local\nand global regularization parameters ({\u00ce\u00bbi}n\ni=1 and \u00ce\u00bb, as we\nhave said in section 3.3, we have set all \u00ce\u00bbi\"s to be identical to\n\u00ce\u00bb\u00e2\u02c6\u2014\nin our experiments), and the size of the neighborhoods.\nTherefore we have also done two sets of experiments:\n1. Fixing the size of the neighborhoods, and testing the\nclustering performance with varying \u00ce\u00bb\u00e2\u02c6\u2014\nand \u00ce\u00bb. In this\nset of experiments, we find that our CLGR algorithm\ncan achieve good results when the two regularization\nparameters are neither too large nor too small. \nTypically our method can achieve good results when \u00ce\u00bb\u00e2\u02c6\u2014\nand \u00ce\u00bb are around 0.1. Figure 1 shows us such a \ntesting example on the WebACE dataset.\n2. Fixing the local and global regularization parameters,\nand testing the clustering performance with different\n\u00e2\u02c6\u20195\n\u00e2\u02c6\u20194.5\n\u00e2\u02c6\u20194\n\u00e2\u02c6\u20193.5\n\u00e2\u02c6\u20193\n\u00e2\u02c6\u20195\n\u00e2\u02c6\u20194.5\n\u00e2\u02c6\u20194\n\u00e2\u02c6\u20193.5\n\u00e2\u02c6\u20193\n0.35\n0.4\n0.45\n0.5\n0.55\nlocal regularization para\n(log\n2\nvalue)\nglobal regularization para\n(log\n2\nvalue)\nclusteringaccuracy\nFigure 1: Parameter sensibility testing results on\nthe WebACE dataset with the neighborhood size\nfixed to 20, and the x-axis and y-axis represents the\nlog2 value of \u00ce\u00bb\u00e2\u02c6\u2014\nand \u00ce\u00bb.\nsizes of neighborhoods. In this set of experiments,\nwe find that the neighborhood with a too large or\ntoo small size will all deteriorate the final clustering\nresults. This can be easily understood since when\nthe neighborhood size is very small, then the data\npoints used for training the local classifiers may not\nbe sufficient; when the neighborhood size is very large,\nthe trained classifiers will tend to be global and \ncannot capture the typical local characteristics. Figure 2\nshows us a testing example on the WebACE dataset.\nTherefore, we can see that our CLGR algorithm (1) can\nachieve satisfactory results and (2) is not very sensitive to\nthe choice of parameters, which makes it practical in real\nworld applications.\n4. CONCLUSIONS AND FUTURE WORKS\nIn this paper, we derived a new clustering algorithm called\nclustering with local and global regularization. Our method\npreserves the merit of local learning algorithms and spectral\nclustering. Our experiments show that the proposed \nalgorithm outperforms most of the state of the art algorithms on\nmany benchmark datasets. In the future, we will focus on\nthe parameter selection and acceleration issues of the CLGR\nalgorithm.\n5. REFERENCES\n[1] L. Baker and A. McCallum. Distributional Clustering\nof Words for Text Classification. In Proceedings of the\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, 1998.\n[2] M. Belkin and P. Niyogi. Laplacian Eigenmaps for\nDimensionality Reduction and Data Representation.\nNeural Computation, 15 (6):1373-1396. June 2003.\n[3] M. Belkin and P. Niyogi. Towards a Theoretical\nFoundation for Laplacian-Based Manifold Methods. In\nProceedings of the 18th Conference on Learning\nTheory (COLT). 2005.\n10 20 30 40 50 60 70 80 90 100\n0.35\n0.4\n0.45\n0.5\n0.55\nsize of the neighborhood\nclusteringaccuracy\nFigure 2: Parameter sensibility testing results on\nthe WebACE dataset with the regularization \nparameters being fixed to 0.1, and the neighborhood size\nvaring from 10 to 100.\n[4] M. Belkin, P. Niyogi and V. Sindhwani. Manifold\nRegularization: a Geometric Framework for Learning\nfrom Examples. Journal of Machine Learning\nResearch 7, 1-48, 2006.\n[5] D. Boley. Principal Direction Divisive Partitioning.\nData mining and knowledge discovery, 2:325-344, 1998.\n[6] L. Bottou and V. Vapnik. Local learning algorithms.\nNeural Computation, 4:888-900, 1992.\n[7] P. K. Chan, D. F. Schlag and J. Y. Zien. Spectral\nK-way Ratio-Cut Partitioning and Clustering. IEEE\nTrans. Computer-Aided Design, 13:1088-1096, Sep.\n1994.\n[8] D. R. Cutting, D. R. Karger, J. O. Pederson and J.\nW. Tukey. Scatter/Gather: A Cluster-Based Approach\nto Browsing Large Document Collections. In\nProceedings of the International ACM SIGIR\nConference on Research and Development in\nInformation Retrieval, 1992.\n[9] I. S. Dhillon and D. S. Modha. Concept\nDecompositions for Large Sparse Text Data using\nClustering. Machine Learning, vol. 42(1), pages\n143-175, January 2001.\n[10] C. Ding, X. He, and H. Simon. On the equivalence of\nnonnegative matrix factorization and spectral\nclustering. In Proceedings of the SIAM Data Mining\nConference, 2005.\n[11] C. Ding, X. He, H. Zha, M. Gu, and H. D. Simon. A\nmin-max cut algorithm for graph partitioning and\ndata clustering. In Proc. of the 1st International\nConference on Data Mining (ICDM), pages 107-114,\n2001.\n[12] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal\nNonnegative Matrix Tri-Factorizations for Clustering.\nIn Proceedings of the Twelfth ACM SIGKDD\nInternational Conference on Knowledge Discovery and\nData Mining, 2006.\n[13] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern\nClassification. John Wiley & Sons, Inc., 2001.\n[14] T. Li, S. Ma, and M. Ogihara. Document Clustering\nvia Adaptive Subspace Iteration. In Proceedings of the\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, 2004.\n[15] T. Li and C. Ding. The Relationships Among Various\nNonnegative Matrix Factorization Methods for\nClustering. In Proceedings of the 6th International\nConference on Data Mining (ICDM). 2006.\n[16] X. Liu and Y. Gong. Document Clustering with\nCluster Refinement and Model Selection Capabilities.\nIn Proc. of the International ACM SIGIR Conference\non Research and Development in Information\nRetrieval, 2002.\n[17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G.\nKarypis, V. Kumar, B. Mobasher, and J. Moore.\nWebACE: A Web Agent for Document Categorization\nand Exploration. In Proceedings of the 2nd\nInternational Conference on Autonomous Agents\n(Agents98). ACM Press, 1998.\n[18] M. Hein, J. Y. Audibert, and U. von Luxburg. From\nGraphs to Manifolds - Weak and Strong Pointwise\nConsistency of Graph Laplacians. In Proceedings of\nthe 18th Conference on Learning Theory (COLT),\n470-485. 2005.\n[19] J. He, M. Lan, C.-L. Tan, S.-Y. Sung, and H.-B. Low.\nInitialization of Cluster Refinement Algorithms: A\nReview and Comparative Study. In Proc. of Inter.\nJoint Conference on Neural Networks, 2004.\n[20] A. Y. Ng, M. I. Jordan, Y. Weiss. On Spectral\nClustering: Analysis and an algorithm. In Advances in\nNeural Information Processing Systems 14. 2002.\n[21] B. Sch\u00c2\u00a8olkopf and A. Smola. Learning with Kernels.\nThe MIT Press. Cambridge, Massachusetts. 2002.\n[22] J. Shi and J. Malik. Normalized Cuts and Image\nSegmentation. IEEE Trans. on Pattern Analysis and\nMachine Intelligence, 22(8):888-905, 2000.\n[23] A. Strehl and J. Ghosh. Cluster Ensembles - A\nKnowledge Reuse Framework for Combining Multiple\nPartitions. Journal of Machine Learning Research,\n3:583-617, 2002.\n[24] V. N. Vapnik. The Nature of Statistical Learning\nTheory. Berlin: Springer-Verlag, 1995.\n[25] Wu, M. and Sch\u00c2\u00a8olkopf, B. A Local Learning Approach\nfor Clustering. In Advances in Neural Information\nProcessing Systems 18. 2006.\n[26] S. X. Yu, J. Shi. Multiclass Spectral Clustering. In\nProceedings of the International Conference on\nComputer Vision, 2003.\n[27] W. Xu, X. Liu and Y. Gong. Document Clustering\nBased On Non-Negative Matrix Factorization. In\nProceedings of the International ACM SIGIR\nConference on Research and Development in\nInformation Retrieval, 2003.\n[28] H. Zha, X. He, C. Ding, M. Gu and H. Simon. Spectral\nRelaxation for K-means Clustering. In NIPS 14. 2001.\n[29] T. Zhang and F. J. Oles. Text Categorization Based\non Regularized Linear Classification Methods. Journal\nof Information Retrieval, 4:5-31, 2001.\n[30] L. Zelnik-Manor and P. Perona. Self-Tuning Spectral\nClustering. In NIPS 17. 2005.\n[31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B.\nSch\u00c2\u00a8olkopf. Learning with Local and Global\nConsistency. NIPS 17, 2005.\n": ["document clustering", "regularization", "global regularization", "cluster hierarchy", "spectrum", "specified search", "hierarchical method", "partitioning method", "label prediction", "function estimation", "manifold", "document cluster", ""]}