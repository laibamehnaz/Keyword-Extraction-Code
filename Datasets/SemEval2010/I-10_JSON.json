{"SMILE: Sound Multi-agent Incremental LEarning ;-)\u00e2\u02c6\u2014\nGauvain Bourgne\nLAMSADE, UMR 7024 CNRS,\nUniversity Paris-Dauphine,\n75775 Paris Cedex 16\nAmal El Fallah\nSegrouchni\nLIP6, UMR 7606 CNRS,\nUniversity Paris 6, 104, Av. du\npr\u00c2\u00b4esident Kennedy, 75116\nParis\nHenry Soldano\nLIPN, UMR 7030 CNRS,\nUniversity Paris-Nord, 99 Av.\nJ-B Clement, 93430,\nVilletaneuse\nABSTRACT\nThis article deals with the problem of collaborative \nlearning in a multi-agent system. Here each agent can update\nincrementally its beliefs B (the concept representation) so\nthat it is in a way kept consistent with the whole set of\ninformation K (the examples) that he has received from\nthe environment or other agents. We extend this notion\nof consistency (or soundness) to the whole MAS and \ndiscuss how to obtain that, at any moment, a same consistent\nconcept representation is present in each agent. The \ncorresponding protocol is applied to supervised concept learning.\nThe resulting method SMILE (standing for Sound \nMultiagent Incremental LEarning) is described and experimented\nhere. Surprisingly some difficult boolean formulas are \nbetter learned, given the same learning set, by a Multi agent\nsystem than by a single agent.\nCategories and Subject Descriptors\nI.2.6 [Artificial Intelligence]: Learning-Concept \nlearning; I.2.11 [Artificial Intelligence]: Distributed Artificial\nIntelligence-Multiagent system\nGeneral Terms\nExperimentation, Algorithms, Measurement, Performance\n1. INTRODUCTION\nThis article deals with the problem of collaborative \nconcept learning in a multi-agent system. [6] introduces a \ncharacterisation of learning in multi-agent system according to\nthe level of awareness of the agents. At level 1, agents learn\n\u00e2\u02c6\u2014The primary author of this paper is a student.\nin the system without taking into account the presence of\nother agents, except through the modification brought upon\nthe environment by their action. Level 2 implies direct \ninteraction between the agents as they can exchange messages\nto improve their learning. Level 3 would require agents to\ntake into account the competencies of other agents, and be\nable to learn from observation of the other agents\" behaviour\n(while considering them as independant entities and not \nindetermined part of the environment as in level 1). We focus\nin this paper on level 2, studying direct interaction between\nagents involved in a learning process.\nEach agent is assumed to be able to learn incrementally from\nthe data he receives, meaning that each agent can update\nhis belief set B to keep it consistent with the whole set of\ninformation K that he has received from the environment\nor from other agents. In such a case, we will say that he is\na-consistent. Here, the belief set B represents hypothetical\nknowledge that can therefore be revised, whereas the set of\ninformation K represents certain knowledge, consisting of\nnon revisable observations and facts. Moreover, we suppose\nthat at least a part Bc of the beliefs of each agent is \ncommon to all agents and must stay that way. Therefore, an\nupdate of this common set Bc by agent r must provoke an\nupdate of Bc for the whole community of agents. It leads\nus to define what is the mas-consistency of an agent with\nrespect to the community. The update process of the \ncommunity beliefs when one of its members gets new information\ncan then be defined as the consistency maintenance process\nensuring that every agent in the community will stay \nmasconsistent. This mas-consistency maintenance process of an\nagent getting new information gives him the role of a learner\nand implies communication with other agents acting as \ncritics. However, agents are not specialised and can in turn be\nlearners or critics, none of them being kept to a specific role.\nPieces of information are distributed among the agents, but\ncan be redundant. There is no central memory.\nThe work described here has its origin in a former work \nconcerning learning in an intentional multi-agent system using\na BDI formalism [6]. In that work, agents had plans, each\nof them being associated with a context defining in which\nconditions it can be triggered. Plans (each of them having\nits own context) were common to the whole set of agents\nin the community. Agents had to adapt their plan contexts\ndepending on the failure or success of executed plans, using\na learning mechanism and asking other agents for examples\n(plans successes or failures). However this work lacked a\ncollective learning protocol enabling a real autonomy of the\nmulti-agent system. The study of such a protocol is the \nobject of the present paper.\nIn section 2 we formally define the mas-consistency of an\nupdate mechanism for the whole MAS and we propose a\ngeneric update mechanism proved to be mas consistent. In\nsection 3 we describe SMILE, an incremental multi agent\nconcept learner applying our mas consistent update \nmechanism to collaborative concept learning. Section 4 describes\nvarious experiments on SMILE and discusses various issues\nincluding how the accuracy and the simplicity of the current\nhypothesis vary when comparing single agent learning and\nmas learning. In section 5 we briefly present some related\nworks and then conclude in section 6 by discussing further\ninvestigations on mas consistent learning.\n2. FORMAL MODEL\n2.1 Definitions and framework\nIn this section, we present a general formulation of \ncollective incremental learning in a cognitive multi agent system.\nWe represent a MAS as a set of agents r1, ..., rn. Each\nagent ri has a belief set Bi consisting of all the revisable\nknowledge he has. Part of these knowledges must be shared\nwith other agents. The part of Bi that is common to all\nagents is denoted as BC . This common part provokes a \ndependency between the agents. If an agent ri updates his\nbelief set Bi to Bi, changing in the process BC into BC , all\nother agents rk must then update their belief set Bk to Bk\nso that BC \u00e2\u0160\u2020 Bk.\nMoreover, each agent ri has stored some certain information\nKi. We suppose that some consistency property Cons(Bi, Ki)\ncan be verified by the agent itself between its beliefs Bi and\nits information Ki. As said before, Bi represents knowledge\nthat might be revised whereas Ki represents observed facts,\ntaken as being true, and which can possibly contradict Bi.\nDefinition 1. a-consistency of an agent\nAn agent ri is a-consistent iff Cons(Bi, Ki) is true.\nExample 1. Agent r1 has a set of plans which are in the\ncommon part BC of B1. Each plan P has a triggering \ncontext d(P) (which acts as a pre-condition) and a body. Some\npiece of information k could be plan P, triggered in \nsituation s, has failed in spite of s being an instance of d(P).\nIf this piece of information is added to K1, then agent r1 is\nnot a-consistent anymore: Cons(B1, K1 \u00e2\u02c6\u00aa k) is false.\nWe also want to define some notion of consistency for the\nwhole MAS depending on the belief and information sets\nof its constituting elements. We will first define the \nconsistency of an agent ri with respect to its belief set Bi and its\nown information set Ki together with all information sets\nK1...Kn from the other agents of the MAS. We will simply\ndo that by considering what would be the a-consistency of\nthe agent if he has the information of all the other agents.\nWe call this notion the mas-consistency:\nDefinition 2. mas-consistency of an agent\nAn agent ri is mas-consistent iff Cons(Bi, Ki \u00e2\u02c6\u00aa K) is true,\nwhere K = \u00e2\u02c6\u00aaj\u00e2\u02c6\u02c6{1,..,n}\u00e2\u02c6\u2019{i}Kj\n1\nis the set of all information\nfrom other agents of the MAS.\n1\nWe will note this \u00e2\u02c6\u00aa Kj when the context is similar.\nExample 2. Using the previous example, suppose that the\npiece of information k is included in the information K2 of\nagent r2. As long as the piece of information is not \ntransmitted to r1, and so added to K1 , r1 remains a-consistent.\nHowever, r1 is not mas-consistent as k is in the set K of all\ninformation of the MAS.\nThe global consistency of the MAS is then simply the\nmas-consistency of all its agents.\nDefinition 3. Consistency of a MAS\nA MAS r1,...,rn is consistent iff all its agents ri are \nmasconsistent.\nWe now define the required properties for a revision \nmechanism M updating an agent ri when it gets a piece of \ninformation k. In the following, we will suppose that:\n\u00e2\u20ac\u00a2 Update is always possible, that is, an agent can \nalways modify its belief set Bi in order to regain its\na-consistency. We will say that each agent is locally\nefficient.\n\u00e2\u20ac\u00a2 Considering two sets of information Cons(Bi, K1) and\nCons(Bi, K2), we also have Cons(Bi, K1 \u00e2\u02c6\u00aa K2). That\nis, a-consistency of the agents is additive.\n\u00e2\u20ac\u00a2 If a piece of information k concerning the common\nset BC is consistent with an agent, it is consistent\nwith all agents: for all pair of agents (ri,rj) such that\nCons(Bi, Ki) and Cons(Bj, Kj) are true, we have,\nfor all piece of information k: Cons(Bi, Ki \u00e2\u02c6\u00aa k) iff\nCons(Bj, Kj \u00e2\u02c6\u00aa k). In such a case, we will say that\nthe MAS is coherent.\nThis last condition simply means that the common belief\nset BC is independent of the possible differences between\nthe belief sets Bi of each agent ri. In the simplest case,\nB1 = ... = Bn = BC .\nM will also be viewed as an incremental learning \nmechanism and represented as an application changing Bi in Bi.\nIn the following, we shall note ri(Bi, Ki) for ri when it is\nuseful.\nDefinition 4. a-consistency of a revision\nAn update mechanism M is a-consistent iff for any agent ri\nand any piece of information k reaching ri, the a-consistency\nof this agent is preserved. In other words, iff:\nri(Bi, Ki) a-consistent \u00e2\u2021\u2019 ri(Bi, Ki) a-consistent,\nwhere Bi = M(Bi) and Ki = Ki \u00e2\u02c6\u00aa k is the set of all \ninformation from other agents of the MAS.\nIn the same way, we define the mas-consistency of a \nrevision mechanism as the a-consistency of this mechanism\nshould the agents dispose of all information in the MAS. In\nthe following, we shall note, if needed, ri(Bi, Ki, K) for the\nagent ri in MAS r1 . . . rn.\nDefinition 5. mas-consistency of a revision\nAn update mechanism Ms is mas-consistent iff for all agent\nri and all pieces of information k reaching ri, the \nmasconsistency of this agent is preserved. In other words, if:\nri(Bi, Ki, K) mas-consistent \u00e2\u2021\u2019 ri(Bi, Ki, K) mas-consistent,\nwhere Bi = Ms(Bi), Ki = Ki \u00e2\u02c6\u00aa k, and K = \u00e2\u02c6\u00aaKj is the set\nof all information from the MAS.\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 165\nAt last, when a mas-consistent mechanism is applied by\nan agent getting a new piece of information, a desirable \nsideeffect of the mechanism should be that all others agents \nremains mas-consistent after any modification of the common\npart BC , that is, the MAS itself should become consistent\nagain. This property is defined as follows:\nDefinition 6. Strong mas-consistency of a revision\nAn update mechanism Ms is strongly mas-consistent iff\n- Ms is mas-consistent, and\n- the application of Ms by an agent preserves the consistency\nof the MAS.\n2.2 A strongly mas-consistent update \nmechanism\nThe general idea is that, since information is distributed\namong all the agents of the MAS, there must be some \ninteraction between the learner agent and the other agents in\na strongly mas-consistent update mechanism Ms. In order\nto ensure its mas-consistency, Ms will be constituted of \nreiterated applications by the learner agent ri of an internal\na-consistent mechanism M, followed by some interactions\nbetween ri and the other agents, until ri regain its \nmasconsistency. We describe below such a mechanism, first with\na description of an interaction, then an iteration, and finally\na statement of the termination condition of the mechanism.\nThe mechanism is triggered by an agent ri upon receipt\nof a piece of information k disrupting the mas-consistency.\nWe shall note M(Bi) the belief set of the learner agent\nri after an update, BC the common part modified by ri,\nand Bj the belief set of another agent rj induced by the\nmodification of its common part BC in BC .\nAn interaction I(ri, rj) between the learner agent ri and\nanother agent rj, acting as critic is constituted of the \nfollowing steps:\n\u00e2\u20ac\u00a2 agent ri sends the update BC of the common part of\nits beliefs. Having applied its update mechanism, ri is\na-consistent.\n\u00e2\u20ac\u00a2 agent rj checks the modification Bj of its beliefs \ninduced by the update BC . If this modification preserve\nits a-consistency, rj adopts this modification.\n\u00e2\u20ac\u00a2 agent rj sends either an acceptation of BC or a denial\nalong with one (or more) piece(s) of information k\nsuch that Cons(Bj, k ) is false.\nAn iteration of Ms will then be composed of:\n\u00e2\u20ac\u00a2 the reception by the learner agent ri of a piece of\ninformation and the update M(Bi) restoring its \naconsistency\n\u00e2\u20ac\u00a2 a set of interactions I(ri, rj) (in which several critic\nagents can possibly participate). If at least one piece\nof information k is transmitted to ri, the addition of\nk will necessarily make ri a-inconsistent and a new\niteration will then occur.\nThis mechanism Ms ends when no agent can provide such\na piece of information k . When it is the case, the \nmasconsistency of the learner agent ri is restored.\nProposition 1. Let r1,...,rn be a consistent MAS in which\nagent ri receives a piece of information k breaking its \naconsistency, and M an a-consistent internal update \nmechanism. The update mechanism Ms described above is strongly\nmas-consistent.\nProof. The proof directly derives from the mechanism\ndescription. This mechanism ensures that each time an\nagent receives an event, its mas-consistency will be restored.\nAs the other agents all adopt the final update BC , they are\nall mas-consistent, and the MAS is consistent. Therefore\nMs is a strongly consistent update mechanism.\nIn the mechanism Ms described above, the learner agent\nis the only one that receives and memorizes information\nduring the mechanism execution. It ensures that Ms \nterminates. The pieces of information transmitted by other\nagents and memorized by the learner agent are redundant\nas they are already present in the MAS, more precisely in\nthe memory of the critic agents that transmitted them.\nNote that the mechanism Ms proposed here does not \nexplicitly indicate the order nor the scope of the interactions.\nWe will consider in the following that the modification \nproposal BC is sent sequentially to the different agents \n(synchronous mechanism). Moreover, the response of a critic\nagent will only contain one piece of information inconsistent\nwith the proposed modification. We will say that the \nresponse of the agent is minimal. This mechanism Ms, being\nsynchronous with minimal response, minimizes the amount\nof information transmitted by the agents. We will now \nillustrate it in the case of multi-agent concept learning.\n3. SOUNDMULTI-AGENTINCREMENTAL\nLEARNING\n3.1 The learning task\nWe experiment the mechanism proposed above in the case\nof incremental MAS concept learning. We consider here\na hypothesis language in which a hypothesis is a \ndisjunction of terms. Each term is a conjunction of atoms from a\nset A. An example is represented by a tag + or \u00e2\u02c6\u2019 and a\ndescription 2\ncomposed of a subset of atoms e \u00e2\u0160\u2020 A. A term\ncovers an example if its constituting atoms are included in\nthe example. A hypothesis covers an example if one of its\nterm covers it.\nThis representation will be used below for learning boolean\nformulae. Negative literals are here represented by \nadditional atoms, like not \u00e2\u02c6\u2019 a. The boolean formulae f =(a \u00e2\u02c6\u00a7\nb) \u00e2\u02c6\u00a8 (b \u00e2\u02c6\u00a7 \u00c2\u00acc) will then be written (a \u00e2\u02c6\u00a7 b) \u00e2\u02c6\u00a8 (b \u00e2\u02c6\u00a7 not \u00e2\u02c6\u2019 c). A\npositive example of f, like {not \u00e2\u02c6\u2019 a, b, not \u00e2\u02c6\u2019 c}, represents\na model for f.\n3.2 Incremental learning process\nThe learning process is an update mechanism that, given\na current hypothesis H, a memory E = E+\n\u00e2\u02c6\u00aa E\u00e2\u02c6\u2019\nfilled\nwith the previously received examples, and a new positive\nor negative example e, produces a new updated \nhypothesis. Before this update, the given hypothesis is complete,\nmeaning that it covers all positive examples of E+\n, and\n2\nWhen no confusion is possible, the word example will be\nused to refer to the pair (tag, description) as well as the\ndescription alone.\n166 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\ncoherent, meaning that it does not cover any negative \nexample of E\u00e2\u02c6\u2019\n. After the update, the new hypothesis must be\ncomplete and coherent with the new memory state E \u00e2\u02c6\u00aa {e}.\nWe describe below our single agent update mechanism, \ninspired from a previous work on incremental learning[7].\nIn the following, a hypothesis H for the target formula f is\na list of terms h, each of them being a conjunction of atoms.\nH is coherent if all terms h are coherent, and H is complete\nif each element of E+\nis covered by at least one term h of\nH. Each term is by construction the lgg (least general \ngeneralization) of a subset of positives instances {e1, ..., en}[5],\nthat is the most specific term covering {e1, ..., en}. The\nlgg operator is defined by considering examples as terms,\nso we denote as lgg(e) the most specific term that covers\ne, and as lgg(h, e) the most specific term which is more\ngeneral than h and that covers e. Restricting the term to\nlgg is the basis of a lot of Bottom-Up learning algorithms\n(for instance [5]). In the typology proposed by [9], our \nupdate mechanism is an incremental learner with full instance\nmemory: learning is made by successive updates and all\nexamples are stored.\nThe update mechanism depends of the ongoing hypothesis\nH, the ongoing examples E+\nand E\u00e2\u02c6\u2019\n, and the new example\ne. There are three possible cases:\n\u00e2\u20ac\u00a2 e is positive and H covers e, or e is negative and H\ndoes not cover e. No update is needed, H is already\ncomplete and coherent with E \u00e2\u02c6\u00aa {e}.\n\u00e2\u20ac\u00a2 e is positive and H does not cover e: e is denoted\nas a positive counterexample of H. Then we seek\nto generalize in turn the terms h of H. As soon\nas a correct generalization h = lgg(h, e) is found, h\nreplaces h in H. If there is a term that is less general\nthat h , it is discarded. If no generalization is correct\n(meaning here coherent), H \u00e2\u02c6\u00aa lgg(e) replaces H.\n\u00e2\u20ac\u00a2 e is negative and H covers e: e is denoted as a \nnegative counterexample of H. Each term h covering e\nis then discarded from H and replaced by a set of\nterms {h1, ...., hn} that is, as a whole, coherent with\nE\u00e2\u02c6\u2019\n\u00e2\u02c6\u00aa {e} and that covers the examples of E+\n\nuncovered by H \u00e2\u02c6\u2019 {h}. Terms of the final hypothesis H\nthat are less general than others are discarded from\nH.\nWe will now describe the case where e = e\u00e2\u02c6\u2019\nis a covered\nnegative example. The following functions are used here:\n\u00e2\u20ac\u00a2 coveredOnlyBy(h, E+) gives the subset of E+\ncovered\nby h and no other term of H.\n\u00e2\u20ac\u00a2 bestCover(h1, h2) gives h1 if h1 covers more examples\nfrom uncoveredPos than h2, otherwise it gives h2.\n\u00e2\u20ac\u00a2 covered(h) gives the elements of uncoveredPos covered\nby h.\n// Specialization of each h covering e\u00e2\u02c6\u2019\nfor each h of H covering e\u00e2\u02c6\u2019\ndo\nH = H \u00e2\u02c6\u2019 {h}\nuncoveredPos = coveredOnlyBy(h, E+\n)\nAr= atoms that are neither in e\u00e2\u02c6\u2019\nnor in h\nwhile (uncoveredPos = \u00e2\u02c6\u2026) do\n// seeking the best specialization of h\nhc=h\nbest=\u00e2\u0160\u00a5 // \u00e2\u0160\u00a5 covers no example\nfor each a of Ar do\nhc= h \u00e2\u02c6\u00a7 a\nbest = bestCover(hc, best)\nendfor\nAr=Ar\u00e2\u02c6\u2019{best}\nhi=lgg(covered(best))\nH = H \u00e2\u02c6\u00aa {hi}\nuncoveredPos=uncoveredPos - covered(best)\nendwhile\nendfor\nTerms of H that are less general than others are discarded.\nNote that this mechanism tends to both make a minimal\nupdate of the current hypothesis and minimize the number\nof terms in the hypothesis, in particular by discarding terms\nless general than other ones after updating a hypothesis.\n3.3 Collective learning\nIf H is the current hypothesis, Ei the current example\nmemory of agent ri and E the set of all the examples\nreceived by the system, the notation of section 2 becomes\nBi = BC = H, Ki = Ei and K = E. Cons(H, Ei) states\nthat H is complete and coherent with Ei. In such a case,\nri is a-consistent. The piece of information k received by\nagent ri is here simply an example e along with its tag.\nIf e is such that the current hypothesis H is not complete\nor coherent with Ei \u00e2\u02c6\u00aa {e}, e contradicts H: ri becomes\na-inconsistent, and therefore the MAS is not consistent\nanymore.\nThe update of a hypothesis when a new example arrives\nis an a- consistent mechanism. Following proposition 1 this\nmechanism can be used to produce a strong mas-consistent\nmechanism: upon reception of a new example in the MAS\nby an agent r, an update is possibly needed and, after a set\nof interactions between r and the other agents, results in a\nnew hypothesis shared by all the agents and that restores\nthe consistency of the MAS, that is which is complete and\ncoherent with the set ES of all the examples present in the\nMAS.\nIt is clear that by minimizing the number of \nhypothesis modifications, this synchronous and minimal \nmechanism minimize the number of examples received by the\nlearner from other agents, and therefore, the total number\nof examples stored in the system.\n4. EXPERIMENTS\nIn the following, we will learn a boolean formula that is\na difficult test for the learning method: the 11-multiplexer\n(see [4]). It concerns 3 address boolean attributes a0, a1, a2\nand 8 data boolean attributes d0, ..., d7. Formulae f11 is\nsatisfied if the number coded by the 3 address attributes is\nthe number of a data attribute whose value is 1. Its formula\nis the following:\nf11 = (a0 \u00e2\u02c6\u00a7a1 \u00e2\u02c6\u00a7a2 \u00e2\u02c6\u00a7d7)\u00e2\u02c6\u00a8(a0 \u00e2\u02c6\u00a7a1 \u00e2\u02c6\u00a7\u00c2\u00aca2 \u00e2\u02c6\u00a7d6)\u00e2\u02c6\u00a8(a0 \u00e2\u02c6\u00a7\u00c2\u00aca1 \u00e2\u02c6\u00a7\na2 \u00e2\u02c6\u00a7d5)\u00e2\u02c6\u00a8(a0 \u00e2\u02c6\u00a7\u00c2\u00aca1 \u00e2\u02c6\u00a7\u00c2\u00aca2 \u00e2\u02c6\u00a7d4)\u00e2\u02c6\u00a8(\u00c2\u00aca0 \u00e2\u02c6\u00a7a1 \u00e2\u02c6\u00a7a2 \u00e2\u02c6\u00a7d3)\u00e2\u02c6\u00a8(\u00c2\u00aca0 \u00e2\u02c6\u00a7\na1 \u00e2\u02c6\u00a7\u00c2\u00aca2 \u00e2\u02c6\u00a7d2)\u00e2\u02c6\u00a8(\u00c2\u00aca0 \u00e2\u02c6\u00a7\u00c2\u00aca1 \u00e2\u02c6\u00a7a2 \u00e2\u02c6\u00a7d1)\u00e2\u02c6\u00a8(\u00c2\u00aca0 \u00e2\u02c6\u00a7\u00c2\u00aca1 \u00e2\u02c6\u00a7\u00c2\u00aca2 \u00e2\u02c6\u00a7d0).\nThere are 2048 = 211\npossible examples, half of whom are\npositive (meaning they satisfy f11) while the other half is\nnegative.\nAn experiment is typically composed of 50 trials. Each\nrun corresponds to a sequence of 600 examples that are \nincrementally learned by a Multi Agent System with n agents\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 167\n(n-MAS). A number of variables such as accuracy, (i.e. the\nfrequency of correct classification of a set of unseen \nexamples), hypothesis size (i.e. the number of terms in the \ncurrent formula) or number of stored examples, is recorded each\ntime 25 examples are received by the system during those\nruns.\nIn the protocol that is used here, a new example is sent\nto a random agent when the MAS is consistent. The next\nexample will be sent in turn to an other agent when the\nMAS consistency will have been restored. In such a way we\nsimulate a kind of slow learning: the frequency of example\narrivals is slow compared to the time taken by an update.\n4.1 Efficiency of MAS concept learning\n4.1.1 Execution time\nWe briefly discuss here execution time of learning in the\nMAS. Note that the whole set of action and interaction in\nthe MAS is simulated on a single processor. Figure 1 shows\nthat time linearly depends on the number of agents. At the\nend of the most active part of learning (200 examples), a \n16MAS has taken 4 times more learning time than a 4-MAS.\nThis execution time represents the whole set of learning and\nFigure 1: Execution time of a n-MAS (from n = 2 at\nthe bottom to n = 20 on the top).\ncommunication activity and hints at the cost of \nmaintaining a consistent learning hypothesis in a MAS composed of\nautonomous agents.\n4.1.2 Redundancy in the MAS memory\nWe study now the distribution of the examples in the MAS\nmemory. Redundancy is written RS = nS/ne, where nS is\nthe total number of examples stored in the MAS, that is the\nsum of the sizes of agents examples memories Ei, and ne is\nthe total number of examples received from the environment\nin the MAS. In figure 2, we compare redundancies in 2 to\n20 agents MAS. There is a peak, slowly moving from 80 to\n100 examples, that represents the number of examples for\nwhich the learning is most active. For 20 agents, maximal\nredundancy is no more than 6, which is far less than the\nmaximal theoretical value of 20. Note that when learning\nbecomes less active, redundancy tends towards its minimal\nvalue 1: when there is no more updates, examples are only\nFigure 2: Redundancy of examples stored in a \nnMAS (from n = 2 at the bottom to n = 20 on the\ntop) .\nstored by the agent that receives them.\n4.1.3 A n-MAS selects a simpler solution than a \nsingle agent\nThe proposed mechanism tends to minimize the number of\nterms in the selected hypothesis. During learning, the size of\nthe current hypothesis grows up beyond the optimum, and\nthen decreases when the MAS converges. In the Multiplexer\n11 testbed, the optimal number of terms is 8, but there also\nexist equivalent formulas with more terms. It is interesting\nto note that in this case the 10-MAS converges towards an\nexact solution closer to the optimal number of terms (here\n8) (see Figure 3). After 1450 examples have been presented\nboth 1-MAS and 10-MAS have exactly learned the concept\n(the respective accuracies are 0.9999 and 1) but the single\nagent expresses in average the result as a 11.0 terms DNF\nwhereas the 10-MAS expresses it as a 8.8 terms DNF. \nHowever for some other boolean functions we found that \nduring learning 1-MAS always produces larger hypotheses than\n10-MAS but that both MAS converge to hypotheses with\nsimilar size results.\n4.1.4 A n-MAS is more accurate than a single agent\nFigure 4 shows the improvement brought by a MAS with\nn agents compared to a single agent. This improvement was\nnot especially expected, because whether we have one or n\nagents, when N examples are given to the MAS it has access\nto the same amount of information, maintains only on \nongoing hypothesis and uses the same basic revision algorithm\nwhenever an agent has to modify the current hypothesis.\nNote that if the accuracy of 1, 2, 4 and 10-MAS are \nsignificantly different, getting better as the number of agents\nincreases, there is no clear difference beyond this point: the\naccuracy curve of the 100 agents MAS is very close to the\none of the 10 agents MAS.\n4.1.4.1 Boolean formulas.\nTo evaluate this accuracy improvement, we have \nexperimented our protocol on other problems of boolean \nfunction learning, As in the Multiplexer-11 case, these functions\n168 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\nFigure 3: Size of the hypothesis built by 1 and \n10MAS: the M11 case.\nFigure 4: Accuracy of a n-MAS: the M11 case (from\nbottom to top, n = 1, 2, 4, 10, 100).\nare learnt in the form of more or less syntactically complex\nDNF3\n(that is with more or less conjunctive terms in the\nDNF), but are also more or less difficult to learn as it can\nbe difficult to get its way in the hypothesis space to reach\nthem. Furthermore, the presence in the description of \nirrelevant attributes (that is attributes that does not belong to\nthe target DNF) makes the problem more difficult. The \nfollowing problems have been selected to experiment our \nprotocol: (i) the multiplexer-11 with 9 irrelevant attributes:\nM11 9, (ii) the 20-multiplexer M20 (with 4 address bits and\n16 data bits), (iii) a difficult parity problem (see [4]) the\nXorp m: there must be an odd number of bits with value 1\nin the p first attributes for the instance to be positive, the\np others bits being irrelevant, and (iv) a simple DNF \nformula (a \u00e2\u02c6\u00a7 b \u00e2\u02c6\u00a7 c) \u00e2\u02c6\u00a8 (c \u00e2\u02c6\u00a7 d \u00e2\u02c6\u00a7 e)(e \u00e2\u02c6\u00a7 f \u00e2\u02c6\u00a7 g) \u00e2\u02c6\u00a7 (g \u00e2\u02c6\u00a7 h \u00e2\u02c6\u00a7 i) with 19\nirrelevant attributes. The following table sums up some \ninformation about these problems, giving the total number of\nattributes including irrelevant ones, the number of irrelevant\n3\nDisjunctive Normal Forms\nattributes, the minimal number of terms of the \ncorresponding DNF, and the number of learning examples used.\nPb att. irre. att. terms ex.\nM11 11 0 8 200\nM11 9 20 9 8 200\nM20 20 0 16 450\nXor3 25 28 25 4 200\nXor5 5 10 5 16 180\nXor5 15 20 15 16 600\nSimple4-9 19 28 19 4 200\nBelow are given the accuracy results of our learning \nmechanism with a single agent and a 10 agents MAS, along with\nthe results of two standard algorithms implemented with the\nlearning environment WEKA[16]: JRip (an implementation\nof RIPPER[2]) and Id3[12]. For the experiments with JRip\nand Id3, we measured the mean accuracy on 50 trials, each\ntime randomly separating examples in a learning set and a\ntest set. JRip and Id3 parameters are default parameters,\nexcept that JRip is used without pruning. The following\ntable shows the results:\nPb JRip Id3 Sm 1 Sm 10\nM11 88.3 80.7 88.7 95.5\nM11 9 73.4 67.9 66.8 83.5\nM20 67.7 62.7 64.6 78.2\nXor3 25 54.4 55.2 71.4 98.5\nXor5 5 52.6 60.8 71.1 78.3\nXor5 15 50.9 51.93 62.4 96.1\nSimple4-9 19 99.9 92.3 87.89 98.21\nIt is clear that difficult problems are better solved with\nmore agents (see for instance xor5 15). We think that these\nbenefits, which can be important with an increasing number\nof agents, are due to the fact that each agent really \nmemorizes only part of the total number of examples, and this\npart is partly selected by other agents as counter examples,\nwhich cause a greater number of current hypothesis updates\nand therefore, a better exploration of the hypotheses space.\n4.1.4.2 ML database problems.\nWe did also experiments with some non boolean problems.\nWe considered only two classes (positive/negative) \nproblems, taken from the UCI\"s learning problems database[3].\nIn all these problems, examples are described as a \nvector of couples (attribute, value). The value domains can\nbe either boolean, numeric (wholly ordered set), or \nnominal (non-ordered set). An adequate set of atoms A must be\nconstituted for each problem. For instance, if a is a numeric\nattribute, we define at most k threshold si, giving k+1 \nintervals of uniform density4\n. Therefore, each distinct threshold\nsi gives two atoms a \u00e2\u2030\u00a4 si and a > si. In our experiments,\nwe took a maximal number of threshold k = 8. For instance,\nin the iono problem case, there were 34 numeric attributes,\nand an instance is described with 506 atoms.\nBelow are given the accuracy results of our system along\nwith previous results. The column Nb ex. refer to the\n4\nThe probability for the value of a to be in any interval is\nconstant\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 169\nnumber of examples used for learning5\n. Column (1) \nrepresents minimal and maximal accuracy values for the thirty\nthree classifiers tested in [8]. Column (2) represents the \nresults of [13], where various learning methods are compared\nto ensemble learning methods using weighted classifiers sets.\nColumn S-1 and S-10 gives the accuracy of SMILE with \nrespectively 1 and 10 agents.\nPb Nb ex. (1) (2) S-1 S-10\nttt 862/574 // 76.2-99.7 99.7 99.9\nkr-vs-kp 2876/958 // 91.4-99.4 96.8 97.3\niono 315 // 88.0-91.8 87.2 88.1\nbupa 310 57-72 58-69.3 62.5 63.3\nbreastw 614 91-97 94.3-97.3 94.7 94.7\nvote 391 94-96 95.3-96 91.9 92.6\npima 691 // 71.5- 73.4 65.0 65.0\nheart 243 66-86 77.1-84.1 69.5 70.7\nThis table shows that the incremental algorithm \ncorresponding to the single agent case, gives honorable results\nrelatively to non-incremental classical methods using larger\nand more complex hypotheses. In some cases, there is an \naccuracy improvement with a 10 agents MAS. However, with\nsuch benchmarks data, which are often noisy, the difficulty\ndoes not really come from the way in which the search space\nis explored, and therefore the improvement observed is not\nalways significant. The same kind of phenomenon have been\nobserved with methods dedicated to hard boolean problems\n[4].\n4.2 MAS synchronization\nHere we consider that n single agents learn without \ninteractions and at a given time start interacting thus forming a\nMAS. The purpose is to observe how the agents take \nadvantage of collaboration when they start from different states of\nbeliefs and memories. We compare in this section a 1-MAS,\na 10-MAS (ref) and a 10-MAS (100sync) whose agents did\nnot communicate during the arrival of the first 100 \nexamples (10 by agents). The three accuracy curves are shown in\nfigure 5. By comparing the single agent curve and the \nsynchronized 10-MAS, we can observe that after the beginning\nof the synchronization, that is at 125 examples, accuracies\nare identical. This was expected since as soon as an example\ne received by the MAS contradicts the current hypothesis of\nthe agent ra receiving it, this agent makes an update and its\nnew hypothesis is proposed to the others agents for criticism.\nTherefore, this first contradictory example brings the MAS\nto reach consistency relatively to the whole set of examples\npresent in agents\" memories. A higher accuracy, \ncorresponding to a 10-MAS is obtained later, from the 175th example.\nIn other words, the benefit of a better exploration of the\nresearch space is obtained slightly later in the learning \nprocess. Note that this synchronization happens naturally in all\nsituations where agents have, for some reason, a divergence\nbetween their hypothesis and the system memory. This \nincludes the fusion of two MAS into a single one or the arrival\nof new agents in an existing MAS.\n4.3 Experiments on asynchronous learning:\nthe effect of a large data stream\n5\nFor ttt and kr-vs-kp, our protocol did not use more than \nrespectively 574 and 958 learning examples, so we put another\nnumber in the column.\nFigure 5: Accuracies of a 1-MAS, a 10-MAS, and a\n10-MAS synchronized after 100 examples.\nIn this experiment we relax our slow learning mode: the\nexamples are sent at a given rate to the MAS. The \nresulting example stream is measured in ms\u00e2\u02c6\u20191\n, and represents\nthe number of examples sent to the MAS each ms. \nWhenever the stream is too large, the MAS cannot reach MAS\nconsistency on reception of an example from the \nenvironment before a new example arrives. This means that the\nupdate process, started by agent r0 as he received an \nexample, may be unfinished when a new example is received by\nr0 or another agent r1. As a result, a critic agent may have\nat instant t to send counterexamples of hypotheses sent by\nvarious agents. However as far as the agents, in our \nsetting, memorizes all the examples they receive whenever the\nstream ends, the MAS necessarily reaches MAS consistency\nwith respect to all the examples received so far. In our \nexperiments, though its learning curve is slowed down during\nthe intense learning phase (corresponding to low accuracy of\nthe current hypotheses), the MAS still reaches a satisfying\nhypothesis later on as there are less and less \ncounterexamples in the example stream. In Figure 6 we compare the\naccuracies of two 11-MAS respectively submitted to \nexample streams of different rates when learning the M11 formula.\nThe learning curve of the MAS receiving an example at a\n1/33 ms\u00e2\u02c6\u20191\nrate is almost not altered (see Figure 4) whereas\nthe 1/16 ms\u00e2\u02c6\u20191\nMAS is first severely slowed down before\ncatching up with the first one.\n5. RELATED WORKS\nSince 96 [15], various work have been performed on \nlearning in MAS, but rather few on concept learning. In [11]\nthe MAS performs a form of ensemble learning in which the\nagents are lazy learners (no explicit representation is \nmaintained) and sell useless examples to other agents. In [10]\neach agent observes all the examples but only perceive a\npart of their representation. In mutual online concept \nlearning [14] the agents converge to a unique hypothesis, but each\nagent produces examples from its own concept \nrepresentation, thus resulting in a kind of synchronization rather than\nin pure concept learning.\n170 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)\nFigure 6: Accuracies of two asynchronous 11-MAS\n(1/33ms\u00e2\u02c6\u20191\nand 1/16ms\u00e2\u02c6\u20191\nexample rates) .\n6. CONCLUSION\nWe have presented here and experimented a protocol for\nMAS online concept learning. The main feature of this \ncollaborative learning mechanism is that it maintains a \nconsistency property: though during the learning process each\nagent only receives and stores, with some limited \nredundancy, part of the examples received by the MAS, at any\nmoment the current hypothesis is consistent with the whole\nset of examples. The hypotheses of our experiments do not\naddress the issues of distributed MAS such as faults (for \ninstance messages could be lost or corrupted) or other failures\nin general (crash, byzantine faults, etc.). Nevertheless, our\nframework is open, i.e., the agents can leave the system or\nenter it while the consistency mechanism is preserved. For\ninstance if we introduce a timeout mechanism, even when\na critic agent crashes or omits to answer, the consistency\nwith the other critics (within the remaining agents) is \nentailed. In [1], a similar approach has been applied to MAS\nabduction problems: the hypotheses to maintain, given an\nincomplete information, are then facts or statements. \nFurther work concerns first coupling induction and abduction in\norder to perform collaborative concept learning when \nexamples are only partially observed by each agent, and second,\ninvestigating partial memory learning: how learning is \npreserved whenever one agent or the whole MAS forgets some\nselected examples.\nAknowledgments\nWe are very grateful to Dominique Bouthinon for \nimplementing late modifications in SMILE, so much easing our\nexperiments. Part of this work has been performed during\nthe first author\"s visit to the Atelier De BioInformatique\nof Paris VI university, France.\n7. REFERENCES\n[1] G. Bourgne, N. Maudet, and S. Pinson. When agents\ncommunicate hypotheses in critical situations. In\nDALT-2006, May 2006.\n[2] W. W. Cohen. Fast effective rule induction. In ICML,\npages 115-123, 1995.\n[3] C. B. D.J. Newman, S. Hettich and C. Merz. UCI\nrepository of machine learning databases, 1998.\n[4] S. Esmeir and S. Markovitch. Lookahead-based\nalgorithms for anytime induction of decision trees. In\nICML\"O4, pages 257-264. Morgan Kaufmann, 2004.\n[5] J. F\u00c2\u00a8urnkranz. A pathology of bottom-up hill-climbing\nin inductive rule learning. In ALT, volume 2533 of\nLNCS, pages 263-277. Springer, 2002.\n[6] A. Guerra-Hern\u00c2\u00b4andez, A. ElFallah-Seghrouchni, and\nH. Soldano. Learning in BDI multi-agent systems. In\nCLIMA IV, volume 3259, pages 218-233. Springer\nVerlag, 2004.\n[7] M. Henniche. Mgi: an incremental bottom-up\nalgorithm. In IEEE Aust. and New Zealand\nConference on Intelligent Information Systems, pages\n347-351, 1994.\n[8] T.-S. Lim, W.-Y. Loh, and Y.-S. Shih. A comparison\nof prediction accuracy, complexity, and training time\nof thirty-three old and new classification algorithms.\nMachine Learning, 40(3):203-228, 2000.\n[9] M. A. Maloof and R. S. Michalski. Incremental\nlearning with partial instance memory. Artif. Intell.,\n154(1-2):95-126, 2004.\n[10] P. J. Modi and W.-M. Shen. Collaborative multiagent\nlearning for classification tasks. In AGENTS \"01,\npages 37-38. ACM Press, 2001.\n[11] S. Onta\u00cb\u0153non and E. Plaza. Recycling data for\nmulti-agent learning. In ICML \"05, pages 633-640.\nACM Press, 2005.\n[12] J. R. Quinlan. Induction of decision trees. Machine\nLearning, 1(1):81-106, 1986.\n[13] U. R\u00c2\u00a8uckert and S. Kramer. Towards tight bounds for\nrule learning. In ICML \"04 (International conference\non Machine learning), page 90, New York, NY, USA,\n2004. ACM Press.\n[14] J. Wang and L. Gasser. Mutual online concept\nlearning for multiple agents. In AAMAS, pages\n362-369. ACM Press, 2002.\n[15] G. Wei\u00c3\u0178 and S. Sen, editors. Adaption and Learning in\nMulti-Agent Systems, volume 1042 of Lecture Notes in\nComputer Science. Springer, 1996.\n[16] I. H. Witten and E. Frank. Data Mining: Practical\nMachine Learning Tools and Techniques with Java\nImplementations. Morgan Kaufmann, October 1999.\nThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 171\n": ["multi-agent learning", "collaborative concept learning", "learning process", "knowledge", "mas-consistency", "incremental learning", "agent", "update mechanism", "synchronization", "multi-agent learn", ""]}