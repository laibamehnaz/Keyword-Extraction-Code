{"Runtime Metrics Collection for Middleware Supported\nAdaptation of Mobile Applications\nHendrik Gani\nSchool of Computer Science and\nInformation Technology,\nRMIT University, Melbourne, Australia\nhgani@cs.rmit.edu.au\nCaspar Ryan\nSchool of Computer Science and\nInformation Technology,\nRMIT University, Melbourne, Australia\ncaspar@cs.rmit.edu.au\nPablo Rossi\nSchool of Computer Science and\nInformation Technology,\nRMIT University, Melbourne, Australia\npablo@cs.rmit.edu.au\nABSTRACT\nThis paper proposes, implements, and evaluates in terms of worst\ncase performance, an online metrics collection strategy to\nfacilitate application adaptation via object mobility using a mobile\nobject framework and supporting middleware. The solution is\nbased upon an abstract representation of the mobile object system,\nwhich holds containers aggregating metrics for each specific\ncomponent including host managers, runtimes and mobile objects.\nA key feature of the solution is the specification of multiple\nconfigurable criteria to control the measurement and propagation\nof metrics through the system. The MobJeX platform was used as\nthe basis for implementation and testing with a number of\nlaboratory tests conducted to measure scalability, efficiency and\nthe application of simple measurement and propagation criteria to\nreduce collection overhead.\nCategories and Subject Descriptors\nC.2.4 Distributed Systems; D.2.8 Metrics\nGeneral Terms\nMeasurement, Performance.\n1. INTRODUCTION\nThe different capabilities of mobile devices, plus the varying\nspeed, error rate and disconnection characteristics of mobile\nnetworks [1], make it difficult to predict in advance the exact\nexecution environment of mobile applications. One solution\nwhich is receiving increasing attention in the research community\nis application adaptation [2-7], in which applications adjust their\nbehaviour in response to factors such as network, processor, or\nmemory usage.\nEffective adaptation requires detailed and up to date\ninformation about both the system and the software itself. Metrics\nrelated to system wide information (e.g. processor, memory and\nnetwork load) are referred to as environmental metrics [5], while\nmetrics representing application behaviour are referred as\nsoftware metrics [8]. Furthermore, the type of metrics required for\nperforming adaptation is dependent upon the type of adaptation\nrequired. For example, service-based adaptation, in which service\nquality or service behaviour is modified in response to changes in\nthe runtime environment, generally requires detailed\nenvironmental metrics but only simple software metrics [4]. On\nthe other hand, adaptation via object mobility [6], also requires\ndetailed software metrics [9] since object placement is dependent\non the execution characteristics of the mobile objects themselves.\nWith the exception of MobJeX [6], existing mobile object\nsystems such as Voyager [10], FarGo [11, 12], and JavaParty [13]\ndo not provide automated adaptation, and therefore lack the\nmetrics collection process required to support this process. In the\ncase of MobJeX, although an adaptation engine has been\nimplemented [5], preliminary testing was done using synthetic\npre-scripted metrics since there is little prior work on the dynamic\ncollection of software metrics in mobile object frameworks, and\nno existing means of automatically collecting them.\nConsequently, the main contribution of this paper is a solution\nfor dynamic metrics collection to support adaptation via object\nmobility for mobile applications. This problem is non-trivial since\ntypical mobile object frameworks consist of multiple application\nand middleware components, and thus metrics collection must be\nperformed at different locations and the results efficiently\npropagated to the adaptation engine. Furthermore, in some cases\nthe location where each metric should be collected is not fixed\n(i.e. it could be done in several places) and thus a decision must\nbe made based on the efficiency of the chosen solution (see\nsection 3).\nThe rest of this paper is organised as follows: Section 2\ndescribes the general structure and implementation of mobile\nobject frameworks in order to understand the challenges related to\nthe collection, propagation and delivery of metrics as described in\nsection 3. Section 4 describes some initial testing and results and\nsection 5 closes with a summary, conclusions and discussion of\nfuture work.\n2. BACKGROUND\nIn general, an object-oriented application consists of objects\ncollaborating to provide the functionality required by a given\nproblem domain. Mobile object frameworks allow some of these\nobjects to be tagged as mobile objects, providing middleware\nsupport for such objects to be moved at runtime to other hosts. At\na minimum, a mobile object framework with at least one running\nmobile application consists of the following components:\nruntimes, mobile objects, and proxies [14], although the\nterminology used by individual frameworks can differ [6, 10-13].\nA runtime is a container process for the management of\nmobile objects. For example, in FarGo [15] this component is\nknown as a core and in most systems separate runtimes are\nrequired to allow different applications to run independently,\nalthough this is not the case with MobJeX, which can run multiple\napplications in a single runtime using threads. The applications\nthemselves comprise mobile objects, which interact with each\nother through proxies [14]. Proxies, which have the same method\ninterface as the object itself but add remote communication and\nobject tracking functionality, are required for each target object\nthat a source object communicates with. Upon migration, proxy\nobjects move with the source object.\nThe Java based system MobJeX, which is used as the\nimplementation platform for the metrics collection solution\ndescribed in this paper, adds a number of additional middleware\ncomponents. Firstly, a host manager (known as a service in\nMobJeX) provides a central point of communication by running\non a known port on a per host basis, thus facilitating the\nenumeration or lookup of components such as runtimes or mobile\nobjects. Secondly, MobJeX has a per-application mobile object\ncontainer called a transport manager (TM). As such the host and\ntransport managers are considered in the solution provided in the\nnext section but could be omitted in the general case. Finally,\ndepending on adaptation mode, MobJeX can have a centralised\nsystem controller incorporating a global adaptation engine for\nperforming system wide optimisation.\n3. METRICS COLLECTION\nThis section discusses the design and derivation of a solution\nfor collecting metrics in order to support the adaptation of\napplications via object migration. The solution, although\nimplemented within the MobJeX framework, is for the most part\ndiscussed in generic terms, except where explicitly stated to be\nMobJeX specific.\n3.1 Metrics Selection\nThe metrics of Ryan and Rossi [9] have been chosen as the\nbasis for this solution, since they are specifically intended for\nmobile application adaptation as well as having been derived from\na series of mathematical models and empirically validated.\nFurthermore, the metrics were empirically shown to improve the\napplication performance in a real adaptation scenario following a\nchange in the execution environment.\nIt would however be beyond the scope of this paper to\nimplement and test the full suite of metrics listed in [9], and thus\nin order to provide a useful non-random subset, we chose to\nimplement the minimum set of metrics necessary to implement\nlocal and global adaptation [9] and thereby satisfy a range of real\nadaptation scenarios. As such the solution presented in this\nsection is discussed primarily in terms of these metrics, although\nthe structure of the solution is intended to support the\nimplementation of the remaining metrics, as well as other\nunspecified metrics such as those related to quality and resource\nutilisation. This subset is listed below and categorised according\nto metric type. Note that some additional metrics were used for\nimplementation purposes in order to derive core metrics or assist\nthe evaluation, and as such are defined in context where\nappropriate.\n1. Software metrics\n- Number of Invocations (NI), the frequency of invocations on\nmethods of a class.\n2. Performance metrics\n- Method Execution Time (ET), the time taken to execute a\nmethod body (ms).\n- Method Invocation Time (IT), the time taken to invoke a\nmethod, excluding the method execution time (ms).\n3. Resource utilization metrics\n- Memory Usage (MU), the memory usage of a process (in\nbytes).\n- Processor Usage (PU), the percentage of the CPU load of a\nhost.\n- Network Usage (NU), the network bandwidth between two\nhosts (in bytes/sec).\nFollowing are brief examples of a number of these metrics in\norder to demonstrate their usage in an adaptation scenario. As\nProcessor Usage (PU) on a certain host increases, the Execution\nTime (ET) of a given method executed on that host also increases\n[9], thus facilitating the decision of whether to move an object\nwith high ET to another host with low PU. Invocation Time (IT)\nshows the overhead of invoking a certain method, with the\ninvocation overhead of marshalling parameters and transmitting\nremote data for a remote call being orders of magnitude higher\nthan the cost of pushing and popping data from the method call\nstack. In other words, remote method invocation is expensive and\nthus should be avoided unless the gains made by moving an\nobject to a host with more processing power (thereby reducing\nET) outweigh the higher IT of the remote call. Finally, Number of\nInvocations (NI) is used primarily as a weighting factor or\nmultiplier in order to enable the adaptation engine to predict the\nvalue over time of a particular adaptation decision.\n3.2 Metrics Measurement\nThis subsection discusses how each of the metrics in the\nsubset under investigation can be obtained in terms of either direct\nmeasurement or derivation, and where in the mobile object\nframework such metrics should actually be measured. Of the\nenvironmental resource metrics, Processor Usage (PU) and\nNetwork Usage (NU) both relate to an individual machine, and\nthus can be directly measured through the resource monitoring\nsubsystem that is instantiated as part of the MobJeX service.\nHowever, Memory Usage (MU), which represents the memory\nstate of a running process rather than the memory usage of a host,\nshould instead be collected within an individual runtime.\nThe measurement of Number of Invocations (NI) and\nExecution Time (ET) metrics can be also be performed via direct\nmeasurement, however in this case within the mobile object\nimplementation (mobject) itself.\nNI involves simply incrementing a counter value at either the\nstart or end of a method call, depending upon the desired\nsemantics with regard to thrown exceptions, while ET can be\nmeasured by starting a timer at the beginning of the method and\nstopping it at the end of the method, then retrieving the duration\nrecorded by the timer.\nIn contrast, collecting Invocation Time (IT) is not as straight\nforward because the time taken to invoke a method can only be\nmeasured after the method finishes its execution and returns to the\ncaller. In order to collect IT metrics, another additional metric is\nneeded. Ryan and Rossi [9] define the metric Response Time\n(RT), as the total time taken for a method call to finish, which is\nthe sum of IT and ET. The Response Time can be measured\ndirectly using the same timer based technique used to measure ET,\nalthough at the start and end of the proxy call rather than the\nmethod implementation. Once the Response Time (RT) is known,\nIT can derived by subtracting RT from ET.\nAlthough this derivation appears simple, in practice it is\ncomplicated by the fact that the RT and ET values from which the\nIT is derived are by necessity measured using timer code in\ndifferent locations i.e. RT measured in the proxy, ET measured in\nthe method body of the object implementation. In addition, the\nproxies are by definition not part of the MobJeX containment\nhierarchy, since although proxies have a reference to their target\nobject, it is not efficient for a mobile object (mobject) to have\nbackward references to all of the many proxies which reference it\n(one per source object). Fortunately, this problem can be solved\nusing the push based propagation mechanism described in section\n3.5 in which the RT metric is pushed to the mobject so that IT can\nbe derived from the ET value stored there. The derived value of IT\nis then stored and propagated further as necessary according to the\ncriteria of section 3.6, the structural relationship of which is\nshown in Figure 1.\n3.3 Measurement Initiation\nThe polling approach was identified as the most appropriate\nmethod for collecting resource utilisation metrics, such as\nProcessor Usage (PU), Network Usage (NU) and Memory Usage\n(MU), since they are not part of, or related to, the direct flow of\nthe application. To measure PU or NU, the resource monitor polls\nthe Operating System for the current CPU or network load\nrespectively. In the case of Memory Usage (MU), the Java Virtual\nMachine (JVM) [16] is polled for the current memory load. Note\nthat in order to minimise the impact on application response time,\nthe polling action should be done asynchronously in a separate\nthread. Metrics that are suitable for application initiated collection\n(i.e. as part of a normal method call) are software and\nperformance related metrics, such as Number of Invocations (NI),\nExecution Time (ET), and Invocation Time (IT), which are\nexplicitly related to the normal invocation of a method, and thus\ncan be measured directly at this time.\n3.4 Metrics Aggregation\nIn the solution presented in this paper, all metrics collected in\nthe same location are aggregated in a MetricsContainer\nwith individual containers corresponding to functional\ncomponents in the mobile object framework. The primary\nadvantage of aggregating metrics in containers is that it allows\nthem to be propagated easily as a cohesive unit through the\ncomponents of the mobility framework so that they can be\ndelivered to the adaptation engine, as discussed in the following\nsubsection.\nNote that this containment captures the different granularity of\nmeasurement attributes and their corresponding metrics. Consider\nthe case of measuring memory consumption. At a coarse level of\ngranularity this could be measured for an entire application or\neven a system, but could also be measured at the level of an\nindividual object; or for an even finer level of granularity, the\nmemory consumption during the execution of a specific method.\nAs an example of the level of granularity required for mobility\nbased adaptation, the local adaptation algorithm proposed by\nRyan and Rossi [9] requires metrics representing both the\nduration of a method execution and the overhead of a method\ninvocation. The use of metrics containers facilitates the collection\nof metrics at levels of granularity ranging from a single machine\ndown to the individual method level.\nNote that some metrics containers do not contain any\nMetric objects, since as previously described, the sample\nimplementation uses only a subset of the adaptation metrics from\n[9]. However, for the sake of consistency and to promote\nflexibility in terms of adding new metrics in the future, these\ncontainers are still considered in the present design for\ncompleteness and for future work.\n3.5 Propagation and Delivery of Metrics\nThe solution in this paper identifies two stages in the metrics\ncollection and delivery process. Firstly, the propagation of\nmetrics through the components of the mobility framework and\nsecondly, the delivery of those metrics from the host\nmanager/service (or runtime if the host manager is not present) to\nthe adaptation engine.\nRegarding propagation, in brief, it is proposed that when a\nlower level system component detects the arrival of a new metric\nupdate (e.g. mobile object), the metric is pushed (possibly along\nwith other relevant metrics) to the next level component (i.e.\nruntime or transport manager containing the mobile object), which\nat some later stage, again determined by a configurable criteria\n(for example when there are a sufficient number of changed\nmobjects) will get pushed to the next level component (i.e. the\nhost manager or the adaptation engine).\nA further incentive for treating propagation separately from\ndelivery is due to the distinction between local and global\nadaptation [9]. Local adaptation is performed by an engine\nrunning on the local host (for example in MobJeX this would\noccur within the service) and thus in this case the delivery phase\nwould be a local inter-process call. Conversely, global adaptation\nis handled by a centralised adaptation engine running on a remote\nhost and thus the delivery of metrics is via a remote call, and in\nthe case where multiple runtimes exist without a separate host\nmanager the delivery process would be even more expensive.\nTherefore, due to the presence of network communication latency,\nit is important for the host manager to pass as many metrics as\npossible to the adaptation engine in one invocation, implying the\nneed to gather these metrics in the host manager, through some\nform of push or propagation, before sending them to the\nadaptation engine.\nConsequently, an abstract representation or model [17] of the\nsystem needs to be maintained. Such a model would contain\nmodel entities, corresponding to each of the main system\ncomponents, connected in a tree like hierarchy, which precisely\nreflects the structure and containment hierarchy of the actual\nsystem. Attaching metrics containers to model entities allows a\nmodel entity representing a host manager to be delivered to the\nadaptation engine enabling it to access all metrics in that\ncomponent and any of its children (i.e. runtimes, and mobile\nobjects). Furthermore it would generally be expected that an\nadaptation engine or system controller would already maintain a\nmodel of the system that can not only be reused for propagation\nbut also provides an effective means of delivering metrics\ninformation from the host manager to the adaptation engine. The\nrelationship between model entities and metrics containers is\ncaptured in Figure 1.\n3.6 Propagation and Delivery Criteria\nThis subsection proposes flexible criteria to allow each\ncomponent to decide when it should propagate its metrics to the\nnext component in line (Figure 1), in order to reduce the overhead\nincurred when metrics are unnecessarily propagated through the\ncomponents of the mobility framework and delivered to the\nadaptation engine.\nThis paper proposes four different types of criterion that are\nexecuted at various stages of the measurement and propagation\nprocess in order to determine whether the next action should be\ntaken or not. This approach was designed such that whenever a\nsingle criterion is not satisfied, the subsequent criteria are not\ntested. These four criteria are described in the following\nsubsections.\nMeasure Metric Criterion - This criterion is attached to\nindividual Metric objects to decide whether a new metric value\nshould be measured or not. This is most useful in the case where it\nis expensive to measure a particular metric. Furthermore, this\ncriterion can be used as a mechanism for limiting storage\nrequirements and manipulation overhead in the case where metric\nhistory is maintained. Simple examples would be either time or\nfrequency based whereas more complex criteria could be domain\nspecific for a particular metric, or based upon information stored\nin the metrics history.\nNotify Metrics Container Criterion - This criterion is also\nattached to individual Metric objects and is used to determine\nthe circumstances under which the Metric object should notify its\nMetricsContainer. This is based on the assumption that\nthere may be cases where it is desirable to measure and store a\nmetric in the history for the analysis of temporal behaviour, but is\nnot yet significant enough to notify the MetricsContainer\nfor further processing.\nA simple example of this criterion would be threshold based\nin which the newest metric value is compared with the previously\nstored value to determine whether the difference is significant\nenough to be of any interest to the MetricsContainer. A\nmore complex criterion could involve analysis of the history to\ndetermine whether a pattern of recent changes is significant\nenough to warrant further processing and possible metrics\ndelivery.\nNotify Model Entity Criterion - Unlike the previous two\ncriteria, this criterion is associated with a MetricsContainer.\nSince a MetricsContainer can have multiple Metric\nobjects, of which it has explicit domain knowledge, it is able to\ndetermine if, when, and how many of these metrics should be\npropagated to the ModelEntity and thus become candidates\nfor being part of the hierarchical ModelEntity push process as\ndescribed below. This decision making is facilitated by the\nnotifications received from individual Metric objects as\ndescribed above.\nA simple implementation would be waiting for a certain\nnumber of updates before sending a notification to the model\nentity. For example, since the MobjectMetricsContainer\nobject contains three metrics, a possible criteria would be to check\nif two or more of the metrics have changed. A slightly more\nadvanced implementation can be done by giving each metric a\nweight to indicate how significant it is in the adaptation decision\nmaking process.\nPush Criterion - The push criterion applies to all of the\nModelEntites which are containers, that is the\nTransportManagerModelEntity,\nRuntimeModelEntity and ServiceModelEntity, as\nwell as the special case of the ProxyMetricsContainer.\nThe purpose of this criterion is twofold. For the\nTransportManagerModelEntity this serves as a criterion\nto determine notification since as with the previously described\ncriteria, a local reference is involved. For the other model entities,\nthis serves as an opportunity to determine both when and what\nmetrics should be pushed to the parent container wherein the case\nof the ServiceModelEntity the parent is the adaptation\nengine itself or in the case of the ProxyMetricsContainer\nthe target of the push is the MobjectMetricsContainer.\nFurthermore, this criterion is evaluated using information\nfrom two sources. Firstly, it responds to the notification received\nfrom its own MetricsContainer but more importantly it\nserves to keep track of notifications from its child\nModelEntities so as to determine when and what metrics\ninformation should be pushed to its parent or target. In the\nspecialised case of the push criterion for the proxy, the decision\nmaking is based on both the ProxyMetricsContainer itself,\nas well as the information accumulated from the individual\nProxyMethodMetricsContainers. Note that a push criterion\nis not required for a mobject since it does not have any\ncontainment or aggregating responsibilities since this is already\nService\nModel\nEntity\nService\nMetrics\nContainer\nNotify Model\nEntity Criterion\nRuntime\nModel\nEntity\nRuntime\nMetrics\nContainer\nNotify Model\nEntity Criterion\nTransport\nManager\nModel\nEntity\nTransport\nManager\nMetrics\nContainer\nNotify Model\nEntity Criterion\nPush\nCriterion\nMobject\nModel\nEntity\nMobject\nMethod\nMetrics\nNotify Model\nEntity Criterion\nPush\nCriterion\nPush\nCriterion\nTo adaptation\nengine\nMobject\nMetrics\nContainer\nNotify Metrics\nContainer\nCriterion\nMeasure\nMetric\nCriterion\nMetric 1\nNotifyMetrics\nContainer\nCriterion\nNotify Metrics\nContainer\nCriterion\nMeasure Metric\nCriterionProxyMethod\nMetrics\nContainers\nRT Metric\nNotify Metrics\nContainer\nCriterion\nProxyMetrics\nContainer\nPush\nCriterion\nMeasure\nMetric\nCriterion\nMetric 2\nMeasure Metric\nCriterion\nMetric 1\n1..n\nnot currently implemented\nNotify Metrics\nContainer\nCriterion\nMetric 1\nMetric 2\nMeasure\nMetric\nCriterion\nMeasure\nMetric\nCriterion\nNotify Metrics\nContainer\nCriterion\nMU Metric\nMeasure\nMetric\nCriterion\nNotify Metrics\nContainer\nCriterion\nET Metric\nIT Metric\nNI Metric\nMeasure\nMetric\nCriterion\nMeasure\nMetric\nCriterion\nMeasure\nMetric\nCriterion\nNotify Metrics\nContainer\nCriterion NU Metric\nPU Metric\nMeasure\nMetric\nCriterion\nMeasure\nMetric\nCriterion\n1..n\nFigure 1. Structural overview of the hierarchical and \ncriteriabased notification relationships between Metrics, Metrics\nContainers, and Model Entities\nhandled by the MobjectMetricsContainer and its\nindividual MobjectMethodMetricsContainers.\nAlthough it is always important to reduce the number of\npushes, this is especially so from a service to a centralised global\nadaptation engine, or from a proxy to a mobject. This is because\nthese relationships involve a remote call [18] which is expensive\ndue to connection setup and data marshalling and unmarshalling\noverhead, and thus it is more efficient to send a given amount of\ndata in aggregate form rather than sending smaller chunks\nmultiple times.\nA simple implementation for reducing the number of pushes\ncan be done using the concept of a process period [19] in which\ncase the model entity accumulates pushes from its child entities\nuntil the process period expires at which time it pushes the\naccumulated metrics to its parent. Alternatively it could be based\non frequency using domain knowledge about the type of children\nfor example when a significant number of mobjects in a particular\napplication (i.e. TransportManager) have undergone\nsubstantial changes.\nFor reducing the size of pushed data, two types of pushes\nwere considered: shallow push and deep push. With shallow push,\na list of metrics containers that contain updated metrics is pushed.\nIn a deep push, the model entity itself is pushed, along with its\nmetrics container and its child entities, which also have reference\nto metrics containers but possibly unchanged metrics. In the case\nof the proxy, a deep push involves pushing the\nProxyMetricsContainer and all of the\nProxyMethodMetricsContainers whereas a shallow push\nmeans only the ProxyMethodMetricsContainers that\nmeet a certain criterion.\n4. EVALUATION\nThe preliminary tests presented in this section aim to analyse\nthe performance and scalability of the solution and evaluate the\nimpact on application execution in terms of metrics collection\noverhead. All tests were executed using two Pentium 4 3.0 GHz\nPCs with 1,024 MB of RAM, running Java 1.4.2_08. The two\nmachines were connected to a router with a third computer acting\nas a file server and hosting the external adaptation engine\nimplemented within the MobJeX system controller, thereby\nsimulating a global adaptation scenario.\nSince only a limited number of tests could be executed, this\nevaluation chose to measure the worst case scenario in which all\nmetrics collection was initiated in mobjects, wherein the\npropagation cost is higher than for any other metrics collected in\nthe system. In addition, since exhaustive testing of criteria is\nbeyond the scope of this paper, two different types of criteria were\nused in the tests. The measure metrics criterion was chosen, since\nthis represents the starting point of the measurement process and\ncan control under what circumstances and how frequently metrics\nare measured. In addition, the push criterion was also\nimplemented on the service, in order to provide an evaluation of\ncontrolling the frequency of metrics delivery to the adaptation\nengine. All other (update and push) criteria were set to always\nmeaning that they always evaluated to true and thus a notification\nwas posted.\nFigure 2 shows the metric collection overhead in the mobject\n(MMCO), for different numbers of mobjects and methods when\nall criteria are set to always to provide the maximum measurement\nand propagation of metrics and thus an absolute worst case\nperformance scenario. It can be seen that the independent factors\nof increasing the number of mobjects and methods independently\nare linear. Although combining these together provides an\nexponential growth that is approximately n-squared, the initial\nresults are not discouraging since delivering all of the metrics\nassociated with 20 mobjects, each having 20 methods (which\nconstitutes quite a large application given that mobjects typically\nrepresent coarse grained object clusters) is approximately 400ms,\nwhich could reasonably be expected to be offset with adaptation\ngains. Note that in contrast, the proxy metrics collection overhead\n(PMCO) was relatively small and constant at < 5ms, since in the\nabsence of a proxy push criterion (this was only implemented on\nthe service) the response time (RT) data for a single method is\npushed during every invocation.\n50\n150\n250\n350\n450\n550\n1 5 10 15 20 25\nNumber of Mobjects/Methods\nMobjectMetricsCollectionOverheadMMCO(ms)\nMethods\nMobjects\nBoth\nFigure 2. Worst case performance characteristics\nThe next step was to determine the percentage metrics\ncollection overhead compared with execution time in order to\nprovide information about the execution characteristics of objects\nthat would be suitable for adaptation using this metric collection\napproach. Clearly, it is not practical to measure metrics and\nperform adaptation on objects with short execution times that\ncannot benefit from remote execution on hosts with greater\nprocessing power, thereby offsetting IT overhead of remote\ncompared with local execution as well as the cost of object\nmigration and the metrics collection process itself.\nIn addition, to demonstrate the effect of using simple\nfrequency based criteria, the MMCO results as a percentage of\nmethod execution time were plotted as a 3-dimensional graph in\nFigure 3 with the z-axis representing the frequency used in both\nthe measure metrics criterion and the service to adaptation engine\npush criterion. This means that for a frequency value of 5 (n=5),\nmetrics are only measured on every fifth method call, which then\nresults in a notification through the model entity hierarchy to the\nservice, on this same fifth invocation. Furthermore, the value of\nn=5 was also applied to the service push criterion so that metrics\nwere only pushed to the adaptation engine after five such\nnotifications, that is for example five different mobjects had\nupdated their metrics.\nThese results are encouraging since even for the worst case\nscenario of n=1 the metric collection overhead is an acceptable\n20% for a method of 1500ms duration (which is relatively short\nfor a component or service level object in a distributed enterprise\nclass application) with previous work on adaptation showing that\nsuch an overhead could easily be recovered by the efficiency gains\nmade by adaptation [5]. Furthermore, the measurement time\nincludes delivering the results synchronously via a remote call to\nthe adaptation engine on a different host, which would normally\nbe done asynchronously, thus further reducing the impact on\nmethod execution performance. The graph also demonstrates that\neven using modest criteria to reduce the metrics measurement to\nmore realistic levels, has a rapid improvement on collection\noverhead at 20% for 500ms of ET.\n0\n1000\n2000\n3000\n4000\n5000\n1\n2\n3\n4\n5\n6\n0\n20\n40\n60\n80\n100\n120\nMMCO (%)\nET (milliseconds) N (interval)\nMMCO (%)\nFigure 3. Performance characteristics with simple criteria\n5. SUMMARY AND CONCLUSIONS\nGiven the challenges of developing mobile applications that\nrun in dynamic/heterogeneous environments, and the subsequent\ninterest in application adaptation, this paper has proposed and\nimplemented an online metrics collection strategy to assist such\nadaptation using a mobile object framework and supporting\nmiddleware.\nControlled lab studies were conducted to determine worst case\nperformance, as well as show the reduction in collection overhead\nwhen applying simple collection criteria. In addition, further\ntesting provided an initial indication of the characteristics of\napplication objects (based on method execution time) that would\nbe good candidates for adaptation using the worst case\nimplementation of the proposed metrics collection strategy.\nA key feature of the solution was the specification of multiple\nconfigurable criteria to control the propagation of metrics through\nthe system, thereby reducing collection overhead. While the\npotentially efficacy of this approach was tested using simple\ncriteria, given the flexibility of the approach we believe there are\nmany opportunities to significantly reduce collection overhead\nthrough the use of more sophisticated criteria. One such approach\ncould be based on maintaining metrics history in order to\ndetermine the temporal behaviour of metrics and thus make more\nintelligent and conservative decisions regarding whether a change\nin a particular metric is likely to be of interest to the adaptation\nengine and should thus serve as a basis for notification for\ninclusion in the next metrics push. Furthermore, such a temporal\nhistory could also facilitate intelligent decisions regarding the\ncollection of metrics since for example a metric that is known to\nbe largely constant need not be frequently measured.\nFuture work will also involve the evaluation of a broad range\nof adaptation scenarios on the MobJeX framework to quantity the\ngains that can be made via adaptation through object mobility and\nthus demonstrate in practise, the efficacy of the solution described\nin this paper. Finally, the authors wish to explore applying the\nmetrics collection concepts described in this paper to a more\ngeneral and reusable context management system [20].\n6. REFERENCES\n1. Katz, R.H., Adaptation and Mobility in Wireless Information\nSystems. IEEE Personal Communications, 1994. 1: p. 6-17.\n2. Hirschfeld, R. and Kawamura, K. Dynamic Service Adaptation.\nin ICDCS Workshops\"04. 2004.\n3. Lemlouma, T. and Layaida, N. Context-Aware Adaptation for\nMobile Devices. in Proceedings of IEEE International\nConference on Mobile Data Management 2004. 2004.\n4. Noble, B.D., et al. Agile Application-Aware Adaptation for\nMobility. in Proc. of the 16th ACM Symposium on Operating\nSystems and Principles SOSP. 1997. Saint-Malo, France.\n5. Rossi, P. and Ryan, C. An Empirical Evaluation of Dynamic\nLocal Adaptation for Distributed Mobile Applications. in Proc.\nof 2005 International Symposium on Distributed Objects and\nApplications (DOA 2005). 2005. Larnaca, Cyprus: \nSpringerVerlag.\n6. Ryan, C. and Westhorpe, C. Application Adaptation through\nTransparent and Portable Object Mobility in Java. in\nInternational Symposium on Distributed Objects and\nApplications (DOA 2004). 2004. Larnaca, Cyprus: \nSpringerVerlag.\n7. da Silva e Silva, F.J., Endler, M., and Kon, F. Developing\nAdaptive Distributed Applications: A Framework Overview and\nExperimental Results. in On The Move to Meaningful Internet\nSystems 2003: CoopIS, DOA, and ODBASE (LNCS 2888). 2003.\n8. Rossi, P. and Fernandez, G. Definition and validation of design\nmetrics for distributed applications. in Ninth International\nSoftware Metrics Symposium. 2003. Sydney: IEEE.\n9. Ryan, C. and Rossi, P. Software, Performance and Resource\nUtilisation Metrics for Context Aware Mobile Applications. in\nProceedings of International Software Metrics Symposium IEEE\nMetrics 2005. 2005. Como, Italy.\n10. Recursion Software Inc. Voyager URL:\nhttp://www.recursionsw.com/voyager.htm. 2005.\n11. Holder, O., Ben-Shaul, I., and Gazit, H., System Support for\nDynamic Layout of Distributed Applications. 1998, \nTechinonIsrael Institute of Technology. p. 163 - 173.\n12. Holder, O., Ben-Shaul, I., and Gazit, H. Dynamic Layout of\nDistributed Applications in FarGo. in 21st Int'l Conf. Software\nEngineering (ICSE'99). 1999: ACM Press.\n13. Philippsen, M. and Zenger, M., JavaParty - Transparent Remote\nObjects in Java. Concurrency: Practice and Experience, 1997.\n9(11): p. 1225-1242.\n14. Shapiro, M. Structure and Encapsulation in Distributed Systems:\nthe Proxy Principle. in Proc.6th Intl. Conference on Distributed\nComputing Systems. 1986. Cambridge, Mass. (USA): IEEE.\n15. Gazit, H., Ben-Shaul, I., and Holder, O. Monitoring-Based\nDynamic Relocation of Components in Fargo. in Proceedings of\nthe Second International Symposium on Agent Systems and\nApplications and Fourth International Symposium on Mobile\nAgents. 2000.\n16. Lindholm, T. and Yellin, F., The Java Virtual Machine\nSpecification 2nd Edition. 1999: Addison-Wesley.\n17. Randell, L.G., Holst, L.G., and Bolmsj\u00c3\u00b6, G.S. Incremental System\nDevelopment of Large Discrete-Event Simulation Models. in\nProceedings of the 31st conference on Winter Simulation. 1999.\nPhoenix, Arizona.\n18. Waldo, J., Remote Procedure Calls and Java Remote Method\nInvocation. IEEE Concurrency, 1998. 6(3): p. 5-7.\n19. Rolia, J. and Lin, B. Consistency Issues in Distributed\nApplication Performance Metrics. in Proceedings of the 1994\nConference of the Centre for Advanced Studies on Collaborative\nResearch. 1994. Toronto, Canada.\n20. Henricksen, K. and Indulska, J. A software engineering\nframework for context-aware pervasive computing. in\nProceedings of the 2nd IEEE Conference on Pervasive\nComputing and Communications (PerCom). 2004. Orlando.\n": ["data", "object-oriented application", "mobile object framework", "mobjex", "java", "metricscontainer", "metric collection", "proxy", "performance and scalability", "measurement", "propagation and delivery", "framework", "adaptation", "mobile object", ""]}