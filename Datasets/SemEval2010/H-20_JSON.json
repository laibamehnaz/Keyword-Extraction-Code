{"New Event Detection Based on Indexing-tree\nand Named Entity\nZhang Kuo\nTsinghua University\nBeijing, 100084, China\n86-10-62771736\nzkuo99@mails.tsinghua.edu.cn\nLi Juan Zi\nTsinghua University\nBeijing, 100084, China\n86-10-62781461\nljz@keg.cs.tsinghua.edu.cn\nWu Gang\nTsinghua University\nBeijing, 100084, China\n86-10-62789831\nwug03@keg.cs.tsinghua.edu.cn\nABSTRACT\nNew Event Detection (NED) aims at detecting from one or\nmultiple streams of news stories that which one is reported on a\nnew event (i.e. not reported previously). With the overwhelming\nvolume of news available today, there is an increasing need for a\nNED system which is able to detect new events more efficiently\nand accurately. In this paper we propose a new NED model to\nspeed up the NED task by using news indexing-tree dynamically.\nMoreover, based on the observation that terms of different types\nhave different effects for NED task, two term reweighting\napproaches are proposed to improve NED accuracy. In the first\napproach, we propose to adjust term weights dynamically based\non previous story clusters and in the second approach, we propose\nto employ statistics on training data to learn the named entity\nreweighting model for each class of stories. Experimental results\non two Linguistic Data Consortium (LDC) datasets TDT2 and\nTDT3 show that the proposed model can improve both efficiency\nand accuracy of NED task significantly, compared to the baseline\nsystem and other existing systems.\nCategories and Subject Descriptors\nH.3.3 [Information Systems]: Information Search and Retrieval;\nH.4.2 [Information Systems Applications]: Types of \nSystemsdecision support.\nGeneral Terms\nAlgorithms, Performance, Experimentation\n1. INTRODUCTION\nTopic Detection and Tracking (TDT) program aims to develop\ntechniques which can effectively organize, search and structure\nnews text materials from a variety of newswire and broadcast\nmedia [1]. New Event Detection (NED) is one of the five tasks in\nTDT. It is the task of online identification of the earliest report for\neach topic as soon as that report arrives in the sequence of\ndocuments. A Topic is defined as a seminal event or activity,\nalong with directly related events and activities [2]. An Event is\ndefined as something (non-trivial) happening in a certain place at\na certain time [3]. For instance, when a bomb explodes in a\nbuilding, the exploding is the seminal event that triggers the topic,\nand other stories on the same topic would be those discussing\nsalvaging efforts, the search for perpetrators, arrests and trial and\nso on. Useful news information is usually buried in a mass of data\ngenerated everyday. Therefore, NED systems are very useful for\npeople who need to detect novel information from real-time news\nstream. These real-life needs often occur in domains like financial\nmarkets, news analysis, and intelligence gathering.\nIn most of state-of-the-art (currently) NED systems, each news\nstory on hand is compared to all the previous received stories. If\nall the similarities between them do not exceed a threshold, then\nthe story triggers a new event. They are usually in the form of\ncosine similarity or Hellinger similarity metric. The core problem\nof NED is to identify whether two stories are on the same topic.\nObviously, these systems cannot take advantage of topic\ninformation. Further more, it is not acceptable in real applications\nbecause of the large amount of computation required in the NED\nprocess. Other systems organize previous stories into clusters\n(each cluster corresponds to a topic), and new story is compared to\nthe previous clusters instead of stories. This manner can reduce\ncomparing times significantly. Nevertheless, it has been proved\nthat this manner is less accurate [4, 5]. This is because sometimes\nstories within a topic drift far away from each other, which could\nlead low similarity between a story and its topic.\nOn the other hand, some proposed NED systems tried to improve\naccuracy by making better use of named entities [10, 11, 12, 13].\nHowever, none of the systems have considered that terms of\ndifferent types (e.g. Noun, Verb or Person name) have different\neffects for different classes of stories in determining whether two\nstories are on the same topic. For example, the names of election\ncandidates (Person name) are very important for stories of election\nclass; the locations (Location name) where accidents happened are\nimportant for stories of accidents class.\nSo, in NED, there still exist following three problems to be\ninvestigated: (1) How to speed up the detection procedure while\ndo not decrease the detection accuracy? (2) How to make good\nuse of cluster (topic) information to improve accuracy? (3) How to\nobtain better news story representation by better understanding of\nnamed entities.\nDriven by these problems, we have proposed three approaches in\nthis paper. (1)To make the detection procedure faster, we propose\na new NED procedure based on news indexing-tree created\ndynamically. Story indexing-tree is created by assembling similar\nstories together to form news clusters in different hierarchies\naccording to their values of similarity. Comparisons between\ncurrent story and previous clusters could help find the most\nsimilar story in less comparing times. The new procedure can\nreduce the amount of comparing times without hurting accuracy.\n(2)We use the clusters of the first floor in the indexing-tree as\nnews topics, in which term weights are adjusted dynamically\naccording to term distribution in the clusters. In this approach,\ncluster (topic) information is used properly, so the problem of\ntheme decentralization is avoided. (3)Based on observations on\nthe statistics obtained from training data, we found that terms of\ndifferent types (e.g. Noun and Verb) have different effects for\ndifferent classes of stories in determining whether two stories are\non the same topic. And we propose to use statistics to optimize the\nweights of the terms of different types in a story according to the\nnews class that the story belongs to. On TDT3 dataset, the new\nNED model just uses 14.9% comparing times of the basic model,\nwhile its minimum normalized cost is 0.5012, which is 0.0797\nbetter than the basic model, and also better than any other results\npreviously reported for this dataset [8, 13].\nThe rest of the paper is organized as follows. We start off this\npaper by summarizing the previous work in NED in section 2.\nSection 3 presents the basic model for NED that most current\nsystems use. Section 4 describes our new detection procedure\nbased on news indexing-tree. In section 5, two term reweighting\nmethods are proposed to improve NED accuracy. Section 6 gives\nour experimental data and evaluation metrics. We finally wrap up\nwith the experimental results in Section 7, and the conclusions and\nfuture work in Section 8.\n2. RELATED WORK\nPapka et al. proposed Single-Pass clustering on NED [6]. When a\nnew story was encountered, it was processed immediately to\nextract term features and a query representation of the story\"s\ncontent is built up. Then it was compared with all the previous\nqueries. If the document did not trigger any queries by exceeding\na threshold, it was marked as a new event. Lam et al build up\nprevious query representations of story clusters, each of which\ncorresponds to a topic [7]. In this manner comparisons happen\nbetween stories and clusters.\nRecent years, most work focus on proposing better methods on\ncomparison of stories and document representation. Brants et al.\n[8] extended a basic incremental TF-IDF model to include \nsourcespecific models, similarity score normalization based on\ndocument-specific averages, similarity score normalization based\non source-pair specific averages, term reweighting based on\ninverse event frequencies, and segmentation of documents. Good\nimprovements on TDT bench-marks were shown. Stokes et al. [9]\nutilized a combination of evidence from two distinct\nrepresentations of a document\"s content. One of the\nrepresentations was the usual free text vector, the other made use\nof lexical chains (created using WordNet) to build another term\nvector. Then the two representations are combined in a linear\nfashion. A marginal increase in effectiveness was achieved when\nthe combined representation was used.\nSome efforts have been done on how to utilize named entities to\nimprove NED. Yang et al. gave location named entities four times\nweight than other terms and named entities [10]. DOREMI\nresearch group combined semantic similarities of person names,\nlocation names and time together with textual similarity [11][12].\nUMass [13] research group split document representation into two\nparts: named entities and non-named entities. And it was found\nthat some classes of news could achieve better performance using\nnamed entity representation, while some other classes of news\ncould achieve better performance using non-named entity\nrepresentation. Both [10] and [13] used text categorization\ntechnique to classify news stories in advance. In [13] news stories\nare classified automatically at first, and then test sensitivities of\nnames and non-name terms for NED for each class. In [10]\nfrequent terms for each class are removed from document\nrepresentation. For example, word election does not help\nidentify different elections. In their work, effectiveness of\ndifferent kinds of names (or terms with different POS) for NED in\ndifferent news classes are not investigated. We use statistical\nanalysis to reveal the fact and use it to improve NED performance.\n3. BASIC MODEL\nIn this section, we present the basic New Event Detection model\nwhich is similar to what most current systems apply. Then, we\npropose our new model by extending the basic model.\nNew Event Detection systems use news story stream as input, in\nwhich stories are strictly time-ordered. Only previously received\nstories are available when dealing with current story. The output is\na decision for whether the current story is on a new event or not\nand the confidence of the decision. Usually, a NED model consists\nof three parts: story representation, similarity calculation and\ndetection procedure.\n3.1 Story Representation\nPreprocessing is needed before generating story representation.\nFor preprocessing, we tokenize words, recognize abbreviations,\nnormalize abbreviations, add part-of-speech tags, remove\nstopwords included in the stop list used in InQuery [14], replace\nwords with their stems using K-stem algorithm[15], and then\ngenerate word vector for each news story.\nWe use incremental TF-IDF model for term weight calculation\n[4]. In a TF-IDF model, term frequency in a news document is\nweighted by the inverse document frequency, which is generated\nfrom training corpus. When a new term occurs in testing process,\nthere are two solutions: simply ignore the new term or set df of the\nterm as a small const (e.g. df = 1). The new term receives too low\nweight in the first solution (0) and too high weight in the second\nsolution. In incremental TF-IDF model, document frequencies are\nupdated dynamically in each time step t:\n1( ) ( ) ( )t t D tdf w df w df w\u00e2\u02c6\u2019= + (1)\nwhere Dt represents news story set received in time t, and dfDt(w)\nmeans the number of documents that term w occurs in, and dft(w)\nmeans the total number of documents that term w occurs in before\ntime t. In this work, each time window includes 50 news stories.\nThus, each story d received in t is represented as follows:\n1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w\u00e2\u2020\u2019\nwhere n means the number of distinct terms in story d, and\n( , , )weight d t w means the weight of term w in story d at time t:\n'\nlog( ( , ) 1) log(( 1) /( ( ) 0.5))\n( , , )\nlog( ( , ') 1) log(( 1) /( ( ') 0.5))\nt t\nt t\nw d\ntf d w N df w\nweight d t w\ntf d w N df w\n\u00e2\u02c6\u02c6\n+ + +\n=\n+ + +\u00e2\u02c6\u2018\n(2)\nwhere Nt means the total number of news stories before time t, and\ntf(d,w) means how many times term w occurs in news story d.\n3.2 Similarity Calculation\nWe use Hellinger distance for the calculation of similarity\nbetween two stories, for two stories d and d\" at time t, their\nsimilarity is defined as follows:\n, '\n( , ', ) ( , , ) * ( ', , )\nw d d\nsim d d t weight d t w weight d t w\n\u00e2\u02c6\u02c6\n= \u00e2\u02c6\u2018 (3)\n3.3 Detection Procedure\nFor each story d received in time step t, the value\n( ') ( )\n( ) ( ( , ', ))\ntime d time d\nn d max sim d d t\n<\n= (4)\nis a score used to determine whether d is a story about a new topic\nand at the same time is an indication of the confidence in our\ndecision [8]. time(d) means the publication time of story d. If the\nscore exceeds the threshold\u00ce\u00b8 new, then there exists a sufficiently\nsimilar document, thus d is a old story, otherwise, there is no\nsufficiently similar previous document, thus d is an new story.\n4. New NED Procedure\nTraditional NED systems can be classified into two main types on\nthe aspect of detection procedure: (1) S-S type, in which the story\non hand is compared to each story received previously, and use\nthe highest similarity to determine whether current story is about a\nnew event; (2) S-C type, in which the story on hand is compared\nto all previous clusters each of which representing a topic, and the\nhighest similarity is used for final decision for current story. If the\nhighest similarity exceeds threshold\u00ce\u00b8 new, then it is an old story,\nand put it into the most similar cluster; otherwise it is a new story\nand create a new cluster. Previous work show that the first manner\nis more accurate than the second one [4][5]. Since sometimes\nstories within a topic drift far away from each other, a story may\nhave very low similarity with its topic. So using similarities\nbetween stories for determining new story is better than using\nsimilarities between story and clusters. Nevertheless, the first\nmanner needs much more comparing times which means the first\nmanner is low efficient. We propose a new detection procedure\nwhich uses comparisons with previous clusters to help find the\nmost similar story in less comparing times, and the final new\nevent decision is made according to the most similar story.\nTherefore, we can get both the accuracy of S-S type methods and\nthe efficiency of S-C type methods.\nThe new procedure creates a news indexing-tree dynamically, in\nwhich similar stories are put together to form a hierarchy of\nclusters. We index similar stories together by their common\nancestor (a cluster node). Dissimilar stories are indexed in\ndifferent clusters. When a story is coming, we use comparisons\nbetween the current story and previous hierarchical clusters to\nhelp find the most similar story which is useful for new event\ndecision. After the new event decision is made, the current story is\ninserted to the indexing-tree for the following detection.\nThe news indexing-tree is defined formally as follows:\nS-Tree = {r, NC\n, NS\n, E}\nwhere r is the root of S-Tree, NC\nis the set of all cluster nodes, NS\nis the set of all story nodes, and E is the set of all edges in S-Tree.\nWe define a set of constraints for a S-Tree:\n\u00e2\u2026\u00b0 . , is an non-terminal node in the treeC\ni i N i\u00e2\u02c6\u20ac \u00e2\u02c6\u02c6 \u00e2\u2020\u2019\n\u00e2\u2026\u00b1 . , is a terminal node in the treeS\ni i N i\u00e2\u02c6\u20ac \u00e2\u02c6\u02c6 \u00e2\u2020\u2019\n\u00e2\u2026\u00b2 . , out degree of is at least 2C\ni i N i\u00e2\u02c6\u20ac \u00e2\u02c6\u02c6 \u00e2\u2020\u2019\n\u00e2\u2026\u00b3 . , is represented as the centroid of its desendantsC\ni i iN\u00e2\u02c6\u20ac \u00e2\u02c6\u02c6 \u00e2\u2020\u2019\nFor a news story di, the comparison procedure and inserting\nprocedure based on indexing-tree are defined as follows. An\nexample is shown by Figure 1 and Figure 2.\nFigure 1. Comparison procedure\nFigure 2. Inserting procedure\nComparison procedure:\nStep 1: compare di to all the direct child nodes of r and select \u00ce\u00bb\nnodes with highest similarities, e.g., C1\n2 and C1\n3 in Figure 1.\nStep 2: for each selected node in the last step, e.g. C1\n2, compare di\nto all its direct child nodes, and select \u00ce\u00bb nodes with highest\nsimilarities, e.g. C2\n2 and d8. Repeat step 2 for all non-terminal\nnodes.\nStep 3: record the terminal node with the highest similarty to di,\ne.g. s5, and the similarity value (0.20).\nInserting di to the S-tree with r as root:\nFind the node n which is direct child of r in the path from r to the\nterminal node with highest similarity s, e.g. C1\n2. If s is smaller\nthan \u00ce\u00b8 init+(h-1)\u00ce\u00b4 , then add di to the tree as a direct child of r.\nOtherwise, if n is a terminal node, then create a cluster node\ninstead of n, and add both n and di as its direct children; if n is an\nnon-terminal node, then repeat this procedure and insert di to the\nsub-tree with n as root recursively. Here h is the length between n\nand the root of S-tree.\nThe more the stories in a cluster similar to each other, the better\nthe cluster represents the stories in it. Hence we add no constraints\non the maximum of tree\"s height and degree of a node. Therefore,\nwe cannot give the complexity of this indexing-tree based\nprocedure. But we will give the number of comparing times\nneeded by the new procedure in our experiments in section7.\n5. Term Reweighting Methods\nIn this section, two term reweighting methods are proposed to\nimprove NED accuracy. In the first method, a new way is\nexplored for better using of cluster (topic) information. The\nsecond one finds a better way to make use of named entities based\non news classification.\n5.1 Term Reweighting Based on Distribution\nDistance\nTF-IDF is the most prevalent model used in information retrieval\nsystems. The basic idea is that the fewer documents a term\nappears in, the more important the term is in discrimination of\ndocuments (relevant or not relevant to a query containing the\nterm). Nevertheless, in TDT domain, we need to discriminate\ndocuments with regard to topics rather than queries. Intuitively,\nusing cluster (topic) vectors to compare with subsequent news\nstories should outperform using story vectors. Unfortunately, the\nexperimental results do not support this intuition [4][5]. Based on\nobservation on data, we find the reason is that a news topic\nusually contains many directly or indirectly related events, while\nthey all have their own sub-subjects which are usually different\nwith each other. Take the topic described in section 1 as an\nexample, events like the explosion and salvage have very low\nsimilarities with events about criminal trial, therefore stories about\ntrial would have low similarity with the topic vector built on its\nprevious events. This section focuses on how to effectively make\nuse of topic information and at the same time avoid the problem of\ncontent decentralization.\nAt first, we classify terms into 5 classes to help analysis the needs\nof the modified model:\nTerm class A: terms that occur frequently in the whole corpus,\ne.g., year and people. Terms of this class should be given low\nweights because they do not help much for topic discrimination.\nTerm class B: terms that occur frequently within a news category,\ne.g., election, storm. They are useful to distinguish two stories\nin different news categories. However, they cannot provide\ninformation to determine whether two stories are on the same or\ndifferent topics. In another words, term election and term\nstorm are not helpful in differentiate two election campaigns\nand two storm disasters. Therefore, terms of this class should be\nassigned lower weights.\nTerm class C: terms that occur frequently in a topic, and\ninfrequently in other topics, e.g., the name of a crash plane, the\nname of a specific hurricane. News stories that belong to different\ntopics rarely have overlap terms in this class. The more frequently\na term appears in a topic, the more important the term is for a\nstory belonging to the topic, therefore the term should be set\nhigher weight.\nTerm class D: terms that appear in a topic exclusively, but not\nfrequently. For example, the name of a fireman who did very well\nin a salvage action, which may appears in only two or three stories\nbut never appeared in other topics. Terms of this type should\nreceive more weights than in TF-IDF model. However, since they\nare not popular in the topic, it is not appropriate to give them too\nhigh weights.\nTerm class E: terms with low document frequency, and appear in\ndifferent topics. Terms of this class should receive lower weights.\nNow we analyze whether TF-IDF model can give proper weights\nto the five classes of terms. Obviously, terms of class A are lowly\nweighted in TF-IDF model, which is conformable with the\nrequirement described above. In TF-IDF model, terms of class B\nare highly dependant with the number of stories in a news class.\nTF-IDF model cannot provide low weights if the story containing\nthe term belongs to a relative small news class. For a term of class\nC, the more frequently it appears in a topic, the less weight \nTFIDF model gives to it. This strongly conflicts with the requirement\nof terms in class C. For terms of class D, TF-IDF model gives\nthem high weights correctly. But for terms of class E, TF-IDF\nmodel gives high weights to them which are not conformable with\nthe requirement of low weights. To sum up, terms of class B, C, E\ncannot be properly weighted in TF-IDF model. So, we propose a\nmodified model to resolve this problem.\nWhen \u00ce\u00b8 init and\u00ce\u00b8 new are set closely, we assume that most of the\nstories in a first-level cluster (a direct child node of root node) are\non the same topic. Therefore, we make use of a first-level cluster\nto capture term distribution (df for all the terms within the cluster)\nwithin the topic dynamically. KL divergence of term distribution\nin a first-level cluster and the whole story set is used to adjust\nterm weights:\n' '\n'\n( , , ) * (1 * ( || ))\n( , , )\n( , , ') * (1 * ( || ))\ncw tw\ncw tw\nw d\nD\nweight d t w KL P P\nweight d t w\nweight d t w KL P P\n\u00ce\u00b3\n\u00ce\u00b3\n\u00e2\u02c6\u02c6\n+\n=\n+\u00e2\u02c6\u2018\n(5)\nwhere\n( ) ( )\n( ) ( ) 1,cw cw\nc c\nc c\ndf w df w\np y p y\nN N\n= = \u00e2\u02c6\u2019 (6)\n( ) ( )\n( ) ( ) 1,t t\ntw tw\nt t\ndf w df w\np y p y\nN N\n= = \u00e2\u02c6\u2019 (7)\nwhere dfc(w) is the number of documents containing term w\nwithin cluster C, and Nc is the number of documents in cluster C,\nand Nt is the total number of documents that arrive before time\nstep t. \u00ce\u00b3 is a const parameter, now is manually set 3.\nKL divergence is defined as follows [17]:\n( )\n( || ) ( ) log\n( )x\np x\nKL P Q p x\nq x\n= \u00e2\u02c6\u2018 (8)\nThe basic idea is: for a story in a topic, the more a term occurs\nwithin the topic, and the less it occurs in other topics, it should be\nassigned higher weights. Obviously, modified model can meet all\nthe requirements of the five term classes listed above.\n5.2 Term Reweighting Based on Term Type\nand Story Class\nPrevious work found that some classes of news stories could\nachieve good improvements by giving extra weight to named\nentities. But we find that terms of different types should be given\ndifferent amount of extra weight for different classes of news\nstories.\nWe use open-NLP1\nto recognize named entity types and \npart-ofspeech tags for terms that appear in news stories. Named entity\ntypes include person name, organization name, location name,\ndate, time, money and percentage, and five POSs are selected:\nnone (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal\nnumber (CD). Statistical analysis shows topic-level discriminative\nterms types for different classes of stories. For the sake of\nconvenience, named entity type and part-of-speech tags are\nuniformly called term type in subsequent sections.\nDetermining whether two stories are about the same topic is a\nbasic component for NED task. So at first we use 2\n\u00cf\u2021 statistic to\ncompute correlations between terms and topics. For a term t and a\ntopic T, a contingence table is derived:\nTable 1. A 2\u00c3\u20142 Contingence Table\nDoc Number\nbelong to\ntopic T\nnot belong to\ntopic T\ninclude t A B\nnot include t C D\nThe 2\n\u00cf\u2021 statistic for a specific term t with respect to topic T is\ndefined to be [16]:\n2\n2\n( , )\n( ) * ( )\n( ) * ( ) * ( ) * ( )\nw T\nA B C D AD CB\nA C B D A B C D\n\u00cf\u2021 =\n+ + + \u00e2\u02c6\u2019\n+ + + +\n(9)\nNews topics for the TDT task are further classified into 11 rules\nof interpretations (ROIs) 2\n. The ROI can be seen as a higher level\nclass of stories. The average correlation between a term type and a\ntopic ROI is computed as:\n2\navg\n2\n( , )( ( , ) )k m\nm km kT R w P\nw TP R p w T\nR P\n\u00cf\u2021 \u00cf\u2021\n\u00e2\u02c6\u02c6 \u00e2\u02c6\u02c6\n\u00e2\u02c6\u2018 \u00e2\u02c6\u2018\u00ef\u00bc\u02c6 , \u00ef\u00bc\u2030=\n1 1\nk=1\u00e2\u20ac\u00a6K, m=1\u00e2\u20ac\u00a6M (10)\nwhere K is the number of term types (set 12 constantly in the\npaper). M is the number news classes (ROIs, set 11 in the paper).\nPk represents the set of all terms of type k, and Rm represents the\nset of all topics of class m, p(t,T) means the probability that t\noccurs in topic T. Because of limitation of space, only parts of the\nterm types (9 term types) and parts of news classes (8 classes) are\nlisted in table 2 with the average correlation values between them.\nThe statistics is derived from labeled data in TDT2 corpus.\n(Results in table 2 are already normalized for convenience in\ncomparison.)\nThe statistics in table 2 indicates the usefulness of different term\ntypes in topic discrimination with respect to different news\nclasses. We can see that, location name is the most useful term\ntype for three news classes: Natural Disasters, Violence or War,\nFinances. And for three other categories Elections, Legal/Criminal\nCases, Science and Discovery, person name is the most\ndiscriminative term type. For Scandals/Hearings, date is the most\nimportant information for topic discrimination. In addition,\nLegal/Criminal Cases and Finance topics have higher correlation\nwith money terms, while Science and Discovery have higher\ncorrelation with percentage terms. Non-name terms are more\nstable for different classes.\n1\n. http://opennlp.sourceforge.net/\n2\n. http://projects.ldc.upenn.edu/TDT3/Guide/label.html\nFrom the analysis of table 2, it is reasonable to adjust term weight\naccording to their term type and the news class the story belongs\nto. New term weights are reweighted as follows:\n( )\n( )\n( )\n( ')\n'\n( , , ) *\n( , , )\n( , , ) *'\nclass d\nD type w\nT class d\nD type w\nw d\nweight d t w\nweight d t w\nweight d t w\n\u00ce\u00b1\n\u00ce\u00b1\n\u00e2\u02c6\u02c6\n=\n\u00e2\u02c6\u2018\n(11)\nwhere type(w) represents the type of term w, and class(d)\nrepresents the class of story d, c\nk\u00ce\u00b1 is reweighting parameter for\nnews class c and term type k. In the work, we just simply use\nstatistics in table 2 as the reweighting parameters. Even thought\nusing the statistics directly may not the best choice, we do not\ndiscuss how to automatically obtain the best parameters. We will\ntry to use machine learning techniques to obtain the best\nparameters in the future work.\nIn the work, we use BoosTexter [20] to classify all stories into one\nof the 11 ROIs. BoosTexter is a boosting based machine learning\nprogram, which creates a series of simple rules for building a\nclassifier for text or attribute-value data. We use term weight\ngenerated using TF-IDF model as feature for story classification.\nWe trained the model on the 12000 judged English stories in\nTDT2, and classify the rest of the stories in TDT2 and all stories\nin TDT3. Classification results are used for term reweighting in\nformula (11). Since the class labels of topic-off stories are not\ngiven in TDT datasets, we cannot give the classification accuracy\nhere. Thus we do not discuss the effects of classification accuracy\nto NED performance in the paper.\n6. EXPERIMENTAL SETUP\n6.1 Datasets\nWe used two LDC [18] datasets TDT2 and TDT3 for our\nexperiments. TDT2 contains news stories from January to June\n1998. It contains around 54,000 stories from sources like ABC,\nAssociated Press, CNN, New York Times, Public Radio\nInternational, Voice of America etc. Only English stories in the\ncollection were considered. TDT3 contains approximately 31,000\nEnglish stories collected from October to December 1998. In\naddition to the sources used in TDT2, it also contains stories from\nNBC and MSNBC TV broadcasts. We used transcribed versions\nof the TV and radio broadcasts besides textual news.\nTDT2 dataset is labeled with about 100 topics, and approximately\n12,000 English stories belong to at least one of these topics. TDT3\ndataset is labeled with about 120 topics, and approximately 8000\nEnglish stories belong to at least one of these topics. All the topics\nare classified into 11 Rules of Interpretation: (1)Elections,\n(2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural\nDisasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science\nand Discovery News, (8)Finance, (9)New Law, (10)Sports News,\n(11)MISC. News.\n6.2 Evaluation Metric\nTDT uses a cost function CDet that combines the probabilities of\nmissing a new story and a false alarm [19]:\n* * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12)\nTable 2. Average correlation between term types and news classes\nwhere CMiss means the cost of missing a new story, PMiss means\nthe probability of missing a new story, and PTarget means the\nprobability of seeing a new story in the data; CFA means the cost\nof a false alarm, PFA means the probability of a false alarm, and\nPNontarget means the probability of seeing an old story. The cost\nCDet is normalized such that a perfect system scores 0 and a trivial\nsystem, which is the better one of mark all stories as new or old,\nscores 1:\n(\n( * , * )\n)\nDet\nDet\nMiss Target FA Nontarget\nC\nNorm C\nmin C P C P\n= (13)\nNew event detection system gives two outputs for each story. The\nfirst part is yes or no indicating whether the story triggers a\nnew event or not. The second part is a score indicating confidence\nof the first decision. Confidence scores can be used to plot DET\ncurve, i.e., curves that plot false alarm vs. miss probabilities.\nMinimum normalized cost can be determined if optimal threshold\non the score were chosen.\n7. EXPERIMENTAL RESULTS\n7.1 Main Results\nTo test the approaches proposed in the model, we implemented\nand tested five systems:\nSystem-1: this system is used as baseline. It is implemented based\non the basic model described in section 3, i.e., using incremental\nTF-IDF model to generate term weights, and using Hellinger\ndistance to compute document similarity. Similarity score\nnormalization is also employed [8]. S-S detection procedure is\nused.\nSystem-2: this system is the same as system-1 except that S-C\ndetection procedure is used.\nSystem-3: this system is the same as system-1 except that it uses\nthe new detection procedure which is based on indexing-tree.\nSystem-4: implemented based on the approach presented in\nsection 5.1, i.e., terms are reweighted according to the distance\nbetween term distributions in a cluster and all stories. The new\ndetection procedure is used.\nSystem-5: implemented based on the approach presented in\nsection 5.2, i.e., terms of different types are reweighted according\nto news class using trained parameters. The new detection\nprocedure is used.\nThe following are some other NED systems:\nSystem-6: [21] for each pair of stories, it computes three\nsimilarity values for named entity, non-named entity and all terms\nrespectively. And employ Support Vector Machine to predict\nnew or old using the similarity values as features.\nSystem-7: [8] it extended a basic incremental TF-IDF model to\ninclude source-specific models, similarity score normalization\nbased on document-specific averages, similarity score\nnormalization based on source-pair specific averages, etc.\nSystem-8: [13] it split document representation into two parts:\nnamed entities and non-named entities, and choose one effective\npart for each news class.\nTable 3 and table 4 show topic-weighted normalized costs and\ncomparing times on TDT2 and TDT3 datasets respectively. Since\nno heldout data set for fine-tuning the threshold \u00ce\u00b8 new was\navailable for experiments on TDT2, we only report minimum\nnormalized costs for our systems in table 3. System-5 outperforms\nall other systems including system-6, and it performs only\n2.78e+8 comparing times in detection procedure which is only\n13.4% of system-1.\nTable 3. NED results on TDT2\nSystems Min Norm(CDet) Cmp times\nSystem-1 0.5749 2.08e+9\nSystem-2\u00e2\u2018\u00a0 0.6673 3.77e+8\nSystem-3\u00e2\u2018\u00a1 0.5765 2.81e+8\nSystem-4\u00e2\u2018\u00a1 0.5431 2.99e+8\nSystem-5\u00e2\u2018\u00a1 0.5089 2.78e+8\nSystem-6 0.5300 \n-\u00e2\u2018\u00a0 \u00ce\u00b8 new=0.13\n\u00e2\u2018\u00a1 \u00ce\u00b8 init=0.13, \u00ce\u00bb =3,\u00ce\u00b4 =0.15\nWhen evaluating on the normalized costs on TDT3, we use the\noptimal thresholds obtained from TDT2 data set for all systems.\nSystem-2 reduces comparing times to 1.29e+9 which is just\n18.3% of system-1, but at the same time it also gets a deteriorated\nminimum normalized cost which is 0.0499 higher than system-1.\nSystem-3 uses the new detection procedure based on news\nindexing-tree. It requires even less comparing times than system-2.\nThis is because story-story comparisons usually yield greater\nsimilarities than story-cluster ones, so stories tend to be combined\nLocation Person Date Organization Money Percentage NN JJ CD\nElections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1\nScandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05\nLegal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09\nNatural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02\nViolence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02\nScience and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03\nFinances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05\nSports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01\ntogether in system-3. And system-3 is basically equivalent to\nsystem-1 in accuracy results. System-4 adjusts term weights based\non the distance of term distributions between the whole corpus\nand cluster story set, yielding a good improvement by 0.0468\ncompared to system-1. The best system (system-5) has a\nminimum normalized cost 0.5012, which is 0.0797 better than\nsystem-1, and also better than any other results previously\nreported for this dataset [8, 13]. Further more, system-5 only\nneeds 1.05e+8 comparing times which is 14.9% of system-1.\nTable 4. NED results on TDT3\nSystems Norm(CDet) Min Norm(CDet) Cmp times\nSystem-1 0.6159 0.5809 7.04e+8\nSystem-2\u00e2\u2018\u00a0 0.6493 0.6308 1.29e+8\nSystem-3\u00e2\u2018\u00a1 0.6197 0.5868 1.03e+8\nSystem-4\u00e2\u2018\u00a1 0.5601 0.5341 1.03e+8\nSystem-5\u00e2\u2018\u00a1 0.5413 0.5012 1.05e+8\nSystem-7 -- 0.5783 \n-System-8 -- 0.5229 \n-\u00e2\u2018\u00a0 \u00ce\u00b8 new=0.13\n\u00e2\u2018\u00a1 \u00ce\u00b8 init=0.13, \u00ce\u00bb =3,\u00ce\u00b4 =0.15\nFigure5 shows the five DET curves for our systems on data set\nTDT3. System-5 achieves the minimum cost at a false alarm rate\nof 0.0157 and a miss rate of 0.4310. We can observe that \nSystem4 and System-5 obtain lower miss probability at regions of low\nfalse alarm probabilities. The hypothesis is that, more weight\nvalue is transferred to key terms of topics from non-key terms.\nSimilarity score between two stories belonging to different topics\nare lower than before, because their overlapping terms are usually\nnot key terms of their topics.\n7.2 Parameter selection for indexing-tree\ndetection\nFigure 3 shows the minimum normalized costs obtained by\nsystem-3 on TDT3 using different parameters. The\u00ce\u00b8 init parameter\nis tested on six values spanning from 0.03 to 0.18. And the \u00ce\u00bb\nparameter is tested on four values 1, 2, 3 and 4. We can see that,\nwhen\u00ce\u00b8 init is set to 0.12, which is the closest one to\u00ce\u00b8 new, the costs\nare lower than others. This is easy to explain, because when\nstories belonging to the same topic are put in a cluster, it is more\nreasonable for the cluster to represent the stories in it. When\nparameter \u00ce\u00bb is set to 3 or 4, the costs are better than other cases,\nbut there is no much difference between 3 and 4.\n0\n0.05\n0.1\n0.15\n0.2\n1\n2\n3\n4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n\u00ce\u00b8-init\u00ce\u00bb\nMinCost\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nFigure 3. Min Cost on TDT3 (\u00ce\u00b4 =0.15)\n0\n0.05\n0.1\n0.15\n0.2\n1\n2\n3\n4\n0\n0.5\n1\n1.5\n2\n2.5\nx 10\n8\n\u00ce\u00b8-init\n\u00ce\u00bb\nComparingtimes\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\nx 10\n8\nFigure 4. Comparing times on TDT3 (\u00ce\u00b4 =0.15)\nFigure 4 gives the comparing times used by system-3 on TDT3\nwith the same parameters as figure 3. The comparing times are\nstrongly dependent on\u00ce\u00b8 init. Because the greater\u00ce\u00b8 init is, the less\nstories combined together, the more comparing times are needed\nfor new event decision.\nSo we use\u00ce\u00b8 init =0.13,\u00ce\u00bb =3,\u00ce\u00b4 =0.15 for system-3, 4, and 5. In this\nparameter setting, we can get both low minimum normalized costs\nand less comparing times.\n8. CONCLUSION\nWe have proposed a news indexing-tree based detection\nprocedure in our model. It reduces comparing times to about one\nseventh of traditional method without hurting NED accuracy. We\nalso have presented two extensions to the basic TF-IDF model.\nThe first extension is made by adjust term weights based on term\ndistributions between the whole corpus and a cluster story set.\nAnd the second extension to basic TF-IDF model is better use of\nterm types (named entities types and part-of-speed) according to\nnews categories. Our experimental results on TDT2 and TDT3\ndatasets show that both of the two extensions contribute\nsignificantly to improvement in accuracy.\nWe did not consider news time information as a clue for NED\ntask, since most of the topics last for a long time and TDT data\nsets only span for a relative short period (no more than 6 months).\nFor the future work, we want to collect news set which span for a\nlonger period from internet, and integrate time information in\nNED task. Since topic is a relative coarse-grained news cluster,\nwe also want to refine cluster granularity to event-level, and\nidentify different events and their relations within a topic.\nAcknowledgments\nThis work is supported by the National Natural Science\nFoundation of China under Grant No. 90604025. Any opinions,\nfindings and conclusions or recommendations expressed in this\nmaterial are the author(s) and do not necessarily reflect those of\nthe sponsor.\n9. REFERENCES\n[1] http://www.nist.gov/speech/tests/tdt/index.htm\n[2] In Topic Detection and Tracking. Event-based Information\nOrganization. Kluwer Academic Publishers, 2002.\n.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90\n1\n2\n5\n10\n20\n40\n60\n80\n90\nFalse Alarm Probability (in %)\nMissProbability(in%) SYSTEM1 Topic Weighted Curve\nSYSTEM1 Min Norm(Cost)\nSYSTEM2 Topic Weighted Curve\nSYSTEM2 Min Norm(Cost)\nSYSTEM3 Topic Weighted Curve\nSYSTEM3 Min Norm(Cost)\nSYSTEM4 Topic Weighted Curve\nSYSTEM4 Min Norm(Cost)\nSYSTEM5 Topic Weighted Curve\nSYSTEM5 Min Norm(Cost)\nRandom Performance\nFigure 5. DET curves on TDT3\n[3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald,\nand X. Liu. Learning Approaches for Detecting and Tracking\nNews Events. In IEEE Intelligent Systems Special Issue on\nApplications of Intelligent Information Retrieval, volume 14\n(4), 1999, 32-43.\n[4] Y. Yang, T. Pierce, and J. Carbonell. A Study on\nRetrospective and On-line Event Detection. In Proceedings\nof SIGIR-98, Melbourne, Australia, 1998, 28-36.\n[5] J. Allan, V. Lavrenko, D. Malin, and R. Swan. Detections,\nBounds, and Timelines: Umass and tdt-3. In Proceedings of\nTopic Detection and Tracking Workshop (TDT-3), Vienna,\nVA, 2000, 167-174.\n[6] R. Papka and J. Allan. On-line New Event Detection Using\nSingle Pass Clustering TITLE2:. Technical Report \nUM-CS1998-021, 1998.\n[7] W. Lam, H. Meng, K. Wong, and J. Yen. Using Contextual\nAnalysis for News Event Detection. International Journal on\nIntelligent Systems, 2001, 525-546.\n[8] B. Thorsten, C. Francine, and F. Ayman. A System for New\nEvent Detection. In Proceedings of the 26th Annual\nInternational ACM SIGIR Conference, New York, NY, USA.\nACM Press. 2003, 330-337.\n[9] S. Nicola and C. Joe. Combining Semantic and Syntactic\nDocument Classifiers to Improve First Story Detection. In\nProceedings of the 24th Annual International ACM SIGIR\nConference, New York, NY, USA. ACM Press. 2001, \n424425.\n[10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \nTopicconditioned Novelty Detection. In Proceedings of the 8th\nACM SIGKDD International Conference, ACM Press. 2002,\n688-693.\n[11] M. Juha, A.M. Helena, and S. Marko. Applying Semantic\nClasses in Event Detection and Tracking. In Proceedings of\nInternational Conference on Natural Language Processing\n(ICON 2002), 2002, pages 175-183.\n[12] M. Juha, A.M. Helena, and S. Marko. Simple Semantics in\nTopic Detection and Tracking. Information Retrieval, 7(3-4):\n2004, 347-368.\n[13] K. Giridhar and J. Allan. Text Classification and Named\nEntities for New Event Detection. In Proceedings of the 27th\nAnnual International ACM SIGIR Conference, New York,\nNY, USA. ACM Press. 2004, 297-304.\n[14] J. P. Callan, W. B. Croft, and S. M. Harding. The INQUERY\nRetrieval System. In Proceedings of DEXA-92, 3rd\nInternational Conference on Database and Expert Systems\nApplications, 1992, 78-83.\n[15] R. Krovetz. Viewing Morphology as An Inference Process.\nIn Proceedings of ACM SIGIR93, 1993, 61-81.\n[16] Y. Yang and J. Pedersen. A Comparative Study on Feature\nSelection in Text Categorization. In J. D. H. Fisher, editor,\nThe Fourteenth International Conference on Machine\nLearning (ICML'97), Morgan Kaufmann, 1997, 412-420.\n[17] T. M. Cover, and J.A. Thomas. Elements of Information\nTheory. Wiley. 1991.\n[18] The linguistic data consortium, http://www.ldc,upenn.edu/.\n[19] The 2001 TDT task definition and evaluation plan,\nhttp://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm.\n[20] R. E. Schapire and Y. Singer. Boostexter: A Boosting-based\nSystem for Text Categorization. In Machine Learning\n39(2/3):1, Kluwer Academic Publishers, 2000, 35-168.\n[21] K. Giridhar and J. Allan. 2005. Using Names and Topics for\nNew Event Detection. In Proceedings of Human Technology\nConference and Conference on Empirical Methods in\nNatural Language, Vancouver, 2005, 121-128\n": ["new event detection", "stream of news story", "news story stream", "volume of news", "news volume", "speed up the ned task", "news indexing-tree", "term reweighting approach", "ned accuracy", "term weight", "statistics", "training data", "named entity reweighting mode", "class of story", "story class", "linguistic data consortium", "baseline system", "existing system", "topic detection and track", "name entity", "real-time index", ""]}